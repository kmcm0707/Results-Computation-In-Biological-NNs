@incollection{aguilera2021,
  title = {Programmable {{Fading Memory}} in {{Atomic Switch Systems}} for {{Error Checking Applications}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Aguilera, Renato and Sillin, Henry O. and Stieg, Adam Z. and Gimzewski, James K.},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {273--303},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_12},
  urldate = {2024-12-10},
  abstract = {Disruptive technology in computational devices is required as the universal computing machines approach quantum mechanical limits. Integration of state-of-the-art memristive devices provides optimal scaling of current technologies beyond this limit through the adoption of neuromorphic models. Universal computing machines pioneered by Alan Turing are strictly based on top-down intelligent design. Neuromorphic models instead engage in bottom-up programmability by emulating mammalian brain design and characteristics. Here we show the design, characterization, and implementation of a massively parallel memristor neuromorphic network based on metal chalcogenide atomic switch network (ASN) systems with key characteristics such as short- and long-term potentiation, power-law dynamics, and scale-free topology.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@misc{ahmad2020,
  title = {{{GAIT-prop}}: {{A}} Biologically Plausible Learning Rule Derived from Backpropagation of Error},
  shorttitle = {{{GAIT-prop}}},
  author = {Ahmad, Nasir and van Gerven, Marcel A. J. and Ambrogioni, Luca},
  year = {2020},
  month = nov,
  number = {arXiv:2006.06438},
  eprint = {2006.06438},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {Traditional backpropagation of error, though a highly successful algorithm for learning in artificial neural network models, includes features which are biologically implausible for learning in real neural circuits. An alternative called target propagation proposes to solve this implausibility by using a top-down model of neural activity to convert an error at the output of a neural network into layer-wise and plausible 'targets' for every unit. These targets can then be used to produce weight updates for network training. However, thus far, target propagation has been heuristically proposed without demonstrable equivalence to backpropagation. Here, we derive an exact correspondence between backpropagation and a modified form of target propagation (GAIT-prop) where the target is a small perturbation of the forward pass. Specifically, backpropagation and GAIT-prop give identical updates when synaptic weight matrices are orthogonal. In a series of simple computer vision experiments, we show near-identical performance between backpropagation and GAIT-prop with a soft orthogonality-inducing regularizer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\8EXS5WIW\\Ahmad et al. - 2020 - GAIT-prop A biologically plausible learning rule derived from backpropagation of error.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\6ZKLIF72\\2006.html}
}

@misc{akrout2020,
  title = {Deep {{Learning}} without {{Weight Transport}}},
  author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter C. and Lillicrap, Timothy and Tweed, Douglas},
  year = {2020},
  month = jan,
  number = {arXiv:1904.05391},
  eprint = {1904.05391},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.05391},
  urldate = {2025-02-04},
  abstract = {Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms - a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 - both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring.Tested on the ImageNet visual-recognition task, these mechanisms outperform both feedback alignment and the newer sign-symmetry method, and nearly match backprop, the standard algorithm of deep learning, which uses weight transport.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\G6JHVPTZ\\Akrout et al. - 2020 - Deep Learning without Weight Transport.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\43EUVEXQ\\1904.html}
}

@article{appeltant2011,
  title = {Information Processing Using a Single Dynamical Node as Complex System},
  author = {Appeltant, L. and Soriano, M.C. and Van Der Sande, G. and Danckaert, J. and Massar, S. and Dambre, J. and Schrauwen, B. and Mirasso, C.R. and Fischer, I.},
  year = {2011},
  month = sep,
  journal = {Nature Communications},
  volume = {2},
  number = {1},
  pages = {468},
  issn = {2041-1723},
  doi = {10.1038/ncomms1476},
  urldate = {2024-12-10},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\N9DT9PHC\Appeltant et al. - 2011 - Information processing using a single dynamical node as complex system.pdf}
}

@misc{ba2016,
  title = {Using {{Fast Weights}} to {{Attend}} to the {{Recent Past}}},
  author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
  year = {2016},
  month = dec,
  number = {arXiv:1610.06258},
  eprint = {1610.06258},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.06258},
  urldate = {2024-12-02},
  abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\7JX4R85U\\Ba et al. - 2016 - Using Fast Weights to Attend to the Recent Past.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\MKJYDGBJ\\1610.html}
}

@article{bendayanrubin2007,
  title = {Long Memory Lifetimes Require Complex Synapses and Limited Sparseness},
  author = {Ben Dayan Rubin, Daniel D. and Fusi, Stefano},
  year = {2007},
  month = nov,
  journal = {Frontiers in Computational Neuroscience},
  volume = {1},
  publisher = {Frontiers},
  issn = {1662-5188},
  doi = {10.3389/neuro.10.007.2007},
  urldate = {2024-11-28},
  abstract = {{$<$}p{$>$}Theoretical studies have shown that memories last longer if the neural representations are sparse, that is, when each neuron is selective for a small fraction of the events creating the memories. Sparseness reduces both the interference between stored memories and the number of synaptic modifications which are necessary for memory storage. Paradoxically, in cortical areas like the inferotemporal cortex, where presumably memory lifetimes are longer than in the medial temporal lobe, neural representations are less sparse. We resolve this paradox by analyzing the effects of sparseness on complex models of synaptic dynamics in which there are metaplastic states with different degrees of plasticity. For these models, memory retention in a large number of synapses across multiple neurons is significantly more efficient in case of many metaplastic states, that is, for an elevated degree of complexity. In other words, larger brain regions allow to retain memories for significantly longer times only if the synaptic complexity increases with the total number of synapses. However, the initial memory trace, the one experienced immediately after memory storage, becomes weaker both when the number of metaplastic states increases and when the neural representations become sparser. Such a memory trace must be above a given threshold in order to permit every single neuron to retrieve the information stored in its synapses. As a consequence, if the initial memory trace is reduced because of the increased synaptic complexity, then the neural representations must be less sparse. We conclude that long memory lifetimes allowed by a larger number of synapses require more complex synapses, and hence, less sparse representations, which is what is observed in the brain.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Learning,sparseness,synaptic plasticity},
  file = {C:\Users\kmc07\Zotero\storage\BJYWGE8F\Ben Dayan Rubin and Fusi - 2007 - Long memory lifetimes require complex synapses and limited sparseness.pdf}
}

@article{bengio,
  title = {On the {{Optimization}} of a {{Synaptic Learning Rule}}},
  author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\4SMDED63\Bengio et al. - On the Optimization of a Synaptic Learning Rule.pdf}
}

@inproceedings{bengio1991,
  title = {Learning a Synaptic Learning Rule},
  booktitle = {{{IJCNN-91-Seattle International Joint Conference}} on {{Neural Networks}}},
  author = {Bengio, Y. and Bengio, S. and Cloutier, J.},
  year = {1991},
  month = jul,
  volume = {ii},
  pages = {969 vol.2-},
  doi = {10.1109/IJCNN.1991.155621},
  urldate = {2024-10-18},
  abstract = {Summary form only given, as follows. The authors discuss an original approach to neural modeling based on the idea of searching, with learning methods, for a synaptic learning rule which is biologically plausible and yields networks that are able to learn to perform difficult tasks. The proposed method of automatically finding the learning rule relies on the idea of considering the synaptic modification rule as a parametric function. This function has local inputs and is the same in many neurons. The parameters that define this function can be estimated with known learning methods. For this optimization, particular attention is given to gradient descent and genetic algorithms. In both cases, estimation of this function consists of a joint global optimization of the synaptic modification function and the networks that are learning to perform some tasks. Both network architecture and the learning function can be designed within constraints derived from biological knowledge.{$<>$}},
  keywords = {Biological system modeling,Biology,Computer science,Genetic algorithms,Learning systems,Neurons},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\AXCLBLYX\\Bengio et al. - 1991 - Learning a synaptic learning rule.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JZ6EYSBC\\Bengio et al. - 1991 - Learning a synaptic learning rule.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JLDA4IE5\\155621.html}
}

@inproceedings{bengio1994,
  title = {Use of Genetic Programming for the Search of a New Learning Rule for Neural Networks},
  booktitle = {Proceedings of the {{First IEEE Conference}} on {{Evolutionary Computation}}. {{IEEE World Congress}} on {{Computational Intelligence}}},
  author = {Bengio, S. and Bengio, Y. and Cloutier, J.},
  year = {1994},
  month = jun,
  pages = {324-327 vol.1},
  doi = {10.1109/ICEC.1994.349932},
  urldate = {2024-10-17},
  abstract = {In previous work we explained how to use standard optimization methods such as simulated annealing, gradient descent and genetic algorithms to optimize a parametric function which could be used as a learning rule for neural networks. To use these methods, we had to choose a fixed number of parameters and a rigid form for the learning rule. In this article, we propose to use genetic programming to find not only the values of rule parameters but also the optimal number of parameters and the form of the rule. Experiments on classification tasks suggest genetic programming finds better learning rules than other optimization methods. Furthermore, the best rule found with genetic programming outperformed the well-known backpropagation algorithm for a given set of tasks.{$<>$}},
  keywords = {Backpropagation algorithms,Biological system modeling,Design optimization,Genetic algorithms,Genetic programming,Learning systems,Neural networks,Neurons,Optimization methods,Simulated annealing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\Y83VM8RA\\Bengio et al. - 1994 - Use of genetic programming for the search of a new learning rule for neural networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\48L99IW7\\349932.html}
}

@inproceedings{bengio2006,
  title = {Greedy {{Layer-Wise Training}} of {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2006},
  volume = {19},
  publisher = {MIT Press},
  urldate = {2024-10-10},
  abstract = {Recent analyses (Bengio, Delalleau, \& Le Roux, 2006; Bengio \& Le Cun, 2007) of modern nonparametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limitations of some learning algorithms. The problem is clear in kernel-based approaches when the kernel is "local" (e.g., the Gaussian kernel), i.e., K (x, y ) converges to a constant when {\textbar}{\textbar}x - y {\textbar}{\textbar} increases. These analyses point to the difficulty of learning "highly-varying functions", i.e., functions that have a large number of "variations" in the domain of interest, e.g., they would require a large number of pieces to be well represented by a piecewise-linear approximation. Since the number of pieces can be made to grow exponentially with the number of factors of variations in the input, this is connected with the well-known curse of dimensionality for classical non-parametric learning algorithms (for regression, classification and density estimation). If the shapes of all these pieces are unrelated, one needs enough examples for each piece in order to generalize properly. However, if these shapes are related and can be predicted from each other, "non-local" learning algorithms have the potential to generalize to pieces not covered by the training set. Such ability would seem necessary for learning in complex domains such as Artificial Intelligence tasks (e.g., related to vision, language, speech, robotics). Kernel machines (not only those with a local kernel) have a shallow architecture, i.e., only two levels of data-dependent computational elements. This is also true of feedforward neural networks with a single hidden layer (which can become SVMs when the number of hidden units becomes large (Bengio, Le Roux, Vincent, Delalleau, \& Marcotte, 2006)). A serious problem with shallow architectures is that they can be very inefficient in terms of the number of computational units (e.g., bases, hidden units), and thus in terms of required examples (Bengio \& Le Cun, 2007). One way to represent a highly-varying function compactly (with few parameters) is through the composition of many non-linearities, i.e., with a deep architecture. For example, the parity function with d inputs requires O(2d ) examples and parameters to be represented by a Gaussian SVM (Bengio et al., 2006), O(d2 ) parameters for a one-hidden-layer neural network, O(d) parameters and units for a multi-layer network with O(log2 d) layers, and O(1) parameters with a recurrent neural network. More generally,},
  file = {C:\Users\kmc07\Zotero\storage\RAEBBY6W\Bengio et al. - 2006 - Greedy Layer-Wise Training of Deep Networks.pdf}
}

@article{benna2016,
  title = {Computational Principles of Synaptic Memory Consolidation},
  author = {Benna, Marcus K. and Fusi, Stefano},
  year = {2016},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {19},
  number = {12},
  pages = {1697--1706},
  issn = {1546-1726},
  doi = {10.1038/nn.4401},
  abstract = {Memories are stored and retained through complex, coupled processes operating on multiple timescales. To understand the computational principles behind these intricate networks of interactions, we construct a broad class of synaptic models that efficiently harness biological complexity to preserve numerous memories by protecting them against the adverse effects of overwriting. The memory capacity scales almost linearly with the number of synapses, which is a substantial improvement over the square root scaling of previous models. This was achieved by combining multiple dynamical processes that initially store memories in fast variables and then progressively transfer them to slower variables. Notably, the interactions between fast and slow variables are bidirectional. The proposed models are robust to parameter perturbations and can explain several properties of biological memory, including delayed expression of synaptic modifications, metaplasticity, and spacing effects.},
  langid = {english},
  pmid = {27694992},
  keywords = {Animals,Computer Simulation,Consolidation,Long-term memory,Memory,Memory Consolidation,Models Neurological,Nerve Net,Neuronal Plasticity,Neurons,Synapses},
  file = {C:\Users\kmc07\Zotero\storage\DJIUDPI3\Benna and Fusi - 2016 - Computational principles of synaptic memory consolidation.pdf}
}

@misc{brannvall2023,
  title = {{{ReLU}} and {{Addition-based Gated RNN}}},
  author = {Br{\"a}nnvall, Rickard and Forsgren, Henrik and Sandin, Fredrik and Liwicki, Marcus},
  year = {2023},
  month = aug,
  number = {arXiv:2308.05629},
  eprint = {2308.05629},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while significantly reducing computational costs such that execution time is reduced by half on CPU and by one-third under encryption. Experimental results on handwritten text recognition tasks furthermore show that the proposed architecture can be trained to achieve comparable accuracy to conventional GRU and LSTM baselines. The gating mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the multiplication of encrypted variables. It can also support quantization in (unencrypted) plaintext applications, with the potential for substantial performance gains since the addition-based formulation can avoid the expansion to double precision often required for multiplication.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\VEAWR2GX\\Br√§nnvall et al. - 2023 - ReLU and Addition-based Gated RNN.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\3XP4LZN6\\2308.html}
}

@article{chevaleyre2007,
  title = {Endocannabinoid-Mediated Long-Term Plasticity Requires {{cAMP}}/{{PKA}} Signaling and {{RIM1alpha}}},
  author = {Chevaleyre, Vivien and Heifets, Boris D. and Kaeser, Pascal S. and S{\"u}dhof, Thomas C. and Castillo, Pablo E.},
  year = {2007},
  month = jun,
  journal = {Neuron},
  volume = {54},
  number = {5},
  pages = {801--812},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2007.05.020},
  abstract = {Endocannabinoids (eCBs) have emerged as key activity-dependent signals that, by activating presynaptic cannabinoid receptors (i.e., CB1) coupled to G(i/o) protein, can mediate short-term and long-term synaptic depression (LTD). While the presynaptic mechanisms underlying eCB-dependent short-term depression have been identified, the molecular events linking CB1 receptors to LTD are unknown. Here we show in the hippocampus that long-term, but not short-term, eCB-dependent depression of inhibitory transmission requires presynaptic cAMP/PKA signaling. We further identify the active zone protein RIM1alpha as a key mediator of both CB1 receptor effects on the release machinery and eCB-dependent LTD in the hippocampus. Moreover, we show that eCB-dependent LTD in the amygdala and hippocampus shares major mechanistic features. These findings reveal the signaling pathway by which CB1 receptors mediate long-term effects of eCBs in two crucial brain structures. Furthermore, our results highlight a conserved mechanism of presynaptic plasticity in the brain.},
  langid = {english},
  pmcid = {PMC2001295},
  pmid = {17553427},
  keywords = {Amygdala,Animals,Cannabinoid Receptor Modulators,Cyclic AMP,Cyclic AMP-Dependent Protein Kinases,Endocannabinoids,GTP-Binding Proteins,Hippocampus,Long-Term Synaptic Depression,Male,Mice,Mice Inbred C57BL,Mice Knockout,Neural Inhibition,Neural Pathways,Organ Culture Techniques,Receptor Cannabinoid CB1,Signal Transduction,Synaptic Transmission},
  file = {C:\Users\kmc07\Zotero\storage\UJV53MQP\Chevaleyre et al. - 2007 - Endocannabinoid-mediated long-term plasticity requires cAMPPKA signaling and RIM1alpha.pdf}
}

@misc{cho2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  number = {arXiv:1406.1078},
  eprint = {1406.1078},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.1078},
  urldate = {2025-01-20},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\SKRT5PCN\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\XDLFRV6N\\1406.html}
}

@misc{chu2023,
  title = {Random {{Feedback Alignment Algorithms}} to Train {{Neural Networks}}: {{Why}} Do They {{Align}}?},
  shorttitle = {Random {{Feedback Alignment Algorithms}} to Train {{Neural Networks}}},
  author = {Chu, Dominique and Bacho, Florian},
  year = {2023},
  month = jun,
  number = {arXiv:2306.02325},
  eprint = {2306.02325},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.02325},
  urldate = {2025-02-17},
  abstract = {Feedback alignment algorithms are an alternative to backpropagation to train neural networks, whereby some of the partial derivatives that are required to compute the gradient are replaced by random terms. This essentially transforms the update rule into a random walk in weight space. Surprisingly, learning still works with those algorithms, including training of deep neural networks. This is generally attributed to an alignment of the update of the random walker with the true gradient - the eponymous gradient alignment -- which drives an approximate gradient descend. The mechanism that leads to this alignment remains unclear, however. In this paper, we use mathematical reasoning and simulations to investigate gradient alignment. We observe that the feedback alignment update rule has fixed points, which correspond to extrema of the loss function. We show that gradient alignment is a stability criterion for those fixed points. It is only a necessary criterion for algorithm performance. Experimentally, we demonstrate that high levels of gradient alignment can lead to poor algorithm performance and that the alignment is not always driving the gradient descend.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\44FT7PQJ\\Chu and Bacho - 2023 - Random Feedback Alignment Algorithms to train Neural Networks Why do they Align.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\GQX6RYRL\\2306.html}
}

@misc{cohen2017,
  title = {{{EMNIST}}: An Extension of {{MNIST}} to Handwritten Letters},
  shorttitle = {{{EMNIST}}},
  author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, Andr{\'e}},
  year = {2017},
  month = feb,
  number = {arXiv:1702.05373},
  eprint = {1702.05373},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.05373},
  urldate = {2025-01-20},
  abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\W6BD6PIW\\Cohen et al. - 2017 - EMNIST an extension of MNIST to handwritten letters.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\5TYMMTP4\\1702.html}
}

@inproceedings{confavreux2020,
  title = {A Meta-Learning Approach to (Re)Discover Plasticity Rules That Carve a Desired Function into a Neural Network},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Confavreux, Basile and Zenke, Friedemann and Agnes, Everton and Lillicrap, Timothy and Vogels, Tim},
  year = {2020},
  volume = {33},
  pages = {16398--16408},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-10-18},
  abstract = {The search for biologically faithful synaptic plasticity rules has resulted in a large body of models. They are usually inspired by -- and fitted to -- experimental data, but they rarely produce neural dynamics that serve complex functions. These failures suggest that current plasticity models are still under-constrained by existing data. Here, we present an alternative approach that uses meta-learning to discover plausible synaptic plasticity rules. Instead of experimental data, the rules are constrained by the functions they implement and the structure they are meant to produce. Briefly, we parameterize synaptic plasticity rules by a Volterra expansion and then use supervised learning methods (gradient descent or evolutionary strategies) to minimize a problem-dependent loss function that quantifies how effectively a candidate plasticity rule transforms an initially random network into one with the desired function. We first validate our approach by re-discovering previously described plasticity rules, starting at the single-neuron level and Oja's rule'', a simple Hebbian plasticity rule that captures the direction of most variability of inputs to a neuron (i.e., the first principal component). We expand the problem to the network level and ask the framework to find Oja's rule together with an anti-Hebbian rule such that an initially random two-layer firing-rate network will recover several principal components of the input space after learning. Next, we move to networks of integrate-and-fire neurons with plastic inhibitory afferents. We train for rules that achieve a target firing rate by countering tuned excitation. Our algorithm discovers a specific subset of the manifold of rules that can solve this task. Our work is a proof of principle of an automated and unbiased approach to unveil synaptic plasticity rules that obey biological constraints and can solve complex functions.},
  file = {C:\Users\kmc07\Zotero\storage\7H328YRP\Confavreux et al. - 2020 - A meta-learning approach to (re)discover plasticity rules that carve a desired function into a neura.pdf}
}

@article{coulombe2017,
  title = {Computing with Networks of Nonlinear Mechanical Oscillators},
  author = {Coulombe, Jean C. and York, Mark C. A. and Sylvestre, Julien},
  year = {2017},
  month = jun,
  journal = {PLOS ONE},
  volume = {12},
  number = {6},
  pages = {e0178663},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0178663},
  urldate = {2024-12-10},
  abstract = {As it is getting increasingly difficult to achieve gains in the density and power efficiency of microelectronic computing devices because of lithographic techniques reaching fundamental physical limits, new approaches are required to maximize the benefits of distributed sensors, micro-robots or smart materials. Biologically-inspired devices, such as artificial neural networks, can process information with a high level of parallelism to efficiently solve difficult problems, even when implemented using conventional microelectronic technologies. We describe a mechanical device, which operates in a manner similar to artificial neural networks, to solve efficiently two difficult benchmark problems (computing the parity of a bit stream, and classifying spoken words). The device consists in a network of masses coupled by linear springs and attached to a substrate by non-linear springs, thus forming a network of anharmonic oscillators. As the masses can directly couple to forces applied on the device, this approach combines sensing and computing functions in a single power-efficient device with compact dimensions.},
  langid = {english},
  keywords = {Artificial neural networks,Computer hardware,Computer networks,Computers,Network analysis,Neurons,Signaling networks,Speech signal processing},
  file = {C:\Users\kmc07\Zotero\storage\SGH33W4Y\Coulombe et al. - 2017 - Computing with networks of nonlinear mechanical oscillators.pdf}
}

@incollection{dale2021,
  title = {Reservoir {{Computing}} in {{Material Substrates}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Dale, Matthew and Miller, Julian F. and Stepney, Susan and Trefzer, Martin A.},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {141--166},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_7},
  urldate = {2024-12-10},
  abstract = {We overview Reservoir Computing (RC) with physical systems from an Unconventional Computing (UC) perspective. We discuss challenges present in both fields, including encoding and representation, or how to manipulate and read information; ways to search large and complex configuration spaces of physical systems; and what makes a ``good'' computing substrate.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@book{dayan2001,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, L. F.},
  year = {2001},
  series = {Computational Neuroscience},
  publisher = {Massachusetts Institute of Technology Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-04199-7},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\S6TW4RBW\Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathematical modeling of neural systems.pdf}
}

@misc{detorakis2018,
  title = {Contrastive {{Hebbian Learning}} with {{Random Feedback Weights}}},
  author = {Detorakis, Georgios and Bartley, Travis and Neftci, Emre},
  year = {2018},
  month = jun,
  number = {arXiv:1806.07406},
  eprint = {1806.07406},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.07406},
  urldate = {2025-01-08},
  abstract = {Neural networks are commonly trained to make predictions through learning algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by gradient backpropagation, is based on Hebb's rule and the contrastive divergence algorithm. It operates in two phases, the forward (or free) phase, where the data are fed to the network, and a backward (or clamped) phase, where the target signals are clamped to the output layer of the network and the feedback signals are transformed through the transpose synaptic weight matrices. This implies symmetries at the synaptic level, for which there is no evidence in the brain. In this work, we propose a new variant of the algorithm, called random contrastive Hebbian learning, which does not rely on any synaptic weights symmetries. Instead, it uses random matrices to transform the feedback signals during the clamped phase, and the neural dynamics are described by first order non-linear differential equations. The algorithm is experimentally verified by solving a Boolean logic task, classification tasks (handwritten digits and letters), and an autoencoding task. This article also shows how the parameters affect learning, especially the random matrices. We use the pseudospectra analysis to investigate further how random matrices impact the learning process. Finally, we discuss the biological plausibility of the proposed algorithm, and how it can give rise to better computational models for learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\34CDRTBB\\Detorakis et al. - 2018 - Contrastive Hebbian Learning with Random Feedback Weights.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\WS4VRAHN\\1806.html}
}

@incollection{dominey2021,
  title = {Cortico-{{Striatal Origins}} of {{Reservoir Computing}}, {{Mixed Selectivity}}, and {{Higher Cognitive Function}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Dominey, Peter Ford},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {29--58},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_2},
  urldate = {2024-12-10},
  abstract = {The computational richness and complexity of recurrent neural networks is well known, and has yet to be fully exploited and understood. It is interesting that in this context, one of the most prevalent features of the cerebral cortex is its massive recurrent connectivity. Despite this central principle of cortical organization, it is only slowly becoming recognized that the cortex is a reservoir. Of course there are mechanisms in the cortex that allow for plasticity. But the general model of a reservoir as a recurrent network that creates a high dimensional temporal expansion of its inputs which can then be harvested for extracting the required output is fully achieved by the cortex. Future research will find this obvious. This chapter provides a framework for more clearly understanding this conception of cortex and the corticostriatal system.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@inproceedings{faraji2015,
  title = {A Biologically Plausible 3-Factor Learning Rule for Expectation Maximization in Reinforcement Learning and Decision Making},
  author = {Faraji, Mohammad Javad and Preuschoff, K. and Gerstner, W.},
  year = {2015},
  urldate = {2024-11-25},
  abstract = {One of the most frequent problems in both decision making and reinforcement learning (RL) is expectation maximization involving functionals such as reward or utility. Generally, these problems consist of computing the optimal solution of a density function. Instead of trying to find this exact solution, a common approach is to approximate it through a learning process. In this work we propose a functional gradient rule for the maximization of a general form of density-dependent functionals using a stochastic gradient ascent algorithm. If a neural network is used for parametrization of the desired density function, the proposed learning rule can be viewed as a modulated Hebbian rule. Such a learning rule is biologically plausible, because it consists of both local and global factors corresponding to the coactivity of pre/post-synaptic neurons and the effect of neuromodulation, respectively. We first apply our technique to standard reward maximization in RL. As expected, this yields the standard policy gradient rule in which parameters of the model are updated proportional to the amount of reward. Next, we use variational free energy as a functional and find that the estimated change in parameters is modulated by a measure of surprise signal. Finally, we propose an information theoretical equivalent of existing models in expected utility maximization, as a standard model of decision making, to incorporate both individual preferences and choice variability. We show that our technique can also be applied into such novel framework.},
  file = {C:\Users\kmc07\Zotero\storage\UZHRDBNL\Faraji et al. - 2015 - A biologically plausible 3-factor learning rule for expectation maximization in reinforcement learni.pdf}
}

@article{fusi2005,
  title = {Cascade Models of Synaptically Stored Memories},
  author = {Fusi, Stefano and Drew, Patrick J. and Abbott, L. F.},
  year = {2005},
  month = feb,
  journal = {Neuron},
  volume = {45},
  number = {4},
  pages = {599--611},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.02.001},
  abstract = {Storing memories of ongoing, everyday experiences requires a high degree of plasticity, but retaining these memories demands protection against changes induced by further activity and experience. Models in which memories are stored through switch-like transitions in synaptic efficacy are good at storing but bad at retaining memories if these transitions are likely, and they are poor at storage but good at retention if they are unlikely. We construct and study a model in which each synapse has a cascade of states with different levels of plasticity, connected by metaplastic transitions. This cascade model combines high levels of memory storage with long retention times and significantly outperforms alternative models. As a result, we suggest that memory storage requires synapses with multiple states exhibiting dynamics over a wide range of timescales, and we suggest experimental tests of this hypothesis.},
  langid = {english},
  pmid = {15721245},
  keywords = {Animals,Humans,Memory,Models Neurological,Neuronal Plasticity,Synapses,Time Factors}
}

@article{fusi2007,
  title = {Limits on the Memory Storage Capacity of Bounded Synapses},
  author = {Fusi, Stefano and Abbott, L. F.},
  year = {2007},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {4},
  pages = {485--493},
  issn = {1097-6256},
  doi = {10.1038/nn1859},
  abstract = {Memories maintained in patterns of synaptic connectivity are rapidly overwritten and destroyed by ongoing plasticity related to the storage of new memories. Short memory lifetimes arise from the bounds that must be imposed on synaptic efficacy in any realistic model. We explored whether memory performance can be improved by allowing synapses to traverse a large number of states before reaching their bounds, or by changing the way these bounds are imposed. In the case of hard bounds, memory lifetimes grow proportional to the square of the number of synaptic states, but only if potentiation and depression are precisely balanced. Improved performance can be obtained without fine tuning by imposing soft bounds, but this improvement is only linear with respect to the number of synaptic states. We explored several other possibilities and conclude that improving memory performance requires a more radical modification of the standard model of memory storage.},
  langid = {english},
  pmid = {17351638},
  keywords = {Animals,Humans,Memory,Models Neurological,Neuronal Plasticity,Synapses},
  file = {C:\Users\kmc07\Zotero\storage\G2SIPD84\Fusi and Abbott - 2007 - Limits on the memory storage capacity of bounded synapses.pdf}
}

@incollection{gallicchio2021,
  title = {Deep {{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {77--95},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_4},
  urldate = {2024-12-10},
  abstract = {This chapter surveys the recent advancements on the extension of Reservoir Computing toward deep architectures, which is gaining increasing research attention in the neural networks community. Within this context, we focus on describing the major features of Deep Echo State Networks based on the hierarchical composition of multiple reservoirs. The intent is to provide a useful reference to guide applications and further developments of this efficient and effective class of approaches to deal with times-series and more complex data within a unified description and analysis.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@inproceedings{gomez2005,
  title = {Evolving {{Modular Fast-Weight Networks}} for {{Control}}},
  booktitle = {Artificial {{Neural Networks}}: {{Formal Models}} and {{Their Applications}} -- {{ICANN}} 2005},
  author = {Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  editor = {Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.z}ny, S{\l}awomir},
  year = {2005},
  pages = {383--389},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11550907_61},
  abstract = {In practice, almost all control systems in use today implement some form of linear control. However, there are many tasks for which conventional control engineering methods are not directly applicable because there is not enough information about how the system should be controlled (i.e. reinforcement learning problems). In this paper, we explore an approach to such problems that evolves fast-weight neural networks. These networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers. The method is tested on 2D mobile robot version of the pole balancing task where the controller must learn to switch between two operating modes, one using a single pole and the other using a jointed pole version that has not before been solved.},
  isbn = {978-3-540-28756-8},
  langid = {english},
  keywords = {Bipedal Robot,Controller Network,Linear Controller,Recurrent Network,Single Pole},
  file = {C:\Users\kmc07\Zotero\storage\TXKPMMNL\Gomez and Schmidhuber - 2005 - Evolving Modular Fast-Weight Networks for Control.pdf}
}

@article{grant2010,
  title = {Computing Behaviour in Complex Synapses: {{Synapse}} Proteome Complexity and the Evolution of Behaviour and Disease},
  shorttitle = {Computing Behaviour in Complex Synapses},
  author = {Grant, Seth G.N.},
  year = {2010},
  month = apr,
  journal = {The Biochemist},
  volume = {32},
  number = {2},
  pages = {6--9},
  issn = {0954-982X, 1740-1194},
  doi = {10.1042/BIO03202006},
  urldate = {2024-11-28},
  abstract = {Synapses are the defining feature of the cellular organization of the brain, connecting each neuron with thousands of others. The great morphological diversity of neurons was recognized in the 19th Century, but only in the last 10 years has the remarkable degree of molecular complexity within synapses become apparent. Over 1000 proteins are found in the proteome of the postsynaptic terminal of mammalian synapses. This complexity is organized into networks providing combinatorial signalling for physiological processes, diversity of synapse types and disease susceptibility, as well as providing a new paradigm for the evolution of the brain.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\QNQE9DZJ\Grant - 2010 - Computing behaviour in complex synapses Synapse proteome complexity and the evolution of behaviour.pdf}
}

@article{guerguiev,
  title = {Towards Deep Learning with Segregated Dendrites},
  author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  journal = {eLife},
  volume = {6},
  pages = {e22901},
  issn = {2050-084X},
  doi = {10.7554/eLife.22901},
  urldate = {2025-01-10},
  abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations---the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons., Artificial intelligence has made major progress in recent years thanks to a technique known as deep learning, which works by mimicking the human brain. When computers employ deep learning, they learn by using networks made up of many layers of simulated neurons. Deep learning has opened the door to computers with human -- or even super-human -- levels of skill in recognizing images, processing speech and controlling vehicles. But many neuroscientists are skeptical about whether the brain itself performs deep learning., The patterns of activity that occur in computer networks during deep learning resemble those seen in human brains. But some features of deep learning seem incompatible with how the brain works. Moreover, neurons in artificial networks are much simpler than our own neurons. For instance, in the region of the brain responsible for thinking and planning, most neurons have complex tree-like shapes. Each cell has `roots' deep inside the brain and `branches' close to the surface. By contrast, simulated neurons have a uniform structure., To find out whether networks made up of more realistic simulated neurons could be used to make deep learning more biologically realistic, Guerguiev et al. designed artificial neurons with two compartments, similar to the `roots' and `branches'. The network learned to recognize hand-written digits more easily when it had many layers than when it had only a few. This shows that artificial neurons more like those in the brain can enable deep learning. It even suggests that our own neurons may have evolved their shape to support this process., If confirmed, the link between neuronal shape and deep learning could help us develop better brain-computer interfaces. These allow people to use their brain activity to control devices such as artificial limbs. Despite advances in computing, we are still superior to computers when it comes to learning. Understanding how our own brains show deep learning could thus help us develop better, more human-like artificial intelligence in the future.},
  pmcid = {PMC5716677},
  pmid = {29205151},
  file = {C:\Users\kmc07\Zotero\storage\HU8TD2C3\Guerguiev et al. - Towards deep learning with segregated dendrites.pdf}
}

@misc{ha2016,
  title = {{{HyperNetworks}}},
  author = {Ha, David and Dai, Andrew and Le, Quoc V.},
  year = {2016},
  month = dec,
  number = {arXiv:1609.09106},
  eprint = {1609.09106},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.09106},
  urldate = {2025-02-05},
  abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\42ZLU96A\\Ha et al. - 2016 - HyperNetworks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\IBNKKM3X\\1609.html}
}

@incollection{hadaeghi2021,
  title = {Neuromorphic {{Electronic Systems}} for~{{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Hadaeghi, Fatemeh},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {221--237},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_10},
  urldate = {2024-12-10},
  abstract = {This chapter provides a comprehensive survey of the researches and motivations for hardware implementation of reservoir computing (RC) on neuromorphic electronic systems. Due to its computational efficiency and the fact that training amounts to a simple linear regression, both spiking and non-spiking implementations of reservoir computing on neuromorphic hardware have been developed. Here, a review of these experimental studies is provided to illustrate the progress in this area and to address the technical challenges which arise from this specific hardware implementation. Moreover, to deal with the challenges of computation on such unconventional substrates, several lines of potential solutions are presented based on advances in other computational approaches in machine learning.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@misc{han2020,
  title = {Extension of {{Direct Feedback Alignment}} to {{Convolutional}} and {{Recurrent Neural Network}} for {{Bio-plausible Deep Learning}}},
  author = {Han, Donghyeon and Park, Gwangtae and Ryu, Junha and Yoo, Hoi-jun},
  year = {2020},
  month = jun,
  number = {arXiv:2006.12830},
  eprint = {2006.12830},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.12830},
  urldate = {2025-02-17},
  abstract = {Throughout this paper, we focus on the improvement of the direct feedback alignment (DFA) algorithm and extend the usage of the DFA to convolutional and recurrent neural networks (CNNs and RNNs). Even though the DFA algorithm is biologically plausible and has a potential of high-speed training, it has not been considered as the substitute for back-propagation (BP) due to the low accuracy in the CNN and RNN training. In this work, we propose a new DFA algorithm for BP-level accurate CNN and RNN training. Firstly, we divide the network into several modules and apply the DFA algorithm within the module. Second, the DFA with the sparse backward weight is applied. It comes with a form of dilated convolution in the CNN case, and in a form of sparse matrix multiplication in the RNN case. Additionally, the error propagation method of CNN becomes simpler through the group convolution. Finally, hybrid DFA increases the accuracy of the CNN and RNN training to the BP-level while taking advantage of the parallelism and hardware efficiency of the DFA algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\R79G88PZ\\Han et al. - 2020 - Extension of Direct Feedback Alignment to Convolutional and Recurrent Neural Network for Bio-plausib.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\5NNZAJ3D\\2006.html}
}

@incollection{hauser2021,
  title = {Physical {{Reservoir Computing}} in {{Robotics}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Hauser, Helmut},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {169--190},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_8},
  urldate = {2024-12-10},
  abstract = {In recent years, there has been an increasing interest in using the concept of physical reservoir computing in robotics. The idea is to employ the robot's body and its dynamics as a computational resource. On one hand, this has been driven by the introduction of mathematical frameworks showing how complex mechanical structures can be used to build reservoirs. On the other hand, with the recent advances in smart materials, novel additive manufacturing techniques, and the corresponding rise of soft robotics, a new and much richer set of tools for designing and building robots is now available. Despite the increased interest, however, there is still a wide range of unanswered research questions and a rich area of under-explored applications. We will discuss the current state of the art, the implications of using robot bodies as reservoirs, and the great potential and future directions of physical reservoir computing in robotics.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{hinton1987,
  title = {Using {{Fast Weights}} to {{Deblur Old Memories}}},
  author = {Hinton, Geoffrey E. and Plaut, David C.},
  year = {1987},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {9},
  number = {0},
  urldate = {2024-11-16},
  abstract = {Connectionist models usually have a single weight on each connection. Some interesting newproperties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associationsare "blurred" by subsequent learning, all the original associations can be "deblurred" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel outthe changes in the slow weights caused by the subsequent learning.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\8CVBYZCV\Hinton and Plaut - 1987 - Using Fast Weights to Deblur Old Memories.pdf}
}

@inproceedings{hochreiter2001,
  title = {Learning to {{Learn Using Gradient Descent}}},
  booktitle = {Artificial {{Neural Networks}} --- {{ICANN}} 2001},
  author = {Hochreiter, Sepp and Younger, A. Steven and Conwell, Peter R.},
  editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
  year = {2001},
  pages = {87--94},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44668-0_13},
  abstract = {This paper introduces the application of gradient descent methods to meta-learning. The concept of ``meta-learning'', i.e. of a system that improves or discovers a learning algorithm, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their attendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.},
  isbn = {978-3-540-44668-2},
  langid = {english},
  keywords = {Boolean Function,Gradient Descent,Hide Layer,Learning Algorithm,Turing Machine},
  file = {C:\Users\kmc07\Zotero\storage\YAE6ZWU7\Hochreiter et al. - 2001 - Learning to Learn Using Gradient Descent.pdf}
}

@incollection{inubushi2021,
  title = {On the {{Characteristics}} and {{Structures}} of~{{Dynamical Systems Suitable}} for~{{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Inubushi, Masanobu and Yoshimura, Kazuyuki and Ikeda, Yoshiaki and Nagasawa, Yuto},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {97--116},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_5},
  urldate = {2024-12-10},
  abstract = {We present an overview of mathematical aspects of Reservoir Computing (RC). RC is a machine learning method suitable for physical implementation, which harnesses a type of synchronization, called Common-Signal-Induced Synchronization. A precise criterion for this synchronization is given by a quantity called the conditional Lyapunov exponent. We describe a class of dynamical systems (physical systems) that are utilizable for RC in terms of this quantity. Then, two notions characterizing the information processing performance of RC are illustrated: (i) Edge of Chaos and (ii) Memory-Nonlinearity Trade-off. Based on the notion (ii), a structure of dynamical systems suitable for RC has been proposed. This structure is called the mixture reservoir. We review the structure and show its remarkable information processing performance.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{ji-an2023,
  title = {Face Familiarity Detection with Complex Synapses},
  author = {{Ji-An}, Li and Stefanini, Fabio and Benna, Marcus K. and Fusi, Stefano},
  year = {2023},
  month = jan,
  journal = {iScience},
  volume = {26},
  number = {1},
  pages = {105856},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2022.105856},
  urldate = {2024-11-24},
  abstract = {Synaptic plasticity is a complex phenomenon involving multiple biochemical processes that operate on different timescales. Complexity can greatly increase memory capacity when the variables characterizing the synaptic dynamics have limited precision, as shown in simple memory retrieval problems involving random patterns. Here we turn to a real-world problem, face familiarity detection, and we show that synaptic complexity can be harnessed to store in memory a large number of faces that can be recognized at a later time. The number of recognizable faces grows almost linearly with the number of synapses and quadratically with the number of neurons. Complex synapses outperform simple ones characterized by a single variable, even when the total number of dynamical variables is matched. Complex and simple synapses have distinct signatures that are testable in experiments. Our results indicate that a system with complex synapses can be used in real-world tasks such as face familiarity detection.},
  keywords = {Applied computing,Artificial intelligence,Neuroscience},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\6VCHZD3M\\Ji-An et al. - 2023 - Face familiarity detection with complex synapses.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JYNIVLV7\\S2589004222021290.html}
}

@inproceedings{jin2020,
  title = {How Does {{Weight Correlation Affect Generalisation Ability}} of {{Deep Neural Networks}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jin, Gaojie and Yi, Xinping and Zhang, Liang and Zhang, Lijun and Schewe, Sven and Huang, Xiaowei},
  year = {2020},
  volume = {33},
  pages = {21346--21356},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-02-17},
  abstract = {This paper studies the novel concept of weight correlation in deep neural networks and discusses its impact on the networks' generalisation ability. For fully-connected layers, the weight correlation is defined as the average cosine similarity between weight vectors of neurons, and for convolutional layers, the weight correlation is defined as the cosine similarity between filter matrices. Theoretically, we show that, weight correlation can, and should, be incorporated into the PAC Bayesian framework for the generalisation of neural networks, and the resulting generalisation bound is monotonic with respect to the weight correlation. We formulate a new complexity measure,  which lifts the PAC Bayes measure  with weight correlation, and experimentally confirm that it is able to rank the generalisation errors of a set of networks more precisely than existing measures. More importantly, we develop a new regulariser for training, and provide extensive experiments that show that the generalisation error can be greatly reduced with our novel approach.},
  file = {C:\Users\kmc07\Zotero\storage\TSWEUGRY\Jin et al. - 2020 - How does Weight Correlation Affect Generalisation Ability of Deep Neural Networks.pdf}
}

@article{johansen2011,
  title = {Molecular {{Mechanisms}} of {{Fear Learning}} and {{Memory}}},
  author = {Johansen, Joshua P. and Cain, Christopher K. and Ostroff, Linnaea E. and LeDoux, Joseph E.},
  year = {2011},
  month = oct,
  journal = {Cell},
  volume = {147},
  number = {3},
  pages = {509--524},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2011.10.009},
  urldate = {2025-01-19},
  abstract = {Pavlovian fear conditioning is a particularly useful behavioral paradigm for exploring the molecular mechanisms of learning and memory because a well-defined response to a specific environmental stimulus is produced through associative learning processes. Synaptic plasticity in the lateral nucleus of the amygdala (LA) underlies this form of associative learning. Here, we summarize the molecular mechanisms that contribute to this synaptic plasticity in the context of auditory fear conditioning, the form of fear conditioning best understood at the molecular level. We discuss the neurotransmitter systems and signaling cascades that contribute to three phases of auditory fear conditioning: acquisition, consolidation, and reconsolidation. These studies suggest that multiple intracellular signaling pathways, including those triggered by activation of Hebbian processes and neuromodulatory receptors, interact to produce neural plasticity in the LA and behavioral fear conditioning. Collectively, this body of research illustrates the power of fear conditioning as a model system for characterizing the mechanisms of learning and memory in mammals and potentially for understanding fear-related disorders, such as PTSD and phobias.},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\BUSPK88G\\Johansen et al. - 2011 - Molecular Mechanisms of Fear Learning and Memory.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\B35S8J4K\\S0092867411012074.html}
}

@misc{kaplanis2018c,
  title = {Continual {{Reinforcement Learning}} with {{Complex Synapses}}},
  author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
  year = {2018},
  month = jun,
  number = {arXiv:1802.07239},
  eprint = {1802.07239},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.07239},
  urldate = {2024-11-24},
  abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna \& Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\IUJ5YR56\\Kaplanis et al. - 2018 - Continual Reinforcement Learning with Complex Synapses.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\7PXRAJMH\\1802.html}
}

@misc{lamb2016,
  title = {Professor {{Forcing}}: {{A New Algorithm}} for {{Training Recurrent Networks}}},
  shorttitle = {Professor {{Forcing}}},
  author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = oct,
  number = {arXiv:1610.09038},
  eprint = {1610.09038},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.09038},
  urldate = {2025-02-21},
  abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\TYB5NYBT\\Lamb et al. - 2016 - Professor Forcing A New Algorithm for Training Recurrent Networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\YAM7EJ3G\\1610.html}
}

@misc{le2015,
  title = {A {{Simple Way}} to {{Initialize Recurrent Networks}} of {{Rectified Linear Units}}},
  author = {Le, Quoc V. and Jaitly, Navdeep and Hinton, Geoffrey E.},
  year = {2015},
  month = apr,
  number = {arXiv:1504.00941},
  eprint = {1504.00941},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.00941},
  urldate = {2025-02-21},
  abstract = {Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to a standard implementation of LSTMs on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\kmc07\Zotero\storage\6SBKI28H\Le et al. - 2015 - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.pdf}
}

@article{lillicrap2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year = {2016},
  month = nov,
  journal = {Nature Communications},
  volume = {7},
  pages = {13276},
  issn = {2041-1723},
  doi = {10.1038/ncomms13276},
  urldate = {2025-01-10},
  abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.,  Multi-layered neural architectures that implement learning require elaborate mechanisms for symmetric backpropagation of errors that are biologically implausible. Here the authors propose a simple resolution to this problem of blame assignment that works even with feedback using random synaptic weights.},
  pmcid = {PMC5105169},
  pmid = {27824044},
  keywords = {Learning algorithms},
  file = {C:\Users\kmc07\Zotero\storage\8QGQM4M4\Lillicrap et al. - 2016 - Random synaptic feedback weights support error backpropagation for deep learning.pdf}
}

@misc{lv2024,
  title = {Towards {{Biologically Plausible Computing}}: {{A Comprehensive Comparison}}},
  shorttitle = {Towards {{Biologically Plausible Computing}}},
  author = {Lv, Changze and Gu, Yufei and Guo, Zhengkang and Xu, Zhibo and Wu, Yixin and Zhang, Feiran and Shi, Tianyuan and Wang, Zhenghua and Yin, Ruicheng and Shang, Yu and Zhong, Siqi and Wang, Xiaohua and Wu, Muling and Liu, Wenhao and Li, Tianlong and Zhu, Jianhao and Zhang, Cenyuan and Ling, Zixuan and Zheng, Xiaoqing},
  year = {2024},
  month = jun,
  number = {arXiv:2406.16062},
  eprint = {2406.16062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.16062},
  urldate = {2025-01-19},
  abstract = {Backpropagation is a cornerstone algorithm in training neural networks for supervised learning, which uses a gradient descent method to update network weights by minimizing the discrepancy between actual and desired outputs. Despite its pivotal role in propelling deep learning advancements, the biological plausibility of backpropagation is questioned due to its requirements for weight symmetry, global error computation, and dual-phase training. To address this long-standing challenge, many studies have endeavored to devise biologically plausible training algorithms. However, a fully biologically plausible algorithm for training multilayer neural networks remains elusive, and interpretations of biological plausibility vary among researchers. In this study, we establish criteria for biological plausibility that a desirable learning algorithm should meet. Using these criteria, we evaluate a range of existing algorithms considered to be biologically plausible, including Hebbian learning, spike-timing-dependent plasticity, feedback alignment, target propagation, predictive coding, forward-forward algorithm, perturbation learning, local losses, and energy-based learning. Additionally, we empirically evaluate these algorithms across diverse network architectures and datasets. We compare the feature representations learned by these algorithms with brain activity recorded by non-invasive devices under identical stimuli, aiming to identify which algorithm can most accurately replicate brain activity patterns. We are hopeful that this study could inspire the development of new biologically plausible algorithms for training multilayer networks, thereby fostering progress in both the fields of neuroscience and machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\S4NZ7V4Z\\Lv et al. - 2024 - Towards Biologically Plausible Computing A Comprehensive Comparison.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JHIHBCIU\\2406.html}
}

@misc{marschall2019,
  title = {A {{Unified Framework}} of {{Online Learning Algorithms}} for {{Training Recurrent Neural Networks}}},
  author = {Marschall, Owen and Cho, Kyunghyun and Savin, Cristina},
  year = {2019},
  month = jul,
  number = {arXiv:1907.02649},
  eprint = {1907.02649},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.02649},
  urldate = {2025-03-08},
  abstract = {We present a framework for compactly summarizing many recent results in efficient and/or biologically plausible online training of recurrent neural networks (RNN). The framework organizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. These axes reveal latent conceptual connections among several recent advances in online learning. Furthermore, we provide novel mathematical intuitions for their degree of success. Testing various algorithms on two synthetic tasks shows that performances cluster according to our criteria. Although a similar clustering is also observed for gradient alignment, alignment with exact methods does not alone explain ultimate performance, especially for stochastic algorithms. This suggests the need for better comparison metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\NGSXIIB3\\Marschall et al. - 2019 - A Unified Framework of Online Learning Algorithms for Training Recurrent Neural Networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\MFMJP8BN\\1907.html}
}

@article{metz2019,
  title = {{{META-LEARNING UPDATE RULES FOR UNSUPER- VISED REPRESENTATION LEARNING}}},
  author = {Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and {Sohl-Dickstein}, Jascha},
  year = {2019},
  abstract = {A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm --an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\L86ZJU64\Metz et al. - 2019 - META-LEARNING UPDATE RULES FOR UNSUPER- VISED REPRESENTATION LEARNING.pdf}
}

@misc{moskovitz2019,
  title = {Feedback Alignment in Deep Convolutional Networks},
  author = {Moskovitz, Theodore H. and {Litwin-Kumar}, Ashok and Abbott, L. F.},
  year = {2019},
  month = jun,
  number = {arXiv:1812.06488},
  eprint = {1812.06488},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.06488},
  urldate = {2025-02-04},
  abstract = {Ongoing studies have identified similarities between neural representations in biological networks and in deep artificial neural networks. This has led to renewed interest in developing analogies between the backpropagation learning algorithm used to train artificial networks and the synaptic plasticity rules operative in the brain. These efforts are challenged by biologically implausible features of backpropagation, one of which is a reliance on symmetric forward and backward synaptic weights. A number of methods have been proposed that do not rely on weight symmetry but, thus far, these have failed to scale to deep convolutional networks and complex data. We identify principal obstacles to the scalability of such algorithms and introduce several techniques to mitigate them. We demonstrate that a modification of the feedback alignment method that enforces a weaker form of weight symmetry, one that requires agreement of weight sign but not magnitude, can achieve performance competitive with backpropagation. Our results complement those of Bartunov et al. (2018) and Xiao et al. (2018b) and suggest that mechanisms that promote alignment of feedforward and feedback weights are critical for learning in deep networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\JNKGKBME\\Moskovitz et al. - 2019 - Feedback alignment in deep convolutional networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\VEV2TYQB\\1812.html}
}

@article{murray,
  title = {Local Online Learning in Recurrent Networks with Random Feedback},
  author = {Murray, James M},
  journal = {eLife},
  volume = {8},
  pages = {e43299},
  issn = {2050-084X},
  doi = {10.7554/eLife.43299},
  urldate = {2025-03-08},
  abstract = {Recurrent neural networks (RNNs) enable the production and processing of time-dependent signals such as those involved in movement or working memory. Classic gradient-based algorithms for training RNNs have been available for decades, but are inconsistent with biological features of the brain, such as causality and locality. We derive an approximation to gradient-based learning that comports with these constraints by requiring synaptic weight updates to depend only on local information about pre- and postsynaptic activities, in addition to a random feedback projection of the RNN output error. In addition to providing mathematical arguments for the effectiveness of the new learning rule, we show through simulations that it can be used to train an RNN to perform a variety of tasks. Finally, to overcome the difficulty of training over very large numbers of timesteps, we propose an augmented circuit architecture that allows the RNN to concatenate short-duration patterns into longer sequences.},
  pmcid = {PMC6561704},
  pmid = {31124785},
  file = {C:\Users\kmc07\Zotero\storage\PJZKURU6\Murray - Local online learning in recurrent networks with random feedback.pdf}
}

@book{nakajima2021,
  title = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  shorttitle = {Reservoir {{Computing}}},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  series = {Natural {{Computing Series}}},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6},
  urldate = {2024-12-10},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-981-13-1686-9 978-981-13-1687-6},
  langid = {english},
  keywords = {dynamical system,Machine Learning,Neural Networks,Reservoir Computing,Signal Processing,Soft Robotics,spintronics}
}

@incollection{pathak2021,
  title = {Reservoir {{Computing}} for {{Forecasting Large Spatiotemporal Dynamical Systems}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Pathak, Jaideep and Ott, Edward},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {117--138},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_6},
  urldate = {2024-12-10},
  abstract = {Forecasting of spatiotemporal chaotic dynamical systems is an important problem in several scientific fields. Crucial scientific applications such as weather forecasting and climate modeling depend on the ability to effectively model spatiotemporal chaotic geophysical systems such as the atmosphere and oceans. Recent advances in the field of machine learning have the potential to be an important tool for modeling such systems. In this chapter, we review several key ideas and discuss some reservoir-computing-based architectures for purely data-driven as well as hybrid data-assisted forecasting of chaotic systems with an emphasis on scalability to large, high-dimensional systems.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{richards2019,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and {de Berker}, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Ken and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  year = {2019},
  month = nov,
  journal = {Nature neuroscience},
  volume = {22},
  number = {11},
  pages = {1761--1770},
  issn = {1097-6256},
  doi = {10.1038/s41593-019-0520-2},
  urldate = {2025-01-10},
  abstract = {Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In the case of artificial neural networks, the three components specified by design are the objective functions, the learning rules, and architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.},
  pmcid = {PMC7115933},
  pmid = {31659335},
  file = {C:\Users\kmc07\Zotero\storage\AZHYBG6B\Richards et al. - 2019 - A deep learning framework for neuroscience.pdf}
}

@article{roelfsema2005,
  title = {Attention-{{Gated Reinforcement Learning}} of {{Internal Representations}} for {{Classification}}},
  author = {Roelfsema, Pieter R. and van Ooyen, Arjen},
  year = {2005},
  month = oct,
  journal = {Neural Computation},
  volume = {17},
  number = {10},
  pages = {2176--2214},
  issn = {0899-7667},
  doi = {10.1162/0899766054615699},
  urldate = {2025-01-10},
  abstract = {Animal learning is associated with changes in the efficacy of connections between neurons. The rules that govern this plasticity can be tested in neural networks. Rules that train neural networks to map stimuli onto outputs are given by supervised learning and reinforcement learning theories. Supervised learning is efficient but biologically implausible. In contrast, reinforcement learning is biologically plausible but comparatively inefficient. It lacks a mechanism that can identify units at early processing levels that play a decisive role in the stimulus-response mapping. Here we show that this so-called credit assignment problem can be solved by a new role for attention in learning. There are two factors in our new learning scheme that determine synaptic plasticity: (1) a reinforcement signal that is homogeneous across the network and depends on the amount of reward obtained after a trial, and (2) an attentional feedback signal from the output layer that limits plasticity to those units at earlier processing levels that are crucial for the stimulus-response mapping. The new scheme is called attention-gated reinforcement learning (AGREL). We show that it is as efficient as supervised learning in classification tasks. AGREL is biologically realistic and integrates the role of feedback connections, attention effects, synaptic plasticity, and reinforcement learning signals into a coherent framework.},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\MCP2GY7J\\Roelfsema and Ooyen - 2005 - Attention-Gated Reinforcement Learning of Internal Representations for Classification.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\YUXFACRK\\Attention-Gated-Reinforcement-Learning-of-Internal.html}
}

@misc{sandler2021,
  title = {Meta-{{Learning Bidirectional Update Rules}}},
  author = {Sandler, Mark and Vladymyrov, Max and Zhmoginov, Andrey and Miller, Nolan and Jackson, Andrew and Madams, Tom and y Arcas, Blaise Aguera},
  year = {2021},
  month = jun,
  number = {arXiv:2104.04657},
  eprint = {2104.04657},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional "genome". We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\5DLI3HPT\\Sandler et al. - 2021 - Meta-Learning Bidirectional Update Rules.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\TQ5WGHBF\\2104.html}
}

@misc{scellier2017,
  title = {Equilibrium {{Propagation}}: {{Bridging}} the {{Gap Between Energy-Based Models}} and {{Backpropagation}}},
  shorttitle = {Equilibrium {{Propagation}}},
  author = {Scellier, Benjamin and Bengio, Yoshua},
  year = {2017},
  month = mar,
  number = {arXiv:1602.05179},
  eprint = {1602.05179},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.05179},
  urldate = {2025-01-10},
  abstract = {We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point, or stationary distribution) towards a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged towards their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal 'back-propagated' during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\R4RRBU73\\Scellier and Bengio - 2017 - Equilibrium Propagation Bridging the Gap Between Energy-Based Models and Backpropagation.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\Z95MXV3A\\1602.html}
}

@article{shervani-tabar2023,
  title = {Meta-Learning Biologically Plausible Plasticity Rules with Random Feedback Pathways},
  author = {{Shervani-Tabar}, Navid and Rosenbaum, Robert},
  year = {2023},
  month = mar,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {1805},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-37562-1},
  urldate = {2024-09-21},
  abstract = {Backpropagation is widely used to train artificial neural networks, but its relationship to synaptic plasticity in the brain is unknown. Some biological models of backpropagation rely on feedback projections that are symmetric with feedforward connections, but experiments do not corroborate the existence of such symmetric backward connectivity. Random feedback alignment offers an alternative model in which errors are propagated backward through fixed, random backward connections. This approach successfully trains shallow models, but learns slowly and does not perform well with deeper models or online learning. In this study, we develop a meta-learning approach to discover interpretable, biologically plausible plasticity rules that improve online learning performance with fixed random feedback connections. The resulting plasticity rules show improved online training of deep models in the low data regime. Our results highlight the potential of meta-learning to discover effective, interpretable learning rules satisfying biological constraints.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational science,Learning algorithms},
  file = {C:\Users\kmc07\Zotero\storage\E6MFRN9F\Shervani-Tabar and Rosenbaum - 2023 - Meta-learning biologically plausible plasticity rules with random feedback pathways.pdf}
}

@incollection{singer2021,
  title = {The {{Cerebral Cortex}}: {{A Delay-Coupled Recurrent Oscillator Network}}?},
  shorttitle = {The {{Cerebral Cortex}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Singer, Wolf},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {3--28},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_1},
  urldate = {2024-12-10},
  abstract = {The refinement of machine learning strategies and deep convolutional networks led to the development of artificial systems whose functions resemble those of natural brains, suggesting that the two systems share the same computational principles. In this chapter, evidence is reviewed which indicates that the computational operations of natural systems differ in some important aspects from those implemented in artificial systems. Natural processing architectures are characterized by recurrence and therefore exhibit high-dimensional, non-linear dynamics. Moreover, they use learning mechanisms that support self-organization. It is proposed that these properties allow for computations that are notoriously difficult to realize in artificial systems. Experimental evidence on the organization and function of the cerebral cortex is reviewed that supports this proposal.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{soo2023,
  title = {Training Biologically Plausible Recurrent Neural Networks on Cognitive Tasks with Long-Term Dependencies},
  author = {Soo, Wayne W.M. and Goudar, Vishwa and Wang, Xiao-Jing},
  year = {2023},
  month = oct,
  journal = {bioRxiv},
  pages = {2023.10.10.561588},
  doi = {10.1101/2023.10.10.561588},
  urldate = {2025-04-10},
  abstract = {Training recurrent neural networks (RNNs) has become a go-to approach for generating and evaluating mechanistic neural hypotheses for cognition. The ease and efficiency of training RNNs with backpropagation through time and the availability of robustly supported deep learning libraries has made RNN modeling more approachable and accessible to neuroscience. Yet, a major technical hindrance remains. Cognitive processes such as working memory and decision making involve neural population dynamics over a long period of time within a behavioral trial and across trials. It is difficult to train RNNs to accomplish tasks where neural representations and dynamics have long temporal dependencies without gating mechanisms such as LSTMs or GRUs which currently lack experimental support and prohibit direct comparison between RNNs and biological neural circuits. We tackled this problem based on the idea of specialized skip-connections through time to support the emergence of task-relevant dynamics, and subsequently reinstitute biological plausibility by reverting to the original architecture. We show that this approach enables RNNs to successfully learn cognitive tasks that prove impractical if not impossible to learn using conventional methods. Over numerous tasks considered here, we achieve less training steps and shorter wall-clock times, particularly in tasks that require learning long-term dependencies via temporal integration over long timescales or maintaining a memory of past events in hidden-states. Our methods expand the range of experimental tasks that biologically plausible RNN models can learn, thereby supporting the development of theory for the emergent neural mechanisms of computations involving long-term dependencies.},
  pmcid = {PMC10592728},
  pmid = {37873445},
  file = {C:\Users\kmc07\Zotero\storage\YNLZQUBM\Soo et al. - 2023 - Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependen.pdf}
}

@incollection{subramoney2021,
  title = {Reservoirs {{Learn}} to {{Learn}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Subramoney, Anand and Scherr, Franz and Maass, Wolfgang},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {59--76},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_3},
  urldate = {2024-12-10},
  abstract = {The common procedure in reservoir computing is to take a ``found'' reservoir, such as a recurrent neural network with randomly chosen synaptic weights or a complex physical device, and to adapt the weights of linear readouts from this reservoir for a particular computing task. We address the question of whether the performance of reservoir computing can be significantly enhanced if one instead optimizes some (hyper)parameters of the reservoir, not for a single task but for the range of all possible tasks in which one is potentially interested, before the weights of linear readouts are optimized for a particular computing task. After all, networks of neurons in the brain are also known to be not randomly connected. Rather, their structure and parameters emerge from complex evolutionary and developmental processes, arguably in a way that enhances the speed and accuracy of subsequent learning of any concrete task that is likely to be essential for the survival of the organism. We apply the Learning-to-Learn (L2L) paradigm to mimic this two-tier process, where a set of (hyper)parameters of the reservoir are optimized for a whole family of learning tasks. We found that this substantially enhances the performance of reservoir computing for the families of tasks that we considered. Furthermore, L2L enables a new form of reservoir learning that tends to enable even faster learning, where not even the weights of readouts need to be adjusted for learning a concrete task. We present demos and performance results of these new forms of reservoir computing for reservoirs that consist of networks of spiking neurons and are hence of particular interest from the perspective of neuroscience and implementations in spike-based neuromorphic hardware. We leave it as an open question of what performance advantage the new methods that we propose provide for other types of reservoirs.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{sudhof2008,
  title = {Understanding {{Synapses}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Understanding {{Synapses}}},
  author = {S{\"u}dhof, Thomas C. and Malenka, Robert C.},
  year = {2008},
  month = nov,
  journal = {Neuron},
  volume = {60},
  number = {3},
  pages = {469--476},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.10.011},
  urldate = {2025-01-19},
  abstract = {Classical physiological work by Katz, Eccles, and others revealed the central importance of synapses in brain function, and characterized the mechanisms involved in synaptic transmission. Building on this work, major advances in the past two decades have elucidated how synapses work molecularly. In the present perspective, we provide a short description of our personal view of these advances, suggest a series of important future questions about synapses, and discuss ideas about how best to achieve further progress in the field.},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\IG65ZT3G\\S√ºdhof and Malenka - 2008 - Understanding Synapses Past, Present, and Future.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\9RSHPHNI\\S0896627308008842.html}
}

@article{szu1987,
  title = {Fast Simulated Annealing},
  author = {Szu, Harold and Hartley, Ralph},
  year = {1987},
  month = jun,
  journal = {Physics Letters A},
  volume = {122},
  number = {3},
  pages = {157--162},
  issn = {0375-9601},
  doi = {10.1016/0375-9601(87)90796-1},
  urldate = {2025-01-19},
  abstract = {Simulated annealing is a stochastic strategy for searching the ground state. A fast simulated annealing (FSA) is a semi-local search and consists of occasional long jumps. The cooling schedule of the FSA algorithm is inversely linear in time which is fast compared with the classical simulated annealing (CSA) which is strictly a local search and requires the cooling schedule to be inversely proportional to the logarithmic function of time. A general D-dimensional Cauchy probability for generating the state is given. Proofs for both FSA and CSA are sketched. A double potential well is used to numerically illustrate both schemes.},
  file = {C:\Users\kmc07\Zotero\storage\NGZ7XIMY\0375960187907961.html}
}

@article{tanaka2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2019},
  month = jul,
  journal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.005},
  urldate = {2024-12-10},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  keywords = {Machine learning,Neural networks,Neuromorphic device,Nonlinear dynamical systems,Reservoir computing}
}

@article{tanaka2020,
  title = {Spatially {{Arranged Sparse Recurrent Neural Networks}} for {{Energy Efficient Associative Memory}}},
  author = {Tanaka, Gouhei and Nakane, Ryosho and Takeuchi, Tomoya and Yamane, Toshiyuki and Nakano, Daiju and Katayama, Yasunao and Hirose, Akira},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {1},
  pages = {24--38},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2899344},
  urldate = {2024-12-10},
  abstract = {The development of hardware neural networks, including neuromorphic hardware, has been accelerated over the past few years. However, it is challenging to operate very large-scale neural networks with low-power hardware devices, partly due to signal transmissions through a massive number of interconnections. Our aim is to deal with the issue of communication cost from an algorithmic viewpoint and study learning algorithms for energy-efficient information processing. Here, we consider two approaches to finding spatially arranged sparse recurrent neural networks with the high cost-performance ratio for associative memory. In the first approach following classical methods, we focus on sparse modular network structures inspired by biological brain networks and examine their storage capacity under an iterative learning rule. We show that incorporating long-range intermodule connections into purely modular networks can enhance the cost-performance ratio. In the second approach, we formulate for the first time an optimization problem where the network sparsity is maximized under the constraints imposed by a pattern embedding condition. We show that there is a tradeoff between the interconnection cost and the computational performance in the optimized networks. We demonstrate that the optimized networks can achieve a better cost-performance ratio compared with those considered in the first approach. We show the effectiveness of the optimization approach mainly using binary patterns and apply it also to gray-scale image restoration. Our results suggest that the presented approaches are useful in seeking more sparse and less costly connectivity of neural networks for the enhancement of energy efficiency in hardware neural networks.},
  keywords = {Associative memory,Biological neural networks,energy efficiency,Hardware,Hopfield network,Integrated circuit interconnections,interconnection cost,iterative learning rule,Neurons,Optimization,Recurrent neural networks,sparse neural networks,sparse optimization}
}

@inproceedings{tong2018,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Tong, Zhiqiang and Tanaka, Gouhei},
  year = {2018},
  month = aug,
  pages = {1289--1294},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2018.8545471},
  urldate = {2024-12-10},
  abstract = {Reservoir computing has attracted much attention for its easy training process as well as its ability to deal with temporal data. A reservoir computing system consists of a reservoir part represented as a sparsely connected recurrent neural network and a readout part represented as a simple regression model. In machine learning tasks, the reservoir part is fixed and only the readout part is trained. Although reservoir computing has been mainly applied to time series prediction and recognition, it can be applied to image recognition as well by considering an image data as a sequence of pixel values. However, to achieve a high performance in image recognition with raw image data, a large-scale reservoir including a large number of neurons is required. This is a bottleneck in terms of computer memory and computational cost. To overcome this bottleneck, we propose a new method which combines reservoir computing with untrained convolutional neural networks. We use an untrained convolutional neural network to transform raw image data into a set of smaller feature maps in a preprocessing step of the reservoir computing. We demonstrate that our method achieves a high classification accuracy in an image recognition task with a much smaller number of trainable parameters compared with a previous study.},
  keywords = {Computational modeling,Convolution,Convolutional neural networks,Feature extraction,Image recognition,Reservoirs,Training},
  file = {C:\Users\kmc07\Zotero\storage\J837JLDM\8545471.html}
}

@article{wang2005,
  title = {Phosphorylation of {{AMPA}} Receptors},
  author = {Wang, John Q. and Arora, Anish and Yang, Lu and Parelkar, Nikhil K. and Zhang, Guochi and Liu, Xianyu and Choe, Eun Sang and Mao, Limin},
  year = {2005},
  month = dec,
  journal = {Molecular Neurobiology},
  volume = {32},
  number = {3},
  pages = {237--249},
  issn = {1559-1182},
  doi = {10.1385/MN:32:3:237},
  urldate = {2025-01-19},
  abstract = {The ionotropic {$\alpha$}-amino-3-hydroxy-5-methylisoxazole-4-propionic acid (AMPA) receptor is densely distributed in the mammalian brain and is primarily involved in mediating fast excitatory synaptic transmission. Recent studies in both heterologous expression systems and cultured neurons have shown that the AMPA receptor can be phosphorylated on their subunits (GluR1, GluR2, and GluR4). All phosphorylation sites reside at serine, threonine, or tyrosine on the intracellular C-terminal domain. Several key protein kinases, such as protein kinase A, protein kinase C, Ca2+/calmodulin-dependent protein kinase II, and tyrosine kinases (Trks; receptor or nonreceptor family Trks) are involved in the site-specific regulation of the AMPA receptor phosphorylation. Other glutamate receptors (N-methyl-d-aspartate receptors and metabotropic glutamate receptors) also regulate AMPA receptors through a protein phosphorylation mechanism. Emerging evidence shows that as a rapid and short-term mechanism, the dynamic protein phosphorylation directly modulates the electrophysiological, morphological (externalization and internalization trafficking and clustering), and biochemical (synthesis and subunit composition) properties of the AMPA receptor, as well as protein-protein interactions between the AMPA receptor subunits and various intracellular interacting proteins. These modulations underlie the major molecular mechanisms that ultimately affect many forms of synaptic plasticity.},
  langid = {english},
  keywords = {dopamine,GluR,Glutamate,kainate,mGluR,N-methyl-d-aspartate (NMDA),serine,striatum,tyrosine},
  file = {C:\Users\kmc07\Zotero\storage\4IIZCCMQ\Wang et al. - 2005 - Phosphorylation of AMPA receptors.pdf}
}

@article{yildiz2012,
  title = {Re-Visiting the Echo State Property},
  author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
  year = {2012},
  month = nov,
  journal = {Neural Networks},
  volume = {35},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.07.005},
  urldate = {2024-12-15},
  abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  keywords = {Bifurcation,Diagonally Schur stable,Echo state network,Lyapunov,Spectral radius},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\DVGN65IF\\Yildiz et al. - 2012 - Re-visiting the echo state property.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\SXTS6G84\\S0893608012001852.html}
}

@misc{zotero-512,
  title = {Cascade {{Models}} of {{Synaptically Stored Memories}} - {{ScienceDirect}}},
  urldate = {2024-11-28},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0896627305001170},
  file = {C:\Users\kmc07\Zotero\storage\CV6YNLRM\S0896627305001170.html}
}

@misc{zotero-569,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-12-10},
  howpublished = {https://ieeexplore.ieee.org/document/8545471},
  file = {C:\Users\kmc07\Zotero\storage\I3NV782M\8545471.html}
}

@misc{zotero-570,
  title = {Scopus - {{Document}} Details - {{Information}} Processing Using a Single Dynamical Node as Complex System},
  doi = {10.1038/ncomms1476},
  urldate = {2024-12-10},
  abstract = {Elsevier's Scopus, the largest abstract and citation database of peer-reviewed literature. Search and access research from the science, technology, medicine, social sciences and arts and humanities fields.},
  howpublished = {https://www.scopus.com/record/display.uri?eid=2-s2.0-80053397808\&origin=inward\&txGid=76e2085e73e050207a9891823415f4ab},
  langid = {american}
}

@misc{zotero-612,
  title = {Principles of {{Neural Science}}, {{Fifth Edition}}},
  journal = {McGraw Hill Medical},
  urldate = {2025-01-19},
  abstract = {Read this chapter of Principles of Neural Science, Fifth Edition online now, exclusively on AccessNeurology. AccessNeurology is a subscription-based resource from McGraw Hill that features trusted medical content from the best minds in medicine.},
  howpublished = {https://neurology.mhmedical.com/content.aspx?sectionid=59138139\&bookid=1049},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\NW5UIWEU\content.html}
}

@misc{zotero-628,
  title = {Fast Simulated Annealing},
  journal = {ResearchGate},
  urldate = {2025-01-19},
  abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
  howpublished = {https://www.researchgate.net/publication/223828956\_Fast\_simulated\_annealing},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\676HC52A\223828956_Fast_simulated_annealing.html}
}

@misc{zotero-642,
  title = {Evolutionary {{Principles}} in {{Self-Referential Learning}}. {{On Learning}} Now to {{Learn}}: {{The Meta-Meta-Meta}}...-{{Hook}} {\textbar} {{BibSonomy}}},
  urldate = {2025-01-23},
  howpublished = {https://www.bibsonomy.org/bibtex/2a96f7c3d42103ab94b13badef5d869f0/brazovayeye},
  file = {C:\Users\kmc07\Zotero\storage\EDZX8YHX\brazovayeye.html}
}

@misc{zotero-643,
  title = {1987 {{THESIS ON LEARNING HOW TO LEARN}}, {{METALEARNING}}, {{META GENETIC PROGRAMMING}}, {{CREDIT-CONSERVING MACHINE LEARNING ECONOMY}}},
  urldate = {2025-01-23},
  howpublished = {https://people.idsia.ch/{\textasciitilde}juergen/diploma.html},
  file = {C:\Users\kmc07\Zotero\storage\YHDM6G78\diploma.html}
}

@misc{zotero-653,
  title = {Papers with {{Code}} - {{Discriminative Fine-Tuning Explained}}},
  urldate = {2025-02-21},
  abstract = {Discriminative Fine-Tuning is a fine-tuning strategy that is used for ULMFiT type models. Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. For context, the regular stochastic gradient descent (SGD) update of a model's parameters \${\textbackslash}theta\$ at time step \$t\$ looks like the following (Ruder, 2016): \$\$ {\textbackslash}theta\_\{t\} = {\textbackslash}theta\_\{t-1\} - {\textbackslash}eta{\textbackslash}cdot{\textbackslash}nabla\_\{{\textbackslash}theta\}J{\textbackslash}left({\textbackslash}theta{\textbackslash}right)\$\$ where \${\textbackslash}eta\$ is the learning rate and \${\textbackslash}nabla\_\{{\textbackslash}theta\}J{\textbackslash}left({\textbackslash}theta{\textbackslash}right)\$ is the gradient with regard to the model's objective function. For discriminative fine-tuning, we split the parameters \${\textbackslash}theta\$ into \{\${\textbackslash}theta\_\{1\}, {\textbackslash}ldots, {\textbackslash}theta\_\{L\}\$\} where \${\textbackslash}theta\_\{l\}\$ contains the parameters of the model at the \$l\$-th layer and \$L\$ is the number of layers of the model. Similarly, we obtain \{\${\textbackslash}eta\_\{1\}, {\textbackslash}ldots, {\textbackslash}eta\_\{L\}\$\} where \${\textbackslash}theta\_\{l\}\$ where \${\textbackslash}eta\_\{l\}\$ is the learning rate of the \$l\$-th layer. The SGD update with discriminative finetuning is then: \$\$ {\textbackslash}theta\_\{t\}{\textasciicircum}\{l\} = {\textbackslash}theta\_\{t-1\}{\textasciicircum}\{l\} - {\textbackslash}eta{\textasciicircum}\{l\}{\textbackslash}cdot{\textbackslash}nabla\_\{{\textbackslash}theta{\textasciicircum}\{l\}\}J{\textbackslash}left({\textbackslash}theta{\textbackslash}right) \$\$ The authors find that empirically it worked well to first choose the learning rate \${\textbackslash}eta{\textasciicircum}\{L\}\$ of the last layer by fine-tuning only the last layer and using \${\textbackslash}eta{\textasciicircum}\{l-1\}={\textbackslash}eta{\textasciicircum}\{l\}/2.6\$ as the learning rate for lower layers.},
  howpublished = {https://paperswithcode.com/method/discriminative-fine-tuning},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\8BEQE79E\discriminative-fine-tuning.html}
}
