@incollection{aguilera2021,
  title = {Programmable {{Fading Memory}} in {{Atomic Switch Systems}} for {{Error Checking Applications}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Aguilera, Renato and Sillin, Henry O. and Stieg, Adam Z. and Gimzewski, James K.},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {273--303},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_12},
  urldate = {2024-12-10},
  abstract = {Disruptive technology in computational devices is required as the universal computing machines approach quantum mechanical limits. Integration of state-of-the-art memristive devices provides optimal scaling of current technologies beyond this limit through the adoption of neuromorphic models. Universal computing machines pioneered by Alan Turing are strictly based on top-down intelligent design. Neuromorphic models instead engage in bottom-up programmability by emulating mammalian brain design and characteristics. Here we show the design, characterization, and implementation of a massively parallel memristor neuromorphic network based on metal chalcogenide atomic switch network (ASN) systems with key characteristics such as short- and long-term potentiation, power-law dynamics, and scale-free topology.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@misc{ahmad2020,
  title = {{{GAIT-prop}}: {{A}} Biologically Plausible Learning Rule Derived from Backpropagation of Error},
  shorttitle = {{{GAIT-prop}}},
  author = {Ahmad, Nasir and van Gerven, Marcel A. J. and Ambrogioni, Luca},
  year = {2020},
  month = nov,
  number = {arXiv:2006.06438},
  eprint = {2006.06438},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {Traditional backpropagation of error, though a highly successful algorithm for learning in artificial neural network models, includes features which are biologically implausible for learning in real neural circuits. An alternative called target propagation proposes to solve this implausibility by using a top-down model of neural activity to convert an error at the output of a neural network into layer-wise and plausible 'targets' for every unit. These targets can then be used to produce weight updates for network training. However, thus far, target propagation has been heuristically proposed without demonstrable equivalence to backpropagation. Here, we derive an exact correspondence between backpropagation and a modified form of target propagation (GAIT-prop) where the target is a small perturbation of the forward pass. Specifically, backpropagation and GAIT-prop give identical updates when synaptic weight matrices are orthogonal. In a series of simple computer vision experiments, we show near-identical performance between backpropagation and GAIT-prop with a soft orthogonality-inducing regularizer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\8EXS5WIW\\Ahmad et al. - 2020 - GAIT-prop A biologically plausible learning rule derived from backpropagation of error.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\6ZKLIF72\\2006.html}
}

@article{akella2021,
  title = {Reward Based {{Hebbian Learning}} in {{Direct Feedback Alignment}} ({{Student Abstract}})},
  author = {Akella, Ashlesha and Singanamalla, Sai Kalyan Ranga and Lin, Chin-Teng},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {18},
  pages = {15749--15750},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i18.17871},
  urldate = {2025-06-02},
  abstract = {Imparting biological realism during the learning process is gaining attention towards producing computationally efficient algorithms without compromising the performance. Feedback alignment and mirror neuron concept are two such approaches where the feedback weight remains static in the former and update via Hebbian learning in the later. Though these approaches have proven to work efficiently for supervised learning, it remained unknown if the same can be applicable to reinforcement learning applications. Therefore, this study introduces RHebb-DFA where the reward-based Hebbian learning is used to update feedback weights in direct feedback alignment mode. This approach is validated on various Atari games and obtained equivalent performance in comparison with DDQN.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\W7FBMBFT\Akella et al. - 2021 - Reward based Hebbian Learning in Direct Feedback Alignment (Student Abstract).pdf}
}

@inproceedings{akrout2019,
  title = {Deep {{Learning}} without {{Weight Transport}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter and Lillicrap, Timothy and Tweed, Douglas B},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-02},
  abstract = {Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms --- a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 --- both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring. Tested on the ImageNet visual-recognition task, these mechanisms outperform both feedback alignment and the newer sign-symmetry method, and nearly match backprop, the standard algorithm of deep learning, which uses weight transport.},
  file = {C:\Users\kmc07\Zotero\storage\XKRTWVBM\Akrout et al. - 2019 - Deep Learning without Weight Transport.pdf}
}

@misc{akrout2020,
  title = {Deep {{Learning}} without {{Weight Transport}}},
  author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter C. and Lillicrap, Timothy and Tweed, Douglas},
  year = {2020},
  month = jan,
  number = {arXiv:1904.05391},
  eprint = {1904.05391},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.05391},
  urldate = {2025-02-04},
  abstract = {Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms - a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 - both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring.Tested on the ImageNet visual-recognition task, these mechanisms outperform both feedback alignment and the newer sign-symmetry method, and nearly match backprop, the standard algorithm of deep learning, which uses weight transport.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\G6JHVPTZ\\Akrout et al. - 2020 - Deep Learning without Weight Transport.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\43EUVEXQ\\1904.html}
}

@article{amit2019,
  title = {Deep {{Learning With Asymmetric Connections}} and {{Hebbian Updates}}},
  author = {Amit, Yali},
  year = {2019},
  month = apr,
  journal = {Frontiers in Computational Neuroscience},
  volume = {13},
  publisher = {Frontiers},
  issn = {1662-5188},
  doi = {10.3389/fncom.2019.00018},
  urldate = {2025-05-23},
  abstract = {We show that deep networks can be trained using Hebbian updates yielding similar performance to ordinary back-propagation on challenging image datasets. To overcome the unrealistic symmetry in connections between layers, implicit in back-propagation, the feedback weights are separate from the feedforward weights. The feedback weights are also updated with a local rule, the same as the feedforward weights---a weight is updated solely based on the product of activity of the units it connects. With fixed feedback weights as proposed in Lillicrap et al. (2016) performance degrades quickly as the depth of the network increases. If the feedforward and feedback weights are initialized with the same values, as proposed in Zipser and Rumelhart (1990), they remain the same throughout training thus precisely implementing back-propagation. We show that even when the weights are initialized differently and at random, and the algorithm is no longer performing back-propagation, performance is comparable on challenging datasets. We also propose a cost function whose derivative can be represented as a local Hebbian update on the last layer. Convolutional layers are updated with tied weights across space, which is not biologically plausible. We show that similar performance is achieved with untied layers, also known as locally connected layers, corresponding to the connectivity implied by the convolutional layers, but where weights are untied and updated separately. In the linear case we show theoretically that the convergence of the error to zero is accelerated by the update of the feedback weights.},
  langid = {english},
  keywords = {Asymmetric backpropoagation,Convolutional networks,feedback connections,Hebbian Learning,Hinge loss},
  file = {C:\Users\kmc07\Zotero\storage\7Z3J2WSS\Amit - 2019 - Deep Learning With Asymmetric Connections and Hebbian Updates.pdf}
}

@article{appeltant2011,
  title = {Information Processing Using a Single Dynamical Node as Complex System},
  author = {Appeltant, L. and Soriano, M.C. and Van Der Sande, G. and Danckaert, J. and Massar, S. and Dambre, J. and Schrauwen, B. and Mirasso, C.R. and Fischer, I.},
  year = {2011},
  month = sep,
  journal = {Nature Communications},
  volume = {2},
  number = {1},
  pages = {468},
  issn = {2041-1723},
  doi = {10.1038/ncomms1476},
  urldate = {2024-12-10},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\N9DT9PHC\Appeltant et al. - 2011 - Information processing using a single dynamical node as complex system.pdf}
}

@misc{ba2016,
  title = {Using {{Fast Weights}} to {{Attend}} to the {{Recent Past}}},
  author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
  year = {2016},
  month = dec,
  number = {arXiv:1610.06258},
  eprint = {1610.06258},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.06258},
  urldate = {2024-12-02},
  abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\7JX4R85U\\Ba et al. - 2016 - Using Fast Weights to Attend to the Recent Past.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\MKJYDGBJ\\1610.html}
}

@article{badia-i-mompel2023,
  title = {Gene Regulatory Network Inference in the Era of Single-Cell Multi-Omics},
  author = {{Badia-i-Mompel}, Pau and Wessels, Lorna and {M{\"u}ller-Dott}, Sophia and Trimbour, R{\'e}mi and Ramirez Flores, Ricardo O. and Argelaguet, Ricard and {Saez-Rodriguez}, Julio},
  year = {2023},
  month = nov,
  journal = {Nature Reviews Genetics},
  volume = {24},
  number = {11},
  pages = {739--754},
  publisher = {Nature Publishing Group},
  issn = {1471-0064},
  doi = {10.1038/s41576-023-00618-5},
  urldate = {2025-06-02},
  abstract = {The interplay between chromatin, transcription factors and genes generates complex regulatory circuits that can be represented as gene regulatory networks (GRNs). The study of GRNs is useful to understand how cellular identity is established, maintained and disrupted in disease. GRNs can be inferred from experimental data --- historically, bulk omics data --- and/or from the literature. The advent of single-cell multi-omics technologies has led to the development of novel computational methods that leverage genomic, transcriptomic and chromatin accessibility information to infer GRNs at an unprecedented resolution. Here, we review the key principles of inferring GRNs that encompass transcription factor--gene interactions from transcriptomics and chromatin accessibility data. We focus on the comparison and classification of methods that use single-cell multimodal data. We highlight challenges in GRN inference, in particular with respect to benchmarking, and potential further developments using additional data modalities.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Gene regulatory networks,Regulatory networks},
  file = {C:\Users\kmc07\Zotero\storage\8TVNQBP8\Badia-i-Mompel et al. - 2023 - Gene regulatory network inference in the era of single-cell multi-omics.pdf}
}

@misc{bartunov2018,
  title = {Assessing the {{Scalability}} of {{Biologically-Motivated Deep Learning Algorithms}} and {{Architectures}}},
  author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake A. and Marris, Luke and Hinton, Geoffrey E. and Lillicrap, Timothy},
  year = {2018},
  month = nov,
  number = {arXiv:1807.04587},
  eprint = {1807.04587},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1807.04587},
  urldate = {2025-06-02},
  abstract = {The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\RCZBG69B\\Bartunov et al. - 2018 - Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\XXQHF2QY\\1807.html}
}

@article{bendayanrubin2007,
  title = {Long Memory Lifetimes Require Complex Synapses and Limited Sparseness},
  author = {Ben Dayan Rubin, Daniel D. and Fusi, Stefano},
  year = {2007},
  month = nov,
  journal = {Frontiers in Computational Neuroscience},
  volume = {1},
  publisher = {Frontiers},
  issn = {1662-5188},
  doi = {10.3389/neuro.10.007.2007},
  urldate = {2024-11-28},
  abstract = {{$<$}p{$>$}Theoretical studies have shown that memories last longer if the neural representations are sparse, that is, when each neuron is selective for a small fraction of the events creating the memories. Sparseness reduces both the interference between stored memories and the number of synaptic modifications which are necessary for memory storage. Paradoxically, in cortical areas like the inferotemporal cortex, where presumably memory lifetimes are longer than in the medial temporal lobe, neural representations are less sparse. We resolve this paradox by analyzing the effects of sparseness on complex models of synaptic dynamics in which there are metaplastic states with different degrees of plasticity. For these models, memory retention in a large number of synapses across multiple neurons is significantly more efficient in case of many metaplastic states, that is, for an elevated degree of complexity. In other words, larger brain regions allow to retain memories for significantly longer times only if the synaptic complexity increases with the total number of synapses. However, the initial memory trace, the one experienced immediately after memory storage, becomes weaker both when the number of metaplastic states increases and when the neural representations become sparser. Such a memory trace must be above a given threshold in order to permit every single neuron to retrieve the information stored in its synapses. As a consequence, if the initial memory trace is reduced because of the increased synaptic complexity, then the neural representations must be less sparse. We conclude that long memory lifetimes allowed by a larger number of synapses require more complex synapses, and hence, less sparse representations, which is what is observed in the brain.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Learning,sparseness,synaptic plasticity},
  file = {C:\Users\kmc07\Zotero\storage\BJYWGE8F\Ben Dayan Rubin and Fusi - 2007 - Long memory lifetimes require complex synapses and limited sparseness.pdf}
}

@article{bengio,
  title = {On the {{Optimization}} of a {{Synaptic Learning Rule}}},
  author = {Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\4SMDED63\Bengio et al. - On the Optimization of a Synaptic Learning Rule.pdf}
}

@inproceedings{bengio1991,
  title = {Learning a Synaptic Learning Rule},
  booktitle = {{{IJCNN-91-Seattle International Joint Conference}} on {{Neural Networks}}},
  author = {Bengio, Y. and Bengio, S. and Cloutier, J.},
  year = {1991},
  month = jul,
  volume = {ii},
  pages = {969 vol.2-},
  doi = {10.1109/IJCNN.1991.155621},
  urldate = {2024-10-18},
  abstract = {Summary form only given, as follows. The authors discuss an original approach to neural modeling based on the idea of searching, with learning methods, for a synaptic learning rule which is biologically plausible and yields networks that are able to learn to perform difficult tasks. The proposed method of automatically finding the learning rule relies on the idea of considering the synaptic modification rule as a parametric function. This function has local inputs and is the same in many neurons. The parameters that define this function can be estimated with known learning methods. For this optimization, particular attention is given to gradient descent and genetic algorithms. In both cases, estimation of this function consists of a joint global optimization of the synaptic modification function and the networks that are learning to perform some tasks. Both network architecture and the learning function can be designed within constraints derived from biological knowledge.{$<>$}},
  keywords = {Biological system modeling,Biology,Computer science,Genetic algorithms,Learning systems,Neurons},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\AXCLBLYX\\Bengio et al. - 1991 - Learning a synaptic learning rule.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JZ6EYSBC\\Bengio et al. - 1991 - Learning a synaptic learning rule.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JLDA4IE5\\155621.html}
}

@inproceedings{bengio1994,
  title = {Use of Genetic Programming for the Search of a New Learning Rule for Neural Networks},
  booktitle = {Proceedings of the {{First IEEE Conference}} on {{Evolutionary Computation}}. {{IEEE World Congress}} on {{Computational Intelligence}}},
  author = {Bengio, S. and Bengio, Y. and Cloutier, J.},
  year = {1994},
  month = jun,
  pages = {324-327 vol.1},
  doi = {10.1109/ICEC.1994.349932},
  urldate = {2024-10-17},
  abstract = {In previous work we explained how to use standard optimization methods such as simulated annealing, gradient descent and genetic algorithms to optimize a parametric function which could be used as a learning rule for neural networks. To use these methods, we had to choose a fixed number of parameters and a rigid form for the learning rule. In this article, we propose to use genetic programming to find not only the values of rule parameters but also the optimal number of parameters and the form of the rule. Experiments on classification tasks suggest genetic programming finds better learning rules than other optimization methods. Furthermore, the best rule found with genetic programming outperformed the well-known backpropagation algorithm for a given set of tasks.{$<>$}},
  keywords = {Backpropagation algorithms,Biological system modeling,Design optimization,Genetic algorithms,Genetic programming,Learning systems,Neural networks,Neurons,Optimization methods,Simulated annealing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\Y83VM8RA\\Bengio et al. - 1994 - Use of genetic programming for the search of a new learning rule for neural networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\48L99IW7\\349932.html}
}

@inproceedings{bengio2006,
  title = {Greedy {{Layer-Wise Training}} of {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2006},
  volume = {19},
  publisher = {MIT Press},
  urldate = {2024-10-10},
  abstract = {Recent analyses (Bengio, Delalleau, \& Le Roux, 2006; Bengio \& Le Cun, 2007) of modern nonparametric machine learning algorithms that are kernel machines, such as Support Vector Machines (SVMs), graph-based manifold and semi-supervised learning algorithms suggest fundamental limitations of some learning algorithms. The problem is clear in kernel-based approaches when the kernel is "local" (e.g., the Gaussian kernel), i.e., K (x, y ) converges to a constant when {\textbar}{\textbar}x - y {\textbar}{\textbar} increases. These analyses point to the difficulty of learning "highly-varying functions", i.e., functions that have a large number of "variations" in the domain of interest, e.g., they would require a large number of pieces to be well represented by a piecewise-linear approximation. Since the number of pieces can be made to grow exponentially with the number of factors of variations in the input, this is connected with the well-known curse of dimensionality for classical non-parametric learning algorithms (for regression, classification and density estimation). If the shapes of all these pieces are unrelated, one needs enough examples for each piece in order to generalize properly. However, if these shapes are related and can be predicted from each other, "non-local" learning algorithms have the potential to generalize to pieces not covered by the training set. Such ability would seem necessary for learning in complex domains such as Artificial Intelligence tasks (e.g., related to vision, language, speech, robotics). Kernel machines (not only those with a local kernel) have a shallow architecture, i.e., only two levels of data-dependent computational elements. This is also true of feedforward neural networks with a single hidden layer (which can become SVMs when the number of hidden units becomes large (Bengio, Le Roux, Vincent, Delalleau, \& Marcotte, 2006)). A serious problem with shallow architectures is that they can be very inefficient in terms of the number of computational units (e.g., bases, hidden units), and thus in terms of required examples (Bengio \& Le Cun, 2007). One way to represent a highly-varying function compactly (with few parameters) is through the composition of many non-linearities, i.e., with a deep architecture. For example, the parity function with d inputs requires O(2d ) examples and parameters to be represented by a Gaussian SVM (Bengio et al., 2006), O(d2 ) parameters for a one-hidden-layer neural network, O(d) parameters and units for a multi-layer network with O(log2 d) layers, and O(1) parameters with a recurrent neural network. More generally,},
  file = {C:\Users\kmc07\Zotero\storage\RAEBBY6W\Bengio et al. - 2006 - Greedy Layer-Wise Training of Deep Networks.pdf}
}

@article{benna2016,
  title = {Computational Principles of Synaptic Memory Consolidation},
  author = {Benna, Marcus K. and Fusi, Stefano},
  year = {2016},
  month = dec,
  journal = {Nature Neuroscience},
  volume = {19},
  number = {12},
  pages = {1697--1706},
  issn = {1546-1726},
  doi = {10.1038/nn.4401},
  abstract = {Memories are stored and retained through complex, coupled processes operating on multiple timescales. To understand the computational principles behind these intricate networks of interactions, we construct a broad class of synaptic models that efficiently harness biological complexity to preserve numerous memories by protecting them against the adverse effects of overwriting. The memory capacity scales almost linearly with the number of synapses, which is a substantial improvement over the square root scaling of previous models. This was achieved by combining multiple dynamical processes that initially store memories in fast variables and then progressively transfer them to slower variables. Notably, the interactions between fast and slow variables are bidirectional. The proposed models are robust to parameter perturbations and can explain several properties of biological memory, including delayed expression of synaptic modifications, metaplasticity, and spacing effects.},
  langid = {english},
  pmid = {27694992},
  keywords = {Animals,Computer Simulation,Consolidation,Long-term memory,Memory,Memory Consolidation,Models Neurological,Nerve Net,Neuronal Plasticity,Neurons,Synapses},
  file = {C:\Users\kmc07\Zotero\storage\DJIUDPI3\Benna and Fusi - 2016 - Computational principles of synaptic memory consolidation.pdf}
}

@article{bienenstock1982,
  title = {Theory for the Development of Neuron Selectivity: Orientation Specificity and Binocular Interaction in Visual Cortex},
  shorttitle = {Theory for the Development of Neuron Selectivity},
  author = {Bienenstock, E. L. and Cooper, L. N. and Munro, P. W.},
  year = {1982},
  month = jan,
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  volume = {2},
  number = {1},
  pages = {32--48},
  issn = {0270-6474},
  doi = {10.1523/JNEUROSCI.02-01-00032.1982},
  abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
  langid = {english},
  pmcid = {PMC6564292},
  pmid = {7054394},
  keywords = {Animals,Cats,Dominance Cerebral,Mathematics,Models Neurological,Neurons,Orientation,Retina,Sensory Deprivation,Synapses,Visual Cortex,Visual Pathways,Visual Perception},
  file = {C:\Users\kmc07\Zotero\storage\SRUA4MHX\Bienenstock et al. - 1982 - Theory for the development of neuron selectivity orientation specificity and binocular interaction.pdf}
}

@misc{brannvall2023,
  title = {{{ReLU}} and {{Addition-based Gated RNN}}},
  author = {Br{\"a}nnvall, Rickard and Forsgren, Henrik and Sandin, Fredrik and Liwicki, Marcus},
  year = {2023},
  month = aug,
  number = {arXiv:2308.05629},
  eprint = {2308.05629},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {We replace the multiplication and sigmoid function of the conventional recurrent gate with addition and ReLU activation. This mechanism is designed to maintain long-term memory for sequence processing but at a reduced computational cost, thereby opening up for more efficient execution or larger models on restricted hardware. Recurrent Neural Networks (RNNs) with gating mechanisms such as LSTM and GRU have been widely successful in learning from sequential data due to their ability to capture long-term dependencies. Conventionally, the update based on current inputs and the previous state history is each multiplied with dynamic weights and combined to compute the next state. However, multiplication can be computationally expensive, especially for certain hardware architectures or alternative arithmetic systems such as homomorphic encryption. It is demonstrated that the novel gating mechanism can capture long-term dependencies for a standard synthetic sequence learning task while significantly reducing computational costs such that execution time is reduced by half on CPU and by one-third under encryption. Experimental results on handwritten text recognition tasks furthermore show that the proposed architecture can be trained to achieve comparable accuracy to conventional GRU and LSTM baselines. The gating mechanism introduced in this paper may enable privacy-preserving AI applications operating under homomorphic encryption by avoiding the multiplication of encrypted variables. It can also support quantization in (unencrypted) plaintext applications, with the potential for substantial performance gains since the addition-based formulation can avoid the expansion to double precision often required for multiplication.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\VEAWR2GX\\Br√§nnvall et al. - 2023 - ReLU and Addition-based Gated RNN.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\3XP4LZN6\\2308.html}
}

@article{brem2013,
  title = {Learning and Memory},
  author = {BREM, ANNA-KATHARINE and RAN, {\relax KATHY} and {PASCUAL-LEONE}, {\relax ALVARO}},
  year = {2013},
  journal = {Handbook of clinical neurology},
  volume = {116},
  pages = {693--737},
  issn = {0072-9752},
  doi = {10.1016/B978-0-444-53497-2.00055-3},
  urldate = {2025-06-02},
  pmcid = {PMC4248571},
  pmid = {24112934},
  file = {C:\Users\kmc07\Zotero\storage\TN3RBKJ3\BREM et al. - 2013 - Learning and memory.pdf}
}

@book{carpenter1991,
  title = {Pattern Recognition by Self-Organizing Neural Networks},
  editor = {Carpenter, Gail A. and Grossberg, Stephen},
  year = {1991},
  series = {Pattern Recognition by Self-Organizing Neural Networks},
  pages = {691},
  publisher = {The MIT Press},
  address = {Cambridge, MA, US},
  doi = {10.7551/mitpress/5271.001.0001},
  abstract = {This book provides a resource for teaching and research in the vitally important area of pattern recognition by self-organizing neural networks. Pattern recognition is a core competence that is as important for understanding biological intelligence as it is for inventing new types of intelligent machines. . . . The present book describes both classical and recent contributions towards understanding autonomous pattern recognition and related processes.  To this end, the book brings together neural network models that have typically been derived from analyses of biological intelligence or from heuristics suggested by observing properties of intelligent organisms.  The book should be of interest to cognitive scientists and neuroscientists on the one hand, and computer scientists, engineers, mathematicians, and physicists on the other. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  isbn = {978-0-262-03176-9},
  keywords = {Neural Networks,Pattern Discrimination}
}

@article{chevaleyre2007,
  title = {Endocannabinoid-Mediated Long-Term Plasticity Requires {{cAMP}}/{{PKA}} Signaling and {{RIM1alpha}}},
  author = {Chevaleyre, Vivien and Heifets, Boris D. and Kaeser, Pascal S. and S{\"u}dhof, Thomas C. and Castillo, Pablo E.},
  year = {2007},
  month = jun,
  journal = {Neuron},
  volume = {54},
  number = {5},
  pages = {801--812},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2007.05.020},
  abstract = {Endocannabinoids (eCBs) have emerged as key activity-dependent signals that, by activating presynaptic cannabinoid receptors (i.e., CB1) coupled to G(i/o) protein, can mediate short-term and long-term synaptic depression (LTD). While the presynaptic mechanisms underlying eCB-dependent short-term depression have been identified, the molecular events linking CB1 receptors to LTD are unknown. Here we show in the hippocampus that long-term, but not short-term, eCB-dependent depression of inhibitory transmission requires presynaptic cAMP/PKA signaling. We further identify the active zone protein RIM1alpha as a key mediator of both CB1 receptor effects on the release machinery and eCB-dependent LTD in the hippocampus. Moreover, we show that eCB-dependent LTD in the amygdala and hippocampus shares major mechanistic features. These findings reveal the signaling pathway by which CB1 receptors mediate long-term effects of eCBs in two crucial brain structures. Furthermore, our results highlight a conserved mechanism of presynaptic plasticity in the brain.},
  langid = {english},
  pmcid = {PMC2001295},
  pmid = {17553427},
  keywords = {Amygdala,Animals,Cannabinoid Receptor Modulators,Cyclic AMP,Cyclic AMP-Dependent Protein Kinases,Endocannabinoids,GTP-Binding Proteins,Hippocampus,Long-Term Synaptic Depression,Male,Mice,Mice Inbred C57BL,Mice Knockout,Neural Inhibition,Neural Pathways,Organ Culture Techniques,Receptor Cannabinoid CB1,Signal Transduction,Synaptic Transmission},
  file = {C:\Users\kmc07\Zotero\storage\UJV53MQP\Chevaleyre et al. - 2007 - Endocannabinoid-mediated long-term plasticity requires cAMPPKA signaling and RIM1alpha.pdf}
}

@misc{cho2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = sep,
  number = {arXiv:1406.1078},
  eprint = {1406.1078},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.1078},
  urldate = {2025-01-20},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\SKRT5PCN\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\XDLFRV6N\\1406.html}
}

@misc{chu2023,
  title = {Random {{Feedback Alignment Algorithms}} to Train {{Neural Networks}}: {{Why}} Do They {{Align}}?},
  shorttitle = {Random {{Feedback Alignment Algorithms}} to Train {{Neural Networks}}},
  author = {Chu, Dominique and Bacho, Florian},
  year = {2023},
  month = jun,
  number = {arXiv:2306.02325},
  eprint = {2306.02325},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.02325},
  urldate = {2025-02-17},
  abstract = {Feedback alignment algorithms are an alternative to backpropagation to train neural networks, whereby some of the partial derivatives that are required to compute the gradient are replaced by random terms. This essentially transforms the update rule into a random walk in weight space. Surprisingly, learning still works with those algorithms, including training of deep neural networks. This is generally attributed to an alignment of the update of the random walker with the true gradient - the eponymous gradient alignment -- which drives an approximate gradient descend. The mechanism that leads to this alignment remains unclear, however. In this paper, we use mathematical reasoning and simulations to investigate gradient alignment. We observe that the feedback alignment update rule has fixed points, which correspond to extrema of the loss function. We show that gradient alignment is a stability criterion for those fixed points. It is only a necessary criterion for algorithm performance. Experimentally, we demonstrate that high levels of gradient alignment can lead to poor algorithm performance and that the alignment is not always driving the gradient descend.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\44FT7PQJ\\Chu and Bacho - 2023 - Random Feedback Alignment Algorithms to train Neural Networks Why do they Align.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\GQX6RYRL\\2306.html;C\:\\Users\\kmc07\\Zotero\\storage\\YNEEZQ8L\\2306.html}
}

@article{chu2024,
  title = {Random Feedback Alignment Algorithms to Train Neural Networks: Why Do They Align?},
  shorttitle = {Random Feedback Alignment Algorithms to Train Neural Networks},
  author = {Chu, Dominique and Bacho, Florian},
  year = {2024},
  month = may,
  journal = {Machine Learning: Science and Technology},
  volume = {5},
  number = {2},
  pages = {025023},
  publisher = {IOP Publishing},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/ad3ee5},
  urldate = {2025-06-02},
  abstract = {Feedback alignment algorithms are an alternative to backpropagation to train neural networks, whereby some of the partial derivatives that are required to compute the gradient are replaced by random terms. This essentially transforms the update rule into a random walk in weight space. Surprisingly, learning still works with those algorithms, including training of deep neural networks. The performance of FA is generally attributed to an alignment of the update of the random walker with the true gradient---the eponymous gradient alignment---which drives an approximate gradient descent. The mechanism that leads to this alignment remains unclear, however. In this paper, we use mathematical reasoning and simulations to investigate gradient alignment. We observe that the feedback alignment update rule has fixed points, which correspond to extrema of the loss function. We show that gradient alignment is a stability criterion for those fixed points. It is only a necessary criterion for algorithm performance. Experimentally, we demonstrate that high levels of gradient alignment can lead to poor algorithm performance and that the alignment is not always driving the gradient descent.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\XIJEDPYY\Chu and Bacho - 2024 - Random feedback alignment algorithms to train neural networks why do they align.pdf}
}

@misc{cohen2017,
  title = {{{EMNIST}}: An Extension of {{MNIST}} to Handwritten Letters},
  shorttitle = {{{EMNIST}}},
  author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, Andr{\'e}},
  year = {2017},
  month = feb,
  number = {arXiv:1702.05373},
  eprint = {1702.05373},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.05373},
  urldate = {2025-01-20},
  abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\W6BD6PIW\\Cohen et al. - 2017 - EMNIST an extension of MNIST to handwritten letters.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\5TYMMTP4\\1702.html}
}

@inproceedings{confavreux2020,
  title = {A Meta-Learning Approach to (Re)Discover Plasticity Rules That Carve a Desired Function into a Neural Network},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Confavreux, Basile and Zenke, Friedemann and Agnes, Everton and Lillicrap, Timothy and Vogels, Tim},
  year = {2020},
  volume = {33},
  pages = {16398--16408},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-10-18},
  abstract = {The search for biologically faithful synaptic plasticity rules has resulted in a large body of models. They are usually inspired by -- and fitted to -- experimental data, but they rarely produce neural dynamics that serve complex functions. These failures suggest that current plasticity models are still under-constrained by existing data. Here, we present an alternative approach that uses meta-learning to discover plausible synaptic plasticity rules. Instead of experimental data, the rules are constrained by the functions they implement and the structure they are meant to produce. Briefly, we parameterize synaptic plasticity rules by a Volterra expansion and then use supervised learning methods (gradient descent or evolutionary strategies) to minimize a problem-dependent loss function that quantifies how effectively a candidate plasticity rule transforms an initially random network into one with the desired function. We first validate our approach by re-discovering previously described plasticity rules, starting at the single-neuron level and Oja's rule'', a simple Hebbian plasticity rule that captures the direction of most variability of inputs to a neuron (i.e., the first principal component). We expand the problem to the network level and ask the framework to find Oja's rule together with an anti-Hebbian rule such that an initially random two-layer firing-rate network will recover several principal components of the input space after learning. Next, we move to networks of integrate-and-fire neurons with plastic inhibitory afferents. We train for rules that achieve a target firing rate by countering tuned excitation. Our algorithm discovers a specific subset of the manifold of rules that can solve this task. Our work is a proof of principle of an automated and unbiased approach to unveil synaptic plasticity rules that obey biological constraints and can solve complex functions.},
  file = {C:\Users\kmc07\Zotero\storage\7H328YRP\Confavreux et al. - 2020 - A meta-learning approach to (re)discover plasticity rules that carve a desired function into a neura.pdf}
}

@article{coulombe2017,
  title = {Computing with Networks of Nonlinear Mechanical Oscillators},
  author = {Coulombe, Jean C. and York, Mark C. A. and Sylvestre, Julien},
  year = {2017},
  month = jun,
  journal = {PLOS ONE},
  volume = {12},
  number = {6},
  pages = {e0178663},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0178663},
  urldate = {2024-12-10},
  abstract = {As it is getting increasingly difficult to achieve gains in the density and power efficiency of microelectronic computing devices because of lithographic techniques reaching fundamental physical limits, new approaches are required to maximize the benefits of distributed sensors, micro-robots or smart materials. Biologically-inspired devices, such as artificial neural networks, can process information with a high level of parallelism to efficiently solve difficult problems, even when implemented using conventional microelectronic technologies. We describe a mechanical device, which operates in a manner similar to artificial neural networks, to solve efficiently two difficult benchmark problems (computing the parity of a bit stream, and classifying spoken words). The device consists in a network of masses coupled by linear springs and attached to a substrate by non-linear springs, thus forming a network of anharmonic oscillators. As the masses can directly couple to forces applied on the device, this approach combines sensing and computing functions in a single power-efficient device with compact dimensions.},
  langid = {english},
  keywords = {Artificial neural networks,Computer hardware,Computer networks,Computers,Network analysis,Neurons,Signaling networks,Speech signal processing},
  file = {C:\Users\kmc07\Zotero\storage\SGH33W4Y\Coulombe et al. - 2017 - Computing with networks of nonlinear mechanical oscillators.pdf}
}

@article{crick1989,
  title = {The Recent Excitement about Neural Networks},
  author = {Crick, Francis},
  year = {1989},
  month = jan,
  journal = {Nature},
  volume = {337},
  number = {6203},
  pages = {129--132},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/337129a0},
  urldate = {2025-06-02},
  abstract = {The remarkable properties of some recent computer algorithms for neural networks seemed to promise a fresh approach to understanding the computational properties of the brain. Unfortunately most of these neural nets are unrealistic in important respects.},
  copyright = {1989 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {C:\Users\kmc07\Zotero\storage\52E7M22F\Crick - 1989 - The recent excitement about neural networks.pdf}
}

@book{d.ohebb1949,
  title = {The {{Organization Of Behavior}}},
  author = {{D.o Hebb}},
  year = {1949},
  urldate = {2025-06-02},
  abstract = {Book Source: Digital Library of India Item 2015.226341 dc.contributor.author: D.o Hebb dc.date.accessioned: 2015-07-10T15:34:22Z dc.date.available: 2015-07-10T15:34:22Z dc.date.digitalpublicationdate: 2015-05-07 dc.date.citation: 1949 dc.identifier.barcode: 5990010121546 dc.identifier.origpath: /rawdataupload/upload/0121/548 dc.identifier.copyno: 1 dc.identifier.uri: http://www.new.dli.ernet.in/handle/2015/226341 dc.description.scannerno: 17 dc.description.scanningcentre: IIIT, Allahabad dc.description.main: 1 dc.description.tagged: 0 dc.description.totalpages: 342 dc.format.mimetype: application/pdf dc.language.iso: English dc.publisher.digitalrepublisher: Vishal dc.publisher: John Wiley And Sons Inc New York dc.rights: Out\_of\_copyright dc.source.library: University Liberey  Allahabad dc.subject.classification: English Literature dc.title: The Organization Of Behavior},
  langid = {english},
  keywords = {IIIT}
}

@incollection{dale2021,
  title = {Reservoir {{Computing}} in {{Material Substrates}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Dale, Matthew and Miller, Julian F. and Stepney, Susan and Trefzer, Martin A.},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {141--166},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_7},
  urldate = {2024-12-10},
  abstract = {We overview Reservoir Computing (RC) with physical systems from an Unconventional Computing (UC) perspective. We discuss challenges present in both fields, including encoding and representation, or how to manipulate and read information; ways to search large and complex configuration spaces of physical systems; and what makes a ``good'' computing substrate.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@book{dayan2001,
  title = {Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems},
  shorttitle = {Theoretical Neuroscience},
  author = {Dayan, Peter and Abbott, L. F.},
  year = {2001},
  series = {Computational Neuroscience},
  publisher = {Massachusetts Institute of Technology Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-04199-7},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\S6TW4RBW\Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathematical modeling of neural systems.pdf}
}

@misc{detorakis2018,
  title = {Contrastive {{Hebbian Learning}} with {{Random Feedback Weights}}},
  author = {Detorakis, Georgios and Bartley, Travis and Neftci, Emre},
  year = {2018},
  month = jun,
  number = {arXiv:1806.07406},
  eprint = {1806.07406},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1806.07406},
  urldate = {2025-01-08},
  abstract = {Neural networks are commonly trained to make predictions through learning algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by gradient backpropagation, is based on Hebb's rule and the contrastive divergence algorithm. It operates in two phases, the forward (or free) phase, where the data are fed to the network, and a backward (or clamped) phase, where the target signals are clamped to the output layer of the network and the feedback signals are transformed through the transpose synaptic weight matrices. This implies symmetries at the synaptic level, for which there is no evidence in the brain. In this work, we propose a new variant of the algorithm, called random contrastive Hebbian learning, which does not rely on any synaptic weights symmetries. Instead, it uses random matrices to transform the feedback signals during the clamped phase, and the neural dynamics are described by first order non-linear differential equations. The algorithm is experimentally verified by solving a Boolean logic task, classification tasks (handwritten digits and letters), and an autoencoding task. This article also shows how the parameters affect learning, especially the random matrices. We use the pseudospectra analysis to investigate further how random matrices impact the learning process. Finally, we discuss the biological plausibility of the proposed algorithm, and how it can give rise to better computational models for learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\34CDRTBB\\Detorakis et al. - 2018 - Contrastive Hebbian Learning with Random Feedback Weights.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\WS4VRAHN\\1806.html}
}

@incollection{dominey2021,
  title = {Cortico-{{Striatal Origins}} of {{Reservoir Computing}}, {{Mixed Selectivity}}, and {{Higher Cognitive Function}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Dominey, Peter Ford},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {29--58},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_2},
  urldate = {2024-12-10},
  abstract = {The computational richness and complexity of recurrent neural networks is well known, and has yet to be fully exploited and understood. It is interesting that in this context, one of the most prevalent features of the cerebral cortex is its massive recurrent connectivity. Despite this central principle of cortical organization, it is only slowly becoming recognized that the cortex is a reservoir. Of course there are mechanisms in the cortex that allow for plasticity. But the general model of a reservoir as a recurrent network that creates a high dimensional temporal expansion of its inputs which can then be harvested for extracting the required output is fully achieved by the cortex. Future research will find this obvious. This chapter provides a framework for more clearly understanding this conception of cortex and the corticostriatal system.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@inproceedings{ester1996,
  title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  year = {1996},
  month = aug,
  series = {{{KDD}}'96},
  pages = {226--231},
  publisher = {AAAI Press},
  address = {Portland, Oregon},
  urldate = {2025-06-01},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.}
}

@article{falahian2015,
  title = {Artificial Neural Network-Based Modeling of Brain Response to Flicker Light},
  author = {Falahian, Razieh and Mehdizadeh Dastjerdi, Maryam and Molaie, Malihe and Jafari, Sajad and Gharibzadeh, Shahriar},
  year = {2015},
  month = sep,
  journal = {Nonlinear Dynamics},
  volume = {81},
  number = {4},
  pages = {1951--1967},
  issn = {1573-269X},
  doi = {10.1007/s11071-015-2118-x},
  urldate = {2025-06-02},
  abstract = {Not only does the modeling of dynamical systems, for instance the biological systems, play an important role in the accurate perception and analysis of these systems, but it also makes the prediction and control of their behavior straightforward. The results of multiple researches in the field of the modeling of biological systems have indicated that the chaotic behavior is a prevalent feature of most complex interactive biological systems. Our results demonstrate that the artificial neural network provides us an effective means to model the underlying dynamics of these systems. In this paper, at first, we represent the results of the use of a multilayer feed-forward neural network to model some famous chaotic systems. The specified neural network is trained with the return maps extracted from the time series. We proceed with the paper by evaluating the accuracy and robustness of our model. The ability of the select neural network to model the dynamics of chosen chaotic systems is verified, even in the presence of noise. Afterwards, we model the brain response to the flicker light. It is known that the brain response to some stimuli such as the flicker light recorded as electroretinogram is an exemplar of chaotic behavior. The need remains, however, for realistic modeling of this behavior of the brain. In this paper, we represent the results of the modeling of this chaotic response by utilizing the proposed neural network. The capability of the neural network to model this specific brain response is confirmed.},
  langid = {english},
  keywords = {Artificial neural network,Bifurcation diagram,Brain response,Chaotic behavior,Computational Neuroscience,Electroretinogram,Mathematical Models of Cognitive Processes and Neural Networks,Modeling,Neural encoding,Neural Patterning,Neurological models,Systems Neuroscience},
  file = {C:\Users\kmc07\Zotero\storage\35RRJAXH\Falahian et al. - 2015 - Artificial neural network-based modeling of brain response to flicker light.pdf}
}

@inproceedings{faraji2015,
  title = {A Biologically Plausible 3-Factor Learning Rule for Expectation Maximization in Reinforcement Learning and Decision Making},
  author = {Faraji, Mohammad Javad and Preuschoff, K. and Gerstner, W.},
  year = {2015},
  urldate = {2024-11-25},
  abstract = {One of the most frequent problems in both decision making and reinforcement learning (RL) is expectation maximization involving functionals such as reward or utility. Generally, these problems consist of computing the optimal solution of a density function. Instead of trying to find this exact solution, a common approach is to approximate it through a learning process. In this work we propose a functional gradient rule for the maximization of a general form of density-dependent functionals using a stochastic gradient ascent algorithm. If a neural network is used for parametrization of the desired density function, the proposed learning rule can be viewed as a modulated Hebbian rule. Such a learning rule is biologically plausible, because it consists of both local and global factors corresponding to the coactivity of pre/post-synaptic neurons and the effect of neuromodulation, respectively. We first apply our technique to standard reward maximization in RL. As expected, this yields the standard policy gradient rule in which parameters of the model are updated proportional to the amount of reward. Next, we use variational free energy as a functional and find that the estimated change in parameters is modulated by a measure of surprise signal. Finally, we propose an information theoretical equivalent of existing models in expected utility maximization, as a standard model of decision making, to incorporate both individual preferences and choice variability. We show that our technique can also be applied into such novel framework.},
  file = {C:\Users\kmc07\Zotero\storage\UZHRDBNL\Faraji et al. - 2015 - A biologically plausible 3-factor learning rule for expectation maximization in reinforcement learni.pdf}
}

@inproceedings{finn2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  pages = {1126--1135},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-06-02},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  langid = {english},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\U5KLLUS3\\Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\XMAD4DWS\\Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf}
}

@article{fusi2005,
  title = {Cascade Models of Synaptically Stored Memories},
  author = {Fusi, Stefano and Drew, Patrick J. and Abbott, L. F.},
  year = {2005},
  month = feb,
  journal = {Neuron},
  volume = {45},
  number = {4},
  pages = {599--611},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.02.001},
  abstract = {Storing memories of ongoing, everyday experiences requires a high degree of plasticity, but retaining these memories demands protection against changes induced by further activity and experience. Models in which memories are stored through switch-like transitions in synaptic efficacy are good at storing but bad at retaining memories if these transitions are likely, and they are poor at storage but good at retention if they are unlikely. We construct and study a model in which each synapse has a cascade of states with different levels of plasticity, connected by metaplastic transitions. This cascade model combines high levels of memory storage with long retention times and significantly outperforms alternative models. As a result, we suggest that memory storage requires synapses with multiple states exhibiting dynamics over a wide range of timescales, and we suggest experimental tests of this hypothesis.},
  langid = {english},
  pmid = {15721245},
  keywords = {Animals,Humans,Memory,Models Neurological,Neuronal Plasticity,Synapses,Time Factors}
}

@article{fusi2005a,
  title = {Cascade Models of Synaptically Stored Memories},
  author = {Fusi, Stefano and Drew, Patrick J. and Abbott, L. F.},
  year = {2005},
  month = feb,
  journal = {Neuron},
  volume = {45},
  number = {4},
  pages = {599--611},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.02.001},
  abstract = {Storing memories of ongoing, everyday experiences requires a high degree of plasticity, but retaining these memories demands protection against changes induced by further activity and experience. Models in which memories are stored through switch-like transitions in synaptic efficacy are good at storing but bad at retaining memories if these transitions are likely, and they are poor at storage but good at retention if they are unlikely. We construct and study a model in which each synapse has a cascade of states with different levels of plasticity, connected by metaplastic transitions. This cascade model combines high levels of memory storage with long retention times and significantly outperforms alternative models. As a result, we suggest that memory storage requires synapses with multiple states exhibiting dynamics over a wide range of timescales, and we suggest experimental tests of this hypothesis.},
  langid = {english},
  pmid = {15721245},
  keywords = {Animals,Humans,Memory,Models Neurological,Neuronal Plasticity,Synapses,Time Factors},
  file = {C:\Users\kmc07\Zotero\storage\IQUXWFBJ\Fusi et al. - 2005 - Cascade models of synaptically stored memories.pdf}
}

@article{fusi2007,
  title = {Limits on the Memory Storage Capacity of Bounded Synapses},
  author = {Fusi, Stefano and Abbott, L. F.},
  year = {2007},
  month = apr,
  journal = {Nature Neuroscience},
  volume = {10},
  number = {4},
  pages = {485--493},
  issn = {1097-6256},
  doi = {10.1038/nn1859},
  abstract = {Memories maintained in patterns of synaptic connectivity are rapidly overwritten and destroyed by ongoing plasticity related to the storage of new memories. Short memory lifetimes arise from the bounds that must be imposed on synaptic efficacy in any realistic model. We explored whether memory performance can be improved by allowing synapses to traverse a large number of states before reaching their bounds, or by changing the way these bounds are imposed. In the case of hard bounds, memory lifetimes grow proportional to the square of the number of synaptic states, but only if potentiation and depression are precisely balanced. Improved performance can be obtained without fine tuning by imposing soft bounds, but this improvement is only linear with respect to the number of synaptic states. We explored several other possibilities and conclude that improving memory performance requires a more radical modification of the standard model of memory storage.},
  langid = {english},
  pmid = {17351638},
  keywords = {Animals,Humans,Memory,Models Neurological,Neuronal Plasticity,Synapses},
  file = {C:\Users\kmc07\Zotero\storage\G2SIPD84\Fusi and Abbott - 2007 - Limits on the memory storage capacity of bounded synapses.pdf}
}

@misc{fusi2021,
  title = {Memory Capacity of Neural Network Models},
  author = {Fusi, Stefano},
  year = {2021},
  month = dec,
  number = {arXiv:2108.07839},
  eprint = {2108.07839},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07839},
  urldate = {2025-05-24},
  abstract = {Memory is a complex phenomenon that involves several distinct mechanisms. These mechanisms operate at different spatial and temporal levels. This chapter focuses on the theoretical framework and the mathematical models that have been developed to understand how these mechanisms are orchestrated to store, preserve and retrieve a large number of memories. In particular, this chapter reviews the theoretical studies on memory capacity, in which the investigators estimated how the number of storable memories scales with the number of neurons and synapses in the neural circuitry. The memory capacity depends on the complexity of the synapses, the sparseness of the representations, the spatial and temporal correlations between memories and the specific way memories are retrieved. Complexity is important when the synapses can only be modified with a limited precision, as in the case of biological synapses, and sparseness can greatly increase memory capacity and be particularly beneficial when memories are structured (correlated to each other). The theoretical tools discussed by this chapter can be harnessed to identify the important computational principles that underlie memory storage, preservation and retrieval and provide guidance in designing and interpreting memory experiments.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\TPMC48KF\\Fusi - 2021 - Memory capacity of neural network models.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\C43WERG6\\2108.html}
}

@incollection{gallicchio2021,
  title = {Deep {{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {77--95},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_4},
  urldate = {2024-12-10},
  abstract = {This chapter surveys the recent advancements on the extension of Reservoir Computing toward deep architectures, which is gaining increasing research attention in the neural networks community. Within this context, we focus on describing the major features of Deep Echo State Networks based on the hierarchical composition of multiple reservoirs. The intent is to provide a useful reference to guide applications and further developments of this efficient and effective class of approaches to deal with times-series and more complex data within a unified description and analysis.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@misc{gharoun2023,
  title = {Meta-Learning Approaches for Few-Shot Learning: {{A}} Survey of Recent Advances},
  shorttitle = {Meta-Learning Approaches for Few-Shot Learning},
  author = {Gharoun, Hassan and Momenifar, Fereshteh and Chen, Fang and Gandomi, Amir H.},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07502},
  eprint = {2303.07502},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.07502},
  urldate = {2025-06-02},
  abstract = {Despite its astounding success in learning deeper multi-dimensional data, the performance of deep learning declines on new unseen tasks mainly due to its focus on same-distribution prediction. Moreover, deep learning is notorious for poor generalization from few samples. Meta-learning is a promising approach that addresses these issues by adapting to new tasks with few-shot datasets. This survey first briefly introduces meta-learning and then investigates state-of-the-art meta-learning methods and recent advances in: (I) metric-based, (II) memory-based, (III), and learning-based methods. Finally, current challenges and insights for future researches are discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\6RXEHY7N\\Gharoun et al. - 2023 - Meta-learning approaches for few-shot learning A survey of recent advances.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\KTUAGTPD\\2303.html}
}

@inproceedings{glorot2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2025-06-02},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\8WIPP4QA\Glorot and Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf}
}

@inproceedings{gomez2005,
  title = {Evolving {{Modular Fast-Weight Networks}} for {{Control}}},
  booktitle = {Artificial {{Neural Networks}}: {{Formal Models}} and {{Their Applications}} -- {{ICANN}} 2005},
  author = {Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  editor = {Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.z}ny, S{\l}awomir},
  year = {2005},
  pages = {383--389},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11550907_61},
  abstract = {In practice, almost all control systems in use today implement some form of linear control. However, there are many tasks for which conventional control engineering methods are not directly applicable because there is not enough information about how the system should be controlled (i.e. reinforcement learning problems). In this paper, we explore an approach to such problems that evolves fast-weight neural networks. These networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers. The method is tested on 2D mobile robot version of the pole balancing task where the controller must learn to switch between two operating modes, one using a single pole and the other using a jointed pole version that has not before been solved.},
  isbn = {978-3-540-28756-8},
  langid = {english},
  keywords = {Bipedal Robot,Controller Network,Linear Controller,Recurrent Network,Single Pole},
  file = {C:\Users\kmc07\Zotero\storage\TXKPMMNL\Gomez and Schmidhuber - 2005 - Evolving Modular Fast-Weight Networks for Control.pdf}
}

@article{grant2010,
  title = {Computing Behaviour in Complex Synapses: {{Synapse}} Proteome Complexity and the Evolution of Behaviour and Disease},
  shorttitle = {Computing Behaviour in Complex Synapses},
  author = {Grant, Seth G.N.},
  year = {2010},
  month = apr,
  journal = {The Biochemist},
  volume = {32},
  number = {2},
  pages = {6--9},
  issn = {0954-982X, 1740-1194},
  doi = {10.1042/BIO03202006},
  urldate = {2024-11-28},
  abstract = {Synapses are the defining feature of the cellular organization of the brain, connecting each neuron with thousands of others. The great morphological diversity of neurons was recognized in the 19th Century, but only in the last 10 years has the remarkable degree of molecular complexity within synapses become apparent. Over 1000 proteins are found in the proteome of the postsynaptic terminal of mammalian synapses. This complexity is organized into networks providing combinatorial signalling for physiological processes, diversity of synapse types and disease susceptibility, as well as providing a new paradigm for the evolution of the brain.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\QNQE9DZJ\Grant - 2010 - Computing behaviour in complex synapses Synapse proteome complexity and the evolution of behaviour.pdf}
}

@article{grossberg1987,
  title = {Competitive Learning: {{From}} Interactive Activation to Adaptive Resonance},
  shorttitle = {Competitive Learning},
  author = {Grossberg, Stephen},
  year = {1987},
  month = jan,
  journal = {Cognitive Science},
  volume = {11},
  number = {1},
  pages = {23--63},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(87)80025-3},
  urldate = {2025-06-02},
  abstract = {Functional and mechanistic comparisons are made between several network models of cognitive processing: competitive learning, interactive activation, adaptive resonance, and back propagation. The starting point of this comparison is the article of Rumelhart and Zipser (1985) on feature discovery through competitive learning. All the models which Rumelhart and Zipser (1985) have described were shown in Grossberg (1976b) to exhibit a type of learning which is temporally unstable. Competitive learning mechanisms can be stabilized in response to an arbitrary input environment by being supplemented with mechanisms for learning top-down expectancies, or templates; for matching bottom-up input patterns with the top-down expectancies; and for releasing orienting reactions in a mismatch situation, thereby updating short-term memory and searching for another internal representation. Network architectures which embody all of these mechanisms were called adaptive resonance models by Grossberg (1976c). Self-stabilizing learning models are candidates for use in real-world applications where unpredictable changes can occur in complex input environments. Competitive learning postulates are inconsistent with the postulates of the interactive activation model of McClelland and Rumelhart (1981), and suggest different levels of processing and interaction rules for the analysis of word recognition. Adaptive resonance models use these alternative levels and interaction rules. The selforganizing learning of an adaptive resonance model is compared and contrasted with the teacher-directed learning of a back propagation model. A number of criteria for evaluating real-time network models of cognitive processing are described and applied.},
  file = {C:\Users\kmc07\Zotero\storage\KTG7KYK8\S0364021387800253.html}
}

@article{guerguiev,
  title = {Towards Deep Learning with Segregated Dendrites},
  author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  journal = {eLife},
  volume = {6},
  pages = {e22901},
  issn = {2050-084X},
  doi = {10.7554/eLife.22901},
  urldate = {2025-01-10},
  abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations---the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons., Artificial intelligence has made major progress in recent years thanks to a technique known as deep learning, which works by mimicking the human brain. When computers employ deep learning, they learn by using networks made up of many layers of simulated neurons. Deep learning has opened the door to computers with human -- or even super-human -- levels of skill in recognizing images, processing speech and controlling vehicles. But many neuroscientists are skeptical about whether the brain itself performs deep learning., The patterns of activity that occur in computer networks during deep learning resemble those seen in human brains. But some features of deep learning seem incompatible with how the brain works. Moreover, neurons in artificial networks are much simpler than our own neurons. For instance, in the region of the brain responsible for thinking and planning, most neurons have complex tree-like shapes. Each cell has `roots' deep inside the brain and `branches' close to the surface. By contrast, simulated neurons have a uniform structure., To find out whether networks made up of more realistic simulated neurons could be used to make deep learning more biologically realistic, Guerguiev et al. designed artificial neurons with two compartments, similar to the `roots' and `branches'. The network learned to recognize hand-written digits more easily when it had many layers than when it had only a few. This shows that artificial neurons more like those in the brain can enable deep learning. It even suggests that our own neurons may have evolved their shape to support this process., If confirmed, the link between neuronal shape and deep learning could help us develop better brain-computer interfaces. These allow people to use their brain activity to control devices such as artificial limbs. Despite advances in computing, we are still superior to computers when it comes to learning. Understanding how our own brains show deep learning could thus help us develop better, more human-like artificial intelligence in the future.},
  pmcid = {PMC5716677},
  pmid = {29205151},
  file = {C:\Users\kmc07\Zotero\storage\HU8TD2C3\Guerguiev et al. - Towards deep learning with segregated dendrites.pdf}
}

@misc{ha2016,
  title = {{{HyperNetworks}}},
  author = {Ha, David and Dai, Andrew and Le, Quoc V.},
  year = {2016},
  month = dec,
  number = {arXiv:1609.09106},
  eprint = {1609.09106},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.09106},
  urldate = {2025-02-05},
  abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\42ZLU96A\\Ha et al. - 2016 - HyperNetworks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\IBNKKM3X\\1609.html}
}

@incollection{hadaeghi2021,
  title = {Neuromorphic {{Electronic Systems}} for~{{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Hadaeghi, Fatemeh},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {221--237},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_10},
  urldate = {2024-12-10},
  abstract = {This chapter provides a comprehensive survey of the researches and motivations for hardware implementation of reservoir computing (RC) on neuromorphic electronic systems. Due to its computational efficiency and the fact that training amounts to a simple linear regression, both spiking and non-spiking implementations of reservoir computing on neuromorphic hardware have been developed. Here, a review of these experimental studies is provided to illustrate the progress in this area and to address the technical challenges which arise from this specific hardware implementation. Moreover, to deal with the challenges of computation on such unconventional substrates, several lines of potential solutions are presented based on advances in other computational approaches in machine learning.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@misc{han2020,
  title = {Extension of {{Direct Feedback Alignment}} to {{Convolutional}} and {{Recurrent Neural Network}} for {{Bio-plausible Deep Learning}}},
  author = {Han, Donghyeon and Park, Gwangtae and Ryu, Junha and Yoo, Hoi-jun},
  year = {2020},
  month = jun,
  number = {arXiv:2006.12830},
  eprint = {2006.12830},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.12830},
  urldate = {2025-02-17},
  abstract = {Throughout this paper, we focus on the improvement of the direct feedback alignment (DFA) algorithm and extend the usage of the DFA to convolutional and recurrent neural networks (CNNs and RNNs). Even though the DFA algorithm is biologically plausible and has a potential of high-speed training, it has not been considered as the substitute for back-propagation (BP) due to the low accuracy in the CNN and RNN training. In this work, we propose a new DFA algorithm for BP-level accurate CNN and RNN training. Firstly, we divide the network into several modules and apply the DFA algorithm within the module. Second, the DFA with the sparse backward weight is applied. It comes with a form of dilated convolution in the CNN case, and in a form of sparse matrix multiplication in the RNN case. Additionally, the error propagation method of CNN becomes simpler through the group convolution. Finally, hybrid DFA increases the accuracy of the CNN and RNN training to the BP-level while taking advantage of the parallelism and hardware efficiency of the DFA algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\R79G88PZ\\Han et al. - 2020 - Extension of Direct Feedback Alignment to Convolutional and Recurrent Neural Network for Bio-plausib.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\5NNZAJ3D\\2006.html}
}

@incollection{hauser2021,
  title = {Physical {{Reservoir Computing}} in {{Robotics}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Hauser, Helmut},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {169--190},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_8},
  urldate = {2024-12-10},
  abstract = {In recent years, there has been an increasing interest in using the concept of physical reservoir computing in robotics. The idea is to employ the robot's body and its dynamics as a computational resource. On one hand, this has been driven by the introduction of mathematical frameworks showing how complex mechanical structures can be used to build reservoirs. On the other hand, with the recent advances in smart materials, novel additive manufacturing techniques, and the corresponding rise of soft robotics, a new and much richer set of tools for designing and building robots is now available. Despite the increased interest, however, there is still a wide range of unanswered research questions and a rich area of under-explored applications. We will discuss the current state of the art, the implications of using robot bodies as reservoirs, and the great potential and future directions of physical reservoir computing in robotics.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{herculano-houzel2009,
  title = {The Human Brain in Numbers: A Linearly Scaled-up Primate Brain},
  shorttitle = {The Human Brain in Numbers},
  author = {{Herculano-Houzel}, Suzana},
  year = {2009},
  journal = {Frontiers in Human Neuroscience},
  volume = {3},
  pages = {31},
  issn = {1662-5161},
  doi = {10.3389/neuro.09.031.2009},
  abstract = {The human brain has often been viewed as outstanding among mammalian brains: the most cognitively able, the largest-than-expected from body size, endowed with an overdeveloped cerebral cortex that represents over 80\% of brain mass, and purportedly containing 100 billion neurons and 10x more glial cells. Such uniqueness was seemingly necessary to justify the superior cognitive abilities of humans over larger-brained mammals such as elephants and whales. However, our recent studies using a novel method to determine the cellular composition of the brain of humans and other primates as well as of rodents and insectivores show that, since different cellular scaling rules apply to the brains within these orders, brain size can no longer be considered a proxy for the number of neurons in the brain. These studies also showed that the human brain is not exceptional in its cellular composition, as it was found to contain as many neuronal and non-neuronal cells as would be expected of a primate brain of its size. Additionally, the so-called overdeveloped human cerebral cortex holds only 19\% of all brain neurons, a fraction that is similar to that found in other mammals. In what regards absolute numbers of neurons, however, the human brain does have two advantages compared to other mammalian brains: compared to rodents, and probably to whales and elephants as well, it is built according to the very economical, space-saving scaling rules that apply to other primates; and, among economically built primate brains, it is the largest, hence containing the most neurons. These findings argue in favor of a view of cognitive abilities that is centered on absolute numbers of neurons, rather than on body size or encephalization, and call for a re-examination of several concepts related to the exceptionality of the human brain.},
  langid = {english},
  pmcid = {PMC2776484},
  pmid = {19915731},
  keywords = {brain scaling,encephalization,human,number of neurons},
  file = {C:\Users\kmc07\Zotero\storage\RDBWGKCZ\Herculano-Houzel - 2009 - The human brain in numbers a linearly scaled-up primate brain.pdf}
}

@article{hinton1987,
  title = {Using {{Fast Weights}} to {{Deblur Old Memories}}},
  author = {Hinton, Geoffrey E. and Plaut, David C.},
  year = {1987},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {9},
  number = {0},
  urldate = {2024-11-16},
  abstract = {Connectionist models usually have a single weight on each connection. Some interesting newproperties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associationsare "blurred" by subsequent learning, all the original associations can be "deblurred" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel outthe changes in the slow weights caused by the subsequent learning.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\8CVBYZCV\Hinton and Plaut - 1987 - Using Fast Weights to Deblur Old Memories.pdf}
}

@article{hinton2003,
  title = {The Ups and Downs of {{Hebb}} Synapses},
  author = {Hinton, Geoffrey},
  year = {2003},
  journal = {Canadian Psychology / Psychologie canadienne},
  volume = {44},
  number = {1},
  pages = {10--13},
  publisher = {Canadian Psychological Association},
  address = {Canada},
  issn = {1878-7304},
  doi = {10.1037/h0085812},
  abstract = {Modelers have come up with many different learning rules for neural networks. When a teacher specifies the correct output, error-driven rules work better than pure Hebb rules in which the changes in synapse strength depend on the correlation between pre and postsynaptic activities. But for unsupervised learning, Hebb rules can be very effective if they are combined with suitable normalization or "unlearning" terms to prevent the synapses growing without bound. Hebb rules that use rates of change of activity instead of activity itself are useful for discovering perceptual invariants and may also provide a way of implementing error-driven learning. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Learning,Neural Networks,Synapses,Theories},
  file = {C:\Users\kmc07\Zotero\storage\RBBS8CSU\Hinton - 2003 - The ups and downs of Hebb synapses.pdf}
}

@inproceedings{hochreiter2001,
  title = {Learning to {{Learn Using Gradient Descent}}},
  booktitle = {Artificial {{Neural Networks}} --- {{ICANN}} 2001},
  author = {Hochreiter, Sepp and Younger, A. Steven and Conwell, Peter R.},
  editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
  year = {2001},
  pages = {87--94},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-44668-0_13},
  abstract = {This paper introduces the application of gradient descent methods to meta-learning. The concept of ``meta-learning'', i.e. of a system that improves or discovers a learning algorithm, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their attendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.},
  isbn = {978-3-540-44668-2},
  langid = {english},
  keywords = {Boolean Function,Gradient Descent,Hide Layer,Learning Algorithm,Turing Machine},
  file = {C:\Users\kmc07\Zotero\storage\YAE6ZWU7\Hochreiter et al. - 2001 - Learning to Learn Using Gradient Descent.pdf}
}

@incollection{inubushi2021,
  title = {On the {{Characteristics}} and {{Structures}} of~{{Dynamical Systems Suitable}} for~{{Reservoir Computing}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Inubushi, Masanobu and Yoshimura, Kazuyuki and Ikeda, Yoshiaki and Nagasawa, Yuto},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {97--116},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_5},
  urldate = {2024-12-10},
  abstract = {We present an overview of mathematical aspects of Reservoir Computing (RC). RC is a machine learning method suitable for physical implementation, which harnesses a type of synchronization, called Common-Signal-Induced Synchronization. A precise criterion for this synchronization is given by a quantity called the conditional Lyapunov exponent. We describe a class of dynamical systems (physical systems) that are utilizable for RC in terms of this quantity. Then, two notions characterizing the information processing performance of RC are illustrated: (i) Edge of Chaos and (ii) Memory-Nonlinearity Trade-off. Based on the notion (ii), a structure of dynamical systems suitable for RC has been proposed. This structure is called the mixture reservoir. We review the structure and show its remarkable information processing performance.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{ioffe,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\S95L3LIR\Ioffe and Szegedy - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf}
}

@inproceedings{ioffe2015,
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  shorttitle = {Batch Normalization},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = jul,
  series = {{{ICML}}'15},
  pages = {448--456},
  publisher = {JMLR.org},
  address = {Lille, France},
  urldate = {2025-06-01},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.}
}

@article{ji-an2023,
  title = {Face Familiarity Detection with Complex Synapses},
  author = {{Ji-An}, Li and Stefanini, Fabio and Benna, Marcus K. and Fusi, Stefano},
  year = {2023},
  month = jan,
  journal = {iScience},
  volume = {26},
  number = {1},
  pages = {105856},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2022.105856},
  urldate = {2024-11-24},
  abstract = {Synaptic plasticity is a complex phenomenon involving multiple biochemical processes that operate on different timescales. Complexity can greatly increase memory capacity when the variables characterizing the synaptic dynamics have limited precision, as shown in simple memory retrieval problems involving random patterns. Here we turn to a real-world problem, face familiarity detection, and we show that synaptic complexity can be harnessed to store in memory a large number of faces that can be recognized at a later time. The number of recognizable faces grows almost linearly with the number of synapses and quadratically with the number of neurons. Complex synapses outperform simple ones characterized by a single variable, even when the total number of dynamical variables is matched. Complex and simple synapses have distinct signatures that are testable in experiments. Our results indicate that a system with complex synapses can be used in real-world tasks such as face familiarity detection.},
  keywords = {Applied computing,Artificial intelligence,Neuroscience},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\6VCHZD3M\\Ji-An et al. - 2023 - Face familiarity detection with complex synapses.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JYNIVLV7\\S2589004222021290.html}
}

@misc{jimenez2021,
  title = {Feedback {{Alignment Methods}}},
  author = {Jimenez, Albert},
  year = {2021},
  month = sep,
  journal = {Towards Data Science},
  urldate = {2025-06-02},
  abstract = {A biologically-motivated alternative to backpropagation},
  langid = {american},
  file = {C:\Users\kmc07\Zotero\storage\F2XXYDN5\feedback-alignment-methods-7e6c41446e36.html}
}

@inproceedings{jin2020,
  title = {How Does {{Weight Correlation Affect Generalisation Ability}} of {{Deep Neural Networks}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jin, Gaojie and Yi, Xinping and Zhang, Liang and Zhang, Lijun and Schewe, Sven and Huang, Xiaowei},
  year = {2020},
  volume = {33},
  pages = {21346--21356},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-02-17},
  abstract = {This paper studies the novel concept of weight correlation in deep neural networks and discusses its impact on the networks' generalisation ability. For fully-connected layers, the weight correlation is defined as the average cosine similarity between weight vectors of neurons, and for convolutional layers, the weight correlation is defined as the cosine similarity between filter matrices. Theoretically, we show that, weight correlation can, and should, be incorporated into the PAC Bayesian framework for the generalisation of neural networks, and the resulting generalisation bound is monotonic with respect to the weight correlation. We formulate a new complexity measure,  which lifts the PAC Bayes measure  with weight correlation, and experimentally confirm that it is able to rank the generalisation errors of a set of networks more precisely than existing measures. More importantly, we develop a new regulariser for training, and provide extensive experiments that show that the generalisation error can be greatly reduced with our novel approach.},
  file = {C:\Users\kmc07\Zotero\storage\TSWEUGRY\Jin et al. - 2020 - How does Weight Correlation Affect Generalisation Ability of Deep Neural Networks.pdf}
}

@article{johansen2011,
  title = {Molecular {{Mechanisms}} of {{Fear Learning}} and {{Memory}}},
  author = {Johansen, Joshua P. and Cain, Christopher K. and Ostroff, Linnaea E. and LeDoux, Joseph E.},
  year = {2011},
  month = oct,
  journal = {Cell},
  volume = {147},
  number = {3},
  pages = {509--524},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2011.10.009},
  urldate = {2025-01-19},
  abstract = {Pavlovian fear conditioning is a particularly useful behavioral paradigm for exploring the molecular mechanisms of learning and memory because a well-defined response to a specific environmental stimulus is produced through associative learning processes. Synaptic plasticity in the lateral nucleus of the amygdala (LA) underlies this form of associative learning. Here, we summarize the molecular mechanisms that contribute to this synaptic plasticity in the context of auditory fear conditioning, the form of fear conditioning best understood at the molecular level. We discuss the neurotransmitter systems and signaling cascades that contribute to three phases of auditory fear conditioning: acquisition, consolidation, and reconsolidation. These studies suggest that multiple intracellular signaling pathways, including those triggered by activation of Hebbian processes and neuromodulatory receptors, interact to produce neural plasticity in the LA and behavioral fear conditioning. Collectively, this body of research illustrates the power of fear conditioning as a model system for characterizing the mechanisms of learning and memory in mammals and potentially for understanding fear-related disorders, such as PTSD and phobias.},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\BUSPK88G\\Johansen et al. - 2011 - Molecular Mechanisms of Fear Learning and Memory.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\B35S8J4K\\S0092867411012074.html}
}

@misc{kaplanis2018c,
  title = {Continual {{Reinforcement Learning}} with {{Complex Synapses}}},
  author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
  year = {2018},
  month = jun,
  number = {arXiv:1802.07239},
  eprint = {1802.07239},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.07239},
  urldate = {2024-11-24},
  abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna \& Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\IUJ5YR56\\Kaplanis et al. - 2018 - Continual Reinforcement Learning with Complex Synapses.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\7PXRAJMH\\1802.html}
}

@article{karayiannis1996,
  title = {Accelerating the Training of Feedforward Neural Networks Using Generalized {{Hebbian}} Rules for Initializing the Internal Representations},
  author = {Karayiannis, N.B.},
  year = {1996},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {7},
  number = {2},
  pages = {419--426},
  issn = {1941-0093},
  doi = {10.1109/72.485677},
  urldate = {2025-05-21},
  abstract = {This paper presents an unsupervised learning scheme for initializing the internal representations of feedforward neural networks, which accelerates the convergence of supervised learning algorithms. It is proposed in this paper that the initial set of internal representations can be formed through a bottom-up unsupervised learning process applied before the top-down supervised training algorithm. The synaptic weights that connect the input of the network with the hidden units can be determined through linear or nonlinear variations of a generalized Hebbian learning rule, known as Oja's rule. Various generalized Hebbian rules were experimentally tested and evaluated in terms of their effect on the convergence of the supervised training process. Several experiments indicated that the use of the proposed initialization of the internal representations significantly improves the convergence of gradient-descent-based algorithms used to perform nontrivial training tasks. The improvement of the convergence becomes significant as the size and complexity of the training task increase.},
  keywords = {Acceleration,Application software,Backpropagation algorithms,Convergence,Entropy,Feedforward neural networks,Multi-layer neural network,Neural networks,Supervised learning,Unsupervised learning},
  file = {C:\Users\kmc07\Zotero\storage\ZA6FH2UH\Karayiannis - 1996 - Accelerating the training of feedforward neural networks using generalized Hebbian rules for initial.pdf}
}

@article{keysers2014,
  title = {Hebbian Learning and Predictive Mirror Neurons for Actions, Sensations and Emotions},
  author = {Keysers, Christian and Gazzola, Valeria},
  year = {2014},
  month = jun,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1644},
  pages = {20130175},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0175},
  urldate = {2025-06-02},
  abstract = {Spike-timing-dependent plasticity is considered the neurophysiological basis of Hebbian learning and has been shown to be sensitive to both contingency and contiguity between pre- and postsynaptic activity. Here, we will examine how applying this Hebbian learning rule to a system of interconnected neurons in the presence of direct or indirect re-afference (e.g. seeing/hearing one's own actions) predicts the emergence of mirror neurons with predictive properties. In this framework, we analyse how mirror neurons become a dynamic system that performs active inferences about the actions of others and allows joint actions despite sensorimotor delays. We explore how this system performs a projection of the self onto others, with egocentric biases to contribute to mind-reading. Finally, we argue that Hebbian learning predicts mirror-like neurons for sensations and emotions and review evidence for the presence of such vicarious activations outside the motor system.},
  pmcid = {PMC4006178},
  pmid = {24778372},
  file = {C:\Users\kmc07\Zotero\storage\VW86KE5V\Keysers and Gazzola - 2014 - Hebbian learning and predictive mirror neurons for actions, sensations and emotions.pdf}
}

@article{kiefer1952,
  title = {Stochastic {{Estimation}} of the {{Maximum}} of a {{Regression Function}}},
  author = {Kiefer, J. and Wolfowitz, J.},
  year = {1952},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {3},
  pages = {462--466},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729392},
  urldate = {2025-06-02},
  abstract = {Let \$M(x)\$ be a regression function which has a maximum at the unknown point \${\textbackslash}theta. M(x)\$ is itself unknown to the statistician who, however, can take observations at any level \$x\$. This paper gives a scheme whereby, starting from an arbitrary point \$x\_1\$, one obtains successively \$x\_2, x\_3, {\textbackslash}cdots\$ such that \$x\_n\$ converges to \${\textbackslash}theta\$ in probability as \$n {\textbackslash}rightarrow {\textbackslash}infty\$.},
  file = {C:\Users\kmc07\Zotero\storage\6V7KUJIQ\Kiefer and Wolfowitz - 1952 - Stochastic Estimation of the Maximum of a Regression Function.pdf}
}

@misc{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2023-08-29},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\4WMB22Y3\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\SQK2S6L9\\1412.html;C\:\\Users\\kmc07\\Zotero\\storage\\W9VJJ8SV\\1412.html}
}

@misc{kunin2020,
  title = {Two {{Routes}} to {{Scalable Credit Assignment}} without {{Weight Symmetry}}},
  author = {Kunin, Daniel and Nayebi, Aran and {Sagastuy-Brena}, Javier and Ganguli, Surya and Bloom, Jonathan M. and Yamins, Daniel L. K.},
  year = {2020},
  month = jun,
  number = {arXiv:2003.01513},
  eprint = {2003.01513},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.01513},
  urldate = {2025-05-22},
  abstract = {The neural plausibility of backpropagation has long been disputed, primarily for its use of non-local weight transport \$-\$ the biologically dubious requirement that one neuron instantaneously measure the synaptic weights of another. Until recently, attempts to create local learning rules that avoid weight transport have typically failed in the large-scale learning scenarios where backpropagation shines, e.g. ImageNet categorization with deep convolutional networks. Here, we investigate a recently proposed local learning rule that yields competitive performance with backpropagation and find that it is highly sensitive to metaparameter choices, requiring laborious tuning that does not transfer across network architecture. Our analysis indicates the underlying mathematical reason for this instability, allowing us to identify a more robust local learning rule that better transfers without metaparameter tuning. Nonetheless, we find a performance and stability gap between this local rule and backpropagation that widens with increasing model depth. We then investigate several non-local learning rules that relax the need for instantaneous weight transport into a more biologically-plausible "weight estimation" process, showing that these rules match state-of-the-art performance on deep networks and operate effectively in the presence of noisy updates. Taken together, our results suggest two routes towards the discovery of neural implementations for credit assignment without weight symmetry: further improvement of local rules so that they perform consistently across architectures and the identification of biological implementations for non-local learning mechanisms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\E43Z9NTG\\Kunin et al. - 2020 - Two Routes to Scalable Credit Assignment without Weight Symmetry.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JATVYSDE\\2003.html}
}

@misc{lamb2016,
  title = {Professor {{Forcing}}: {{A New Algorithm}} for {{Training Recurrent Networks}}},
  shorttitle = {Professor {{Forcing}}},
  author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = oct,
  number = {arXiv:1610.09038},
  eprint = {1610.09038},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.09038},
  urldate = {2025-02-21},
  abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\TYB5NYBT\\Lamb et al. - 2016 - Professor Forcing A New Algorithm for Training Recurrent Networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\YAM7EJ3G\\1610.html}
}

@inproceedings{launay2020,
  title = {Direct {{Feedback Alignment Scales}} to {{Modern Deep Learning Tasks}} and {{Architectures}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Launay, Julien and Poli, Iacopo and Boniface, Fran{\c c}ois and Krzakala, Florent},
  year = {2020},
  volume = {33},
  pages = {9346--9360},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-02},
  abstract = {Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment (DFA) to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. When a larger gap between DFA and backpropagation exists, like in Transformers, we attribute this to a need to rethink common practices for large and complex architectures. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.},
  file = {C:\Users\kmc07\Zotero\storage\F22FGMHV\Launay et al. - 2020 - Direct Feedback Alignment Scales to Modern Deep Learning Tasks and Architectures.pdf}
}

@article{lazari2022,
  title = {Hebbian Activity-Dependent Plasticity in White Matter},
  author = {Lazari, Alberto and Salvan, Piergiorgio and Cottaar, Michiel and Papp, Daniel and Rushworth, Matthew F.S. and {Johansen-Berg}, Heidi},
  year = {2022},
  month = jun,
  journal = {Cell Reports},
  volume = {39},
  number = {11},
  pages = {110951},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2022.110951},
  urldate = {2025-06-02},
  abstract = {Synaptic plasticity is required for learning and follows Hebb's rule, the computational principle underpinning associative learning. In recent years, a complementary type of brain plasticity has been identified in myelinated axons, which make up the majority of brain's white matter. Like synaptic plasticity, myelin plasticity is required for learning, but it is unclear whether it is Hebbian or whether it follows different rules. Here, we provide evidence that white matter plasticity operates following Hebb's rule in humans. Across two experiments, we find that co-stimulating cortical areas to induce Hebbian plasticity leads to relative increases in cortical excitability and associated increases in a myelin marker within the stimulated fiber bundle. We conclude that Hebbian plasticity extends beyond synaptic changes and can be observed in human white matter fibers., {$\bullet$}Induction of Hebbian plasticity in human brain using non-invasive Hebbian stimulation{$\bullet$}Hebbian plasticity induction leads to changes within the stimulated fiber bundle{$\bullet$}Anatomically relevant behavioral changes in action reprogramming{$\bullet$}Activity-dependent white matter plasticity operates following Hebb's rule, Lazari et~al. demonstrate that Hebbian activity-dependent plasticity extends beyond synaptic changes and can be observed in human white-matter fibers.},
  pmcid = {PMC9376741},
  pmid = {35705046},
  file = {C:\Users\kmc07\Zotero\storage\QYNISIJ3\Lazari et al. - 2022 - Hebbian activity-dependent plasticity in white matter.pdf}
}

@misc{le2015,
  title = {A {{Simple Way}} to {{Initialize Recurrent Networks}} of {{Rectified Linear Units}}},
  author = {Le, Quoc V. and Jaitly, Navdeep and Hinton, Geoffrey E.},
  year = {2015},
  month = apr,
  number = {arXiv:1504.00941},
  eprint = {1504.00941},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.00941},
  urldate = {2025-02-21},
  abstract = {Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to a standard implementation of LSTMs on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\kmc07\Zotero\storage\6SBKI28H\Le et al. - 2015 - A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.pdf}
}

@article{liao2016,
  title = {How {{Important Is Weight Symmetry}} in {{Backpropagation}}?},
  author = {Liao, Qianli and Leibo, Joel and Poggio, Tomaso},
  year = {2016},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v30i1.10279},
  urldate = {2025-05-20},
  abstract = {Gradient backpropagation (BP) requires symmetric feedforward and feedback connections---the same weights must be used for forward and backward passes. This ``weight transport problem'' (Grossberg 1987) is thought to be one of the main reasons to doubt BP's biologically plausibility. Using 15 different classification datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter---the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100\% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a ``Batch Manhattan'' (BM) update rule.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\R4RZWVXE\Liao et al. - 2016 - How Important Is Weight Symmetry in Backpropagation.pdf}
}

@article{lillicrap2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year = {2016},
  month = nov,
  journal = {Nature Communications},
  volume = {7},
  pages = {13276},
  issn = {2041-1723},
  doi = {10.1038/ncomms13276},
  urldate = {2025-01-10},
  abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.,  Multi-layered neural architectures that implement learning require elaborate mechanisms for symmetric backpropagation of errors that are biologically implausible. Here the authors propose a simple resolution to this problem of blame assignment that works even with feedback using random synaptic weights.},
  pmcid = {PMC5105169},
  pmid = {27824044},
  keywords = {Learning algorithms},
  file = {C:\Users\kmc07\Zotero\storage\8QGQM4M4\Lillicrap et al. - 2016 - Random synaptic feedback weights support error backpropagation for deep learning.pdf}
}

@article{lillicrap2020,
  title = {Backpropagation and the Brain},
  author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {Nature Reviews Neuroscience},
  volume = {21},
  number = {6},
  pages = {335--346},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-020-0277-3},
  urldate = {2025-06-02},
  abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Cortex,Learning algorithms,Long-term potentiation,Network models,Neurophysiology},
  file = {C:\Users\kmc07\Zotero\storage\WDE6ZWSG\Lillicrap et al. - 2020 - Backpropagation and the brain.pdf}
}

@article{lloyd1982,
  title = {Least Squares Quantization in {{PCM}}},
  author = {Lloyd, S.},
  year = {1982},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {2},
  pages = {129--137},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056489},
  urldate = {2025-06-02},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  file = {C:\Users\kmc07\Zotero\storage\BJXKLQ4S\Lloyd - 1982 - Least squares quantization in PCM.pdf}
}

@misc{lv2024,
  title = {Towards {{Biologically Plausible Computing}}: {{A Comprehensive Comparison}}},
  shorttitle = {Towards {{Biologically Plausible Computing}}},
  author = {Lv, Changze and Gu, Yufei and Guo, Zhengkang and Xu, Zhibo and Wu, Yixin and Zhang, Feiran and Shi, Tianyuan and Wang, Zhenghua and Yin, Ruicheng and Shang, Yu and Zhong, Siqi and Wang, Xiaohua and Wu, Muling and Liu, Wenhao and Li, Tianlong and Zhu, Jianhao and Zhang, Cenyuan and Ling, Zixuan and Zheng, Xiaoqing},
  year = {2024},
  month = jun,
  number = {arXiv:2406.16062},
  eprint = {2406.16062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.16062},
  urldate = {2025-01-19},
  abstract = {Backpropagation is a cornerstone algorithm in training neural networks for supervised learning, which uses a gradient descent method to update network weights by minimizing the discrepancy between actual and desired outputs. Despite its pivotal role in propelling deep learning advancements, the biological plausibility of backpropagation is questioned due to its requirements for weight symmetry, global error computation, and dual-phase training. To address this long-standing challenge, many studies have endeavored to devise biologically plausible training algorithms. However, a fully biologically plausible algorithm for training multilayer neural networks remains elusive, and interpretations of biological plausibility vary among researchers. In this study, we establish criteria for biological plausibility that a desirable learning algorithm should meet. Using these criteria, we evaluate a range of existing algorithms considered to be biologically plausible, including Hebbian learning, spike-timing-dependent plasticity, feedback alignment, target propagation, predictive coding, forward-forward algorithm, perturbation learning, local losses, and energy-based learning. Additionally, we empirically evaluate these algorithms across diverse network architectures and datasets. We compare the feature representations learned by these algorithms with brain activity recorded by non-invasive devices under identical stimuli, aiming to identify which algorithm can most accurately replicate brain activity patterns. We are hopeful that this study could inspire the development of new biologically plausible algorithms for training multilayer networks, thereby fostering progress in both the fields of neuroscience and machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\S4NZ7V4Z\\Lv et al. - 2024 - Towards Biologically Plausible Computing A Comprehensive Comparison.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\JHIHBCIU\\2406.html}
}

@misc{marschall2019,
  title = {A {{Unified Framework}} of {{Online Learning Algorithms}} for {{Training Recurrent Neural Networks}}},
  author = {Marschall, Owen and Cho, Kyunghyun and Savin, Cristina},
  year = {2019},
  month = jul,
  number = {arXiv:1907.02649},
  eprint = {1907.02649},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1907.02649},
  urldate = {2025-03-08},
  abstract = {We present a framework for compactly summarizing many recent results in efficient and/or biologically plausible online training of recurrent neural networks (RNN). The framework organizes algorithms according to several criteria: (a) past vs. future facing, (b) tensor structure, (c) stochastic vs. deterministic, and (d) closed form vs. numerical. These axes reveal latent conceptual connections among several recent advances in online learning. Furthermore, we provide novel mathematical intuitions for their degree of success. Testing various algorithms on two synthetic tasks shows that performances cluster according to our criteria. Although a similar clustering is also observed for gradient alignment, alignment with exact methods does not alone explain ultimate performance, especially for stochastic algorithms. This suggests the need for better comparison metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\NGSXIIB3\\Marschall et al. - 2019 - A Unified Framework of Online Learning Algorithms for Training Recurrent Neural Networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\MFMJP8BN\\1907.html}
}

@article{mcmillan2024,
  title = {Spike-{{Weighted Spiking Neural Network}} with {{Spiking Long Short-Term Memory}}: {{A Biomimetic Approach}} to {{Decoding Brain Signals}}},
  shorttitle = {Spike-{{Weighted Spiking Neural Network}} with {{Spiking Long Short-Term Memory}}},
  author = {McMillan, Kyle and So, Rosa Qiyue and Libedinsky, Camilo and Ang, Kai Keng and Premchand, Brian},
  year = {2024},
  month = apr,
  journal = {Algorithms},
  volume = {17},
  number = {4},
  pages = {156},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1999-4893},
  doi = {10.3390/a17040156},
  urldate = {2025-06-02},
  abstract = {Background. Brain--machine interfaces (BMIs) offer users the ability to directly communicate with digital devices through neural signals decoded with machine learning (ML)-based algorithms. Spiking Neural Networks (SNNs) are a type of Artificial Neural Network (ANN) that operate on neural spikes instead of continuous scalar outputs. Compared to traditional ANNs, SNNs perform fewer computations, use less memory, and mimic biological neurons better. However, SNNs only retain information for short durations, limiting their ability to capture long-term dependencies in time-variant data. Here, we propose a novel spike-weighted SNN with spiking long short-term memory (swSNN-SLSTM) for a regression problem. Spike-weighting captures neuronal firing rate instead of membrane potential, and the SLSTM layer captures long-term dependencies. Methods. We compared the performance of various ML algorithms during decoding directional movements, using a dataset of microelectrode recordings from a macaque during a directional joystick task, and also an open-source dataset. We thus quantified how swSNN-SLSTM performed compared to existing ML models: an unscented Kalman filter, LSTM-based ANN, and membrane-based SNN techniques. Result. The proposed swSNN-SLSTM outperforms both the unscented Kalman filter, the LSTM-based ANN, and the membrane based SNN technique. This shows that incorporating SLSTM can better capture long-term dependencies within neural data. Also, our proposed swSNN-SLSTM algorithm shows promise in reducing power consumption and lowering heat dissipation in implanted BMIs.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {BCI,BMI,brain-computer interface,brain-machine interface,decoding,machine learning,neuroscience,SNN,spiking neural network},
  file = {C:\Users\kmc07\Zotero\storage\UXRA72SN\McMillan et al. - 2024 - Spike-Weighted Spiking Neural Network with Spiking Long Short-Term Memory A Biomimetic Approach to.pdf}
}

@article{metz2019,
  title = {{{META-LEARNING UPDATE RULES FOR UNSUPER- VISED REPRESENTATION LEARNING}}},
  author = {Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and {Sohl-Dickstein}, Jascha},
  year = {2019},
  abstract = {A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm --an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\L86ZJU64\Metz et al. - 2019 - META-LEARNING UPDATE RULES FOR UNSUPER- VISED REPRESENTATION LEARNING.pdf}
}

@article{morris1999,
  title = {D.{{O}}. {{Hebb}}: {{The Organization}} of {{Behavior}}, {{Wiley}}: {{New York}}; 1949},
  shorttitle = {D.{{O}}. {{Hebb}}},
  author = {Morris, R. G.},
  year = {1999},
  journal = {Brain Research Bulletin},
  volume = {50},
  number = {5-6},
  pages = {437},
  issn = {0361-9230},
  doi = {10.1016/s0361-9230(99)00182-3},
  langid = {english},
  pmid = {10643472},
  keywords = {Animals,Behavior Animal,Cognitive Science,History 20th Century,Neurosciences,Publishing}
}

@misc{moskovitz2019,
  title = {Feedback Alignment in Deep Convolutional Networks},
  author = {Moskovitz, Theodore H. and {Litwin-Kumar}, Ashok and Abbott, L. F.},
  year = {2019},
  month = jun,
  number = {arXiv:1812.06488},
  eprint = {1812.06488},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.06488},
  urldate = {2025-02-04},
  abstract = {Ongoing studies have identified similarities between neural representations in biological networks and in deep artificial neural networks. This has led to renewed interest in developing analogies between the backpropagation learning algorithm used to train artificial networks and the synaptic plasticity rules operative in the brain. These efforts are challenged by biologically implausible features of backpropagation, one of which is a reliance on symmetric forward and backward synaptic weights. A number of methods have been proposed that do not rely on weight symmetry but, thus far, these have failed to scale to deep convolutional networks and complex data. We identify principal obstacles to the scalability of such algorithms and introduce several techniques to mitigate them. We demonstrate that a modification of the feedback alignment method that enforces a weaker form of weight symmetry, one that requires agreement of weight sign but not magnitude, can achieve performance competitive with backpropagation. Our results complement those of Bartunov et al. (2018) and Xiao et al. (2018b) and suggest that mechanisms that promote alignment of feedforward and feedback weights are critical for learning in deep networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\JNKGKBME\\Moskovitz et al. - 2019 - Feedback alignment in deep convolutional networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\VEV2TYQB\\1812.html}
}

@article{murray,
  title = {Local Online Learning in Recurrent Networks with Random Feedback},
  author = {Murray, James M},
  journal = {eLife},
  volume = {8},
  pages = {e43299},
  issn = {2050-084X},
  doi = {10.7554/eLife.43299},
  urldate = {2025-03-08},
  abstract = {Recurrent neural networks (RNNs) enable the production and processing of time-dependent signals such as those involved in movement or working memory. Classic gradient-based algorithms for training RNNs have been available for decades, but are inconsistent with biological features of the brain, such as causality and locality. We derive an approximation to gradient-based learning that comports with these constraints by requiring synaptic weight updates to depend only on local information about pre- and postsynaptic activities, in addition to a random feedback projection of the RNN output error. In addition to providing mathematical arguments for the effectiveness of the new learning rule, we show through simulations that it can be used to train an RNN to perform a variety of tasks. Finally, to overcome the difficulty of training over very large numbers of timesteps, we propose an augmented circuit architecture that allows the RNN to concatenate short-duration patterns into longer sequences.},
  pmcid = {PMC6561704},
  pmid = {31124785},
  file = {C:\Users\kmc07\Zotero\storage\PJZKURU6\Murray - Local online learning in recurrent networks with random feedback.pdf}
}

@book{nakajima2021,
  title = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  shorttitle = {Reservoir {{Computing}}},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  series = {Natural {{Computing Series}}},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6},
  urldate = {2024-12-10},
  copyright = {https://www.springer.com/tdm},
  isbn = {978-981-13-1686-9 978-981-13-1687-6},
  langid = {english},
  keywords = {dynamical system,Machine Learning,Neural Networks,Reservoir Computing,Signal Processing,Soft Robotics,spintronics}
}

@misc{nokland2016,
  title = {Direct {{Feedback Alignment Provides Learning}} in {{Deep Neural Networks}}},
  author = {N{\o}kland, Arild},
  year = {2016},
  month = dec,
  number = {arXiv:1609.01596},
  eprint = {1609.01596},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1609.01596},
  urldate = {2025-05-23},
  abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45\% error on the permutation invariant MNIST task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\9JUCPNVJ\\N√∏kland - 2016 - Direct Feedback Alignment Provides Learning in Deep Neural Networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\Q3LFZ8I3\\1609.html}
}

@inproceedings{nokland2016a,
  title = {Direct {{Feedback Alignment Provides Learning}} in {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{N{\o} kland}, Arild},
  year = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-05-23},
  abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45\% error on the permutation invariant MNIST task.},
  file = {C:\Users\kmc07\Zotero\storage\Z6GSAZ59\N√∏ kland - 2016 - Direct Feedback Alignment Provides Learning in Deep Neural Networks.pdf}
}

@article{oja1992,
  title = {Principal Components, Minor Components, and Linear Neural Networks},
  author = {Oja, Erkki},
  year = {1992},
  month = nov,
  journal = {Neural Networks},
  volume = {5},
  number = {6},
  pages = {927--935},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(05)80089-9},
  urldate = {2025-05-17},
  abstract = {Many neural network realizations have been recently proposed for the statistical technique of Principal Component Analysis (PCA). Explicit connections between numerical constrained adaptive algorithms and neural networks with constrained Hebbian learning rules are reviewed. The Stochastic Gradient Ascent (SGA) neural network is proposed and shown to be closely related to the Generalized Hebbian Algorithm (GHA). The SGA behaves better for extracting the less dominant eigenvectors. The SGA algorithm is further extended to the case of learning minor components. The symmetrical Subspace Network is known to give a rotated basis of the dominant eigenvector subspace, but usually not the true eigenvectors themselves. Two extensions are proposed: in the first one, each neuron has a scalar parameter which breaks the symmetry. True eigenvectors are obtained in a local and fully parallel learning rule. In the second one, the case of an arbitrary number of parallel neurons is considered, not necessarily less than the input vector dimension.},
  keywords = {Eigenvector,Generalized Hebbian Algorithm,Minor components,Neural networks,Stochastic gradient ascent,Subspace network},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\L4QUI5G3\\Oja - 1992 - Principal components, minor components, and linear neural networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\XMC4EMLC\\Oja - 1992 - Principal components, minor components, and linear neural networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\87RVVA4M\\S0893608005800899.html}
}

@incollection{pathak2021,
  title = {Reservoir {{Computing}} for {{Forecasting Large Spatiotemporal Dynamical Systems}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Pathak, Jaideep and Ott, Edward},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {117--138},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_6},
  urldate = {2024-12-10},
  abstract = {Forecasting of spatiotemporal chaotic dynamical systems is an important problem in several scientific fields. Crucial scientific applications such as weather forecasting and climate modeling depend on the ability to effectively model spatiotemporal chaotic geophysical systems such as the atmosphere and oceans. Recent advances in the field of machine learning have the potential to be an important tool for modeling such systems. In this chapter, we review several key ideas and discuss some reservoir-computing-based architectures for purely data-driven as well as hybrid data-assisted forecasting of chaotic systems with an emphasis on scalability to large, high-dimensional systems.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{purkey2020,
  title = {Phosphorylation-{{Dependent Regulation}} of {{Ca2}}+-{{Permeable AMPA Receptors During Hippocampal Synaptic Plasticity}}},
  author = {Purkey, Alicia M. and Dell'Acqua, Mark L.},
  year = {2020},
  month = mar,
  journal = {Frontiers in Synaptic Neuroscience},
  volume = {12},
  publisher = {Frontiers},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2020.00008},
  urldate = {2025-05-19},
  abstract = {Experience-dependent learning and memory require multiple forms of plasticity at hippocampal and cortical synapses that are regulated by N-methyl-D-aspartate receptors (NMDA) and {$\alpha$}-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA)-type ionotropic glutamate receptors (NMDAR, AMPAR). These plasticity mechanisms include long-term potentiation (LTP) and depression (LTD), which are Hebbian input-specific mechanisms that rapidly increase or decrease AMPAR synaptic strength at specific inputs, and homeostatic plasticity that globally scales-up or -down AMPAR synaptic strength across many or even all inputs. Frequently, these changes in synaptic strength are also accompanied by a change in the subunit composition of AMPARs at the synapse due to the trafficking to and from the synapse of receptors lacking GluA2 subunits. These GluA2-lacking receptors are most often GluA1 homomeric receptors that exhibit higher single-channel conductance and are Ca2+-permeable (CP-AMPAR). This review article will focus on the role of protein phosphorylation in regulation of GluA1 CP-AMPAR recruitment and removal from hippocampal synapses during synaptic plasticity with an emphasis on the crucial role of local signaling by the cAMP-dependent protein kinase (PKA) and the Ca2+calmodulin-dependent protein phosphatase 2B/calcineurin (CaN) that is coordinated by the postsynaptic scaffold protein A-kinase anchoring protein 79/150 (AKAP79/150).},
  langid = {english},
  keywords = {A-kinase anchoring protein 79/150,AKAP79/150,Ca2+-permeable AMPA receptor,Calcineurin (CaN),Phosphorylation,PKA,Synaptic plasticity (LTP/LTD)},
  file = {C:\Users\kmc07\Zotero\storage\CSAY3B3E\Purkey and Dell‚ÄôAcqua - 2020 - Phosphorylation-Dependent Regulation of Ca2+-Permeable AMPA Receptors During Hippocampal Synaptic Pl.pdf}
}

@article{ramiro-cortes2014,
  title = {Synaptic Competition in Structural Plasticity and Cognitive Function},
  author = {{Ramiro-Cort{\'e}s}, Yazm{\'i}n and Hobbiss, Anna F. and Israely, Inbal},
  year = {2014},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1633},
  pages = {20130157},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0157},
  urldate = {2025-06-01},
  abstract = {Connections between neurons can undergo long-lasting changes in synaptic strength correlating with changes in structure. These events require the synthesis of new proteins, the availability of which can lead to cooperative and competitive interactions between synapses for the expression of plasticity. These processes can occur over limited spatial distances and temporal periods, defining dendritic regions over which activity may be integrated and could lead to the physical rewiring of synapses into functional groups. Such clustering of inputs may increase the computational power of neurons by allowing information to be combined in a greater than additive manner. The availability of new proteins may be a key modulatory step towards activity-dependent, long-term growth or elimination of spines necessary for remodelling of connections. Thus, the aberrant growth or shrinkage of dendritic spines could occur if protein levels are misregulated. Indeed, such perturbations can be seen in several mental retardation disorders, wherein either too much or too little protein translation exists, matching an observed increase or decrease in spine density, respectively. Cellular events which alter protein availability could relieve a constraint on synaptic competition and disturb synaptic clustering mechanisms. These changes may be detrimental to modifications in neural circuitry following activity.},
  pmcid = {PMC3843888},
  pmid = {24298158},
  file = {C:\Users\kmc07\Zotero\storage\AMWFUUHQ\Ramiro-Cort√©s et al. - 2014 - Synaptic competition in structural plasticity and cognitive function.pdf}
}

@article{ramiro-cortes2014a,
  title = {Synaptic Competition in Structural Plasticity and Cognitive Function},
  author = {{Ramiro-Cort{\'e}s}, Yazm{\'i}n and Hobbiss, Anna F. and Israely, Inbal},
  year = {2014},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {369},
  number = {1633},
  pages = {20130157},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0157},
  urldate = {2025-06-02},
  abstract = {Connections between neurons can undergo long-lasting changes in synaptic strength correlating with changes in structure. These events require the synthesis of new proteins, the availability of which can lead to cooperative and competitive interactions between synapses for the expression of plasticity. These processes can occur over limited spatial distances and temporal periods, defining dendritic regions over which activity may be integrated and could lead to the physical rewiring of synapses into functional groups. Such clustering of inputs may increase the computational power of neurons by allowing information to be combined in a greater than additive manner. The availability of new proteins may be a key modulatory step towards activity-dependent, long-term growth or elimination of spines necessary for remodelling of connections. Thus, the aberrant growth or shrinkage of dendritic spines could occur if protein levels are misregulated. Indeed, such perturbations can be seen in several mental retardation disorders, wherein either too much or too little protein translation exists, matching an observed increase or decrease in spine density, respectively. Cellular events which alter protein availability could relieve a constraint on synaptic competition and disturb synaptic clustering mechanisms. These changes may be detrimental to modifications in neural circuitry following activity.},
  pmcid = {PMC3843888},
  pmid = {24298158},
  file = {C:\Users\kmc07\Zotero\storage\89LWTXVJ\Ramiro-Cort√©s et al. - 2014 - Synaptic competition in structural plasticity and cognitive function.pdf}
}

@misc{refinetti2021,
  title = {Align, Then Memorise: The Dynamics of Learning with Feedback Alignment},
  shorttitle = {Align, Then Memorise},
  author = {Refinetti, Maria and {d'Ascoli}, St{\'e}phane and Ohana, Ruben and Goldt, Sebastian},
  year = {2021},
  month = jun,
  number = {arXiv:2011.12428},
  eprint = {2011.12428},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.12428},
  urldate = {2025-05-23},
  abstract = {Direct Feedback Alignment (DFA) is emerging as an efficient and biologically plausible alternative to the ubiquitous backpropagation algorithm for training deep neural networks. Despite relying on random feedback weights for the backward pass, DFA successfully trains state-of-the-art models such as Transformers. On the other hand, it notoriously fails to train convolutional networks. An understanding of the inner workings of DFA to explain these diverging results remains elusive. Here, we propose a theory for the success of DFA. We first show that learning in shallow networks proceeds in two steps: an alignment phase, where the model adapts its weights to align the approximate gradient with the true gradient of the loss function, is followed by a memorisation phase, where the model focuses on fitting the data. This two-step process has a degeneracy breaking effect: out of all the low-loss solutions in the landscape, a network trained with DFA naturally converges to the solution which maximises gradient alignment. We also identify a key quantity underlying alignment in deep linear networks: the conditioning of the alignment matrices. The latter enables a detailed understanding of the impact of data structure on alignment, and suggests a simple explanation for the well-known failure of DFA to train convolutional neural networks. Numerical experiments on MNIST and CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and show that the align-then-memorise process occurs sequentially from the bottom layers of the network to the top.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\W46JC5AB\\Refinetti et al. - 2021 - Align, then memorise the dynamics of learning with feedback alignment.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\F3TZZ587\\2011.html}
}

@inproceedings{refinetti2021a,
  title = {Align, Then Memorise: The Dynamics of Learning with Feedback Alignment},
  shorttitle = {Align, Then Memorise},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Refinetti, Maria and D'Ascoli, St{\'e}phane and Ohana, Ruben and Goldt, Sebastian},
  year = {2021},
  month = jul,
  pages = {8925--8935},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-06-02},
  abstract = {Direct Feedback Alignment (DFA) is emerging as an efficient and biologically plausible alternative to backpropagation for training deep neural networks. Despite relying on random feedback weights for the backward pass, DFA successfully trains state-of-the-art models such as Transformers. On the other hand, it notoriously fails to train convolutional networks. An understanding of the inner workings of DFA to explain these diverging results remains elusive. Here, we propose a theory of feedback alignment algorithms. We first show that learning in shallow networks proceeds in two steps: an alignment phase, where the model adapts its weights to align the approximate gradient with the true gradient of the loss function, is followed by a memorisation phase, where the model focuses on fitting the data. This two-step process has a degeneracy breaking effect: out of all the low-loss solutions in the landscape, a net-work trained with DFA naturally converges to the solution which maximises gradient alignment. We also identify a key quantity underlying alignment in deep linear networks: the conditioning of the alignment matrices. The latter enables a detailed understanding of the impact of data structure on alignment, and suggests a simple explanation for the well-known failure of DFA to train convolutional neural networks. Numerical experiments on MNIST and CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and show that the align-then-memorize process occurs sequentially from the bottom layers of the network to the top.},
  langid = {english},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\I8QEZ9WS\\Refinetti et al. - 2021 - Align, then memorise the dynamics of learning with feedback alignment.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\TM6GZNRQ\\Refinetti et al. - 2021 - Align, then memorise the dynamics of learning with feedback alignment.pdf}
}

@article{richards2019,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and {de Berker}, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Ken and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  year = {2019},
  month = nov,
  journal = {Nature neuroscience},
  volume = {22},
  number = {11},
  pages = {1761--1770},
  issn = {1097-6256},
  doi = {10.1038/s41593-019-0520-2},
  urldate = {2025-01-10},
  abstract = {Systems neuroscience seeks explanations for how the brain implements a wide variety of perceptual, cognitive and motor tasks. Conversely, artificial intelligence attempts to design computational systems based on the tasks they will have to solve. In the case of artificial neural networks, the three components specified by design are the objective functions, the learning rules, and architectures. With the growing success of deep learning, which utilizes brain-inspired architectures, these three designed components have increasingly become central to how we model, engineer and optimize complex artificial learning systems. Here we argue that a greater focus on these components would also benefit systems neuroscience. We give examples of how this optimization-based framework can drive theoretical and experimental progress in neuroscience. We contend that this principled perspective on systems neuroscience will help to generate more rapid progress.},
  pmcid = {PMC7115933},
  pmid = {31659335},
  file = {C:\Users\kmc07\Zotero\storage\AZHYBG6B\Richards et al. - 2019 - A deep learning framework for neuroscience.pdf}
}

@article{roelfsema2005,
  title = {Attention-{{Gated Reinforcement Learning}} of {{Internal Representations}} for {{Classification}}},
  author = {Roelfsema, Pieter R. and van Ooyen, Arjen},
  year = {2005},
  month = oct,
  journal = {Neural Computation},
  volume = {17},
  number = {10},
  pages = {2176--2214},
  issn = {0899-7667},
  doi = {10.1162/0899766054615699},
  urldate = {2025-01-10},
  abstract = {Animal learning is associated with changes in the efficacy of connections between neurons. The rules that govern this plasticity can be tested in neural networks. Rules that train neural networks to map stimuli onto outputs are given by supervised learning and reinforcement learning theories. Supervised learning is efficient but biologically implausible. In contrast, reinforcement learning is biologically plausible but comparatively inefficient. It lacks a mechanism that can identify units at early processing levels that play a decisive role in the stimulus-response mapping. Here we show that this so-called credit assignment problem can be solved by a new role for attention in learning. There are two factors in our new learning scheme that determine synaptic plasticity: (1) a reinforcement signal that is homogeneous across the network and depends on the amount of reward obtained after a trial, and (2) an attentional feedback signal from the output layer that limits plasticity to those units at earlier processing levels that are crucial for the stimulus-response mapping. The new scheme is called attention-gated reinforcement learning (AGREL). We show that it is as efficient as supervised learning in classification tasks. AGREL is biologically realistic and integrates the role of feedback connections, attention effects, synaptic plasticity, and reinforcement learning signals into a coherent framework.},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\MCP2GY7J\\Roelfsema and Ooyen - 2005 - Attention-Gated Reinforcement Learning of Internal Representations for Classification.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\YUXFACRK\\Attention-Gated-Reinforcement-Learning-of-Internal.html}
}

@article{rumelhart1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2025-06-02},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {C:\Users\kmc07\Zotero\storage\F9MIAW56\Rumelhart et al. - 1986 - Learning representations by back-propagating errors.pdf}
}

@misc{sandler2021,
  title = {Meta-{{Learning Bidirectional Update Rules}}},
  author = {Sandler, Mark and Vladymyrov, Max and Zhmoginov, Andrey and Miller, Nolan and Jackson, Andrew and Madams, Tom and y Arcas, Blaise Aguera},
  year = {2021},
  month = jun,
  number = {arXiv:2104.04657},
  eprint = {2104.04657},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional "genome". We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\5DLI3HPT\\Sandler et al. - 2021 - Meta-Learning Bidirectional Update Rules.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\TQ5WGHBF\\2104.html}
}

@inproceedings{santurkar2018,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  year = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-06-02},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  file = {C:\Users\kmc07\Zotero\storage\P96E57W6\Santurkar et al. - 2018 - How Does Batch Normalization Help Optimization.pdf}
}

@misc{scellier2017,
  title = {Equilibrium {{Propagation}}: {{Bridging}} the {{Gap Between Energy-Based Models}} and {{Backpropagation}}},
  shorttitle = {Equilibrium {{Propagation}}},
  author = {Scellier, Benjamin and Bengio, Yoshua},
  year = {2017},
  month = mar,
  number = {arXiv:1602.05179},
  eprint = {1602.05179},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.05179},
  urldate = {2025-01-10},
  abstract = {We introduce Equilibrium Propagation, a learning framework for energy-based models. It involves only one kind of neural computation, performed in both the first phase (when the prediction is made) and the second phase of training (after the target or prediction error is revealed). Although this algorithm computes the gradient of an objective function just like Backpropagation, it does not need a special computation or circuit for the second phase, where errors are implicitly propagated. Equilibrium Propagation shares similarities with Contrastive Hebbian Learning and Contrastive Divergence while solving the theoretical issues of both algorithms: our algorithm computes the gradient of a well defined objective function. Because the objective function is defined in terms of local perturbations, the second phase of Equilibrium Propagation corresponds to only nudging the prediction (fixed point, or stationary distribution) towards a configuration that reduces prediction error. In the case of a recurrent multi-layer supervised network, the output units are slightly nudged towards their target in the second phase, and the perturbation introduced at the output layer propagates backward in the hidden layers. We show that the signal 'back-propagated' during this second phase corresponds to the propagation of error derivatives and encodes the gradient of the objective function, when the synaptic update corresponds to a standard form of spike-timing dependent plasticity. This work makes it more plausible that a mechanism similar to Backpropagation could be implemented by brains, since leaky integrator neural computation performs both inference and error back-propagation in our model. The only local difference between the two phases is whether synaptic changes are allowed or not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\R4RRBU73\\Scellier and Bengio - 2017 - Equilibrium Propagation Bridging the Gap Between Energy-Based Models and Backpropagation.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\Z95MXV3A\\1602.html}
}

@article{shao2023,
  title = {How Can Artificial Neural Networks Approximate the Brain?},
  author = {Shao, Feng and Shen, Zheng},
  year = {2023},
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {13},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2022.970214},
  urldate = {2025-06-02},
  abstract = {The article reviews the history development of artificial neural networks (ANNs), then compares the differences between ANNs and brain networks in their constituent unit, network architecture, and dynamic principle. The authors offer five points of suggestion for ANNs development and eight points of challenge for the interdisciplinary field of brain simulation. Even though brain is a super-complex system with 10 11 neurons, its intelligence does depend rather on the neuronal types and their energy supply mode than the number of neurons. It might be possible for ANN development to follow a new direction that is a combination of multiple modules with different architecture principle and multiple computation, rather than very large scale of neural networks with much more uniformed units and hidden layers.},
  langid = {english},
  keywords = {dual neural node,emergent computation,energy source mode,executive control to socially cognitive behavior,Hierarchical Architecture,neuron type,Spike-time encoding},
  file = {C:\Users\kmc07\Zotero\storage\3GPZFWBN\Shao and Shen - 2023 - How can artificial neural networks approximate the brain.pdf}
}

@article{shervani-tabar2023,
  title = {Meta-Learning Biologically Plausible Plasticity Rules with Random Feedback Pathways},
  author = {{Shervani-Tabar}, Navid and Rosenbaum, Robert},
  year = {2023},
  month = mar,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {1805},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-37562-1},
  urldate = {2024-09-21},
  abstract = {Backpropagation is widely used to train artificial neural networks, but its relationship to synaptic plasticity in the brain is unknown. Some biological models of backpropagation rely on feedback projections that are symmetric with feedforward connections, but experiments do not corroborate the existence of such symmetric backward connectivity. Random feedback alignment offers an alternative model in which errors are propagated backward through fixed, random backward connections. This approach successfully trains shallow models, but learns slowly and does not perform well with deeper models or online learning. In this study, we develop a meta-learning approach to discover interpretable, biologically plausible plasticity rules that improve online learning performance with fixed random feedback connections. The resulting plasticity rules show improved online training of deep models in the low data regime. Our results highlight the potential of meta-learning to discover effective, interpretable learning rules satisfying biological constraints.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational science,Learning algorithms},
  file = {C:\Users\kmc07\Zotero\storage\E6MFRN9F\Shervani-Tabar and Rosenbaum - 2023 - Meta-learning biologically plausible plasticity rules with random feedback pathways.pdf}
}

@misc{shervani-tabar2024,
  title = {Oja's Plasticity Rule Overcomes Several Challenges of Training Neural Networks under Biological Constraints},
  author = {{Shervani-Tabar}, Navid and Mirhoseini, Marzieh Alireza and Rosenbaum, Robert},
  year = {2024},
  month = aug,
  number = {arXiv:2408.08408},
  eprint = {2408.08408},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.08408},
  urldate = {2025-05-21},
  abstract = {There is a large literature on the similarities and differences between biological neural circuits and deep artificial neural networks (DNNs). However, modern training of DNNs relies on several engineering tricks such as data batching, normalization, adaptive optimizers, and precise weight initialization. Despite their critical role in training DNNs, these engineering tricks are often overlooked when drawing parallels between biological and artificial networks, potentially due to a lack of evidence for their direct biological implementation. In this study, we show that Oja's plasticity rule partly overcomes the need for some engineering tricks. Specifically, under difficult, but biologically realistic learning scenarios such as online learning, deep architectures, and sub-optimal weight initialization, Oja's rule can substantially improve the performance of pure backpropagation. Our results demonstrate that simple synaptic plasticity rules can overcome challenges to learning that are typically overcome using less biologically plausible approaches when training DNNs.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\56LCFMTL\\Shervani-Tabar et al. - 2024 - Oja's plasticity rule overcomes several challenges of training neural networks under biological cons.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\3FYTNEPC\\2408.html}
}

@incollection{singer2021,
  title = {The {{Cerebral Cortex}}: {{A Delay-Coupled Recurrent Oscillator Network}}?},
  shorttitle = {The {{Cerebral Cortex}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Singer, Wolf},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {3--28},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_1},
  urldate = {2024-12-10},
  abstract = {The refinement of machine learning strategies and deep convolutional networks led to the development of artificial systems whose functions resemble those of natural brains, suggesting that the two systems share the same computational principles. In this chapter, evidence is reviewed which indicates that the computational operations of natural systems differ in some important aspects from those implemented in artificial systems. Natural processing architectures are characterized by recurrence and therefore exhibit high-dimensional, non-linear dynamics. Moreover, they use learning mechanisms that support self-organization. It is proposed that these properties allow for computations that are notoriously difficult to realize in artificial systems. Experimental evidence on the organization and function of the cerebral cortex is reviewed that supports this proposal.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{soo2023,
  title = {Training Biologically Plausible Recurrent Neural Networks on Cognitive Tasks with Long-Term Dependencies},
  author = {Soo, Wayne W.M. and Goudar, Vishwa and Wang, Xiao-Jing},
  year = {2023},
  month = oct,
  journal = {bioRxiv},
  pages = {2023.10.10.561588},
  doi = {10.1101/2023.10.10.561588},
  urldate = {2025-04-10},
  abstract = {Training recurrent neural networks (RNNs) has become a go-to approach for generating and evaluating mechanistic neural hypotheses for cognition. The ease and efficiency of training RNNs with backpropagation through time and the availability of robustly supported deep learning libraries has made RNN modeling more approachable and accessible to neuroscience. Yet, a major technical hindrance remains. Cognitive processes such as working memory and decision making involve neural population dynamics over a long period of time within a behavioral trial and across trials. It is difficult to train RNNs to accomplish tasks where neural representations and dynamics have long temporal dependencies without gating mechanisms such as LSTMs or GRUs which currently lack experimental support and prohibit direct comparison between RNNs and biological neural circuits. We tackled this problem based on the idea of specialized skip-connections through time to support the emergence of task-relevant dynamics, and subsequently reinstitute biological plausibility by reverting to the original architecture. We show that this approach enables RNNs to successfully learn cognitive tasks that prove impractical if not impossible to learn using conventional methods. Over numerous tasks considered here, we achieve less training steps and shorter wall-clock times, particularly in tasks that require learning long-term dependencies via temporal integration over long timescales or maintaining a memory of past events in hidden-states. Our methods expand the range of experimental tasks that biologically plausible RNN models can learn, thereby supporting the development of theory for the emergent neural mechanisms of computations involving long-term dependencies.},
  pmcid = {PMC10592728},
  pmid = {37873445},
  file = {C:\Users\kmc07\Zotero\storage\YNLZQUBM\Soo et al. - 2023 - Training biologically plausible recurrent neural networks on cognitive tasks with long-term dependen.pdf}
}

@incollection{subramoney2021,
  title = {Reservoirs {{Learn}} to {{Learn}}},
  booktitle = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  author = {Subramoney, Anand and Scherr, Franz and Maass, Wolfgang},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  pages = {59--76},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6_3},
  urldate = {2024-12-10},
  abstract = {The common procedure in reservoir computing is to take a ``found'' reservoir, such as a recurrent neural network with randomly chosen synaptic weights or a complex physical device, and to adapt the weights of linear readouts from this reservoir for a particular computing task. We address the question of whether the performance of reservoir computing can be significantly enhanced if one instead optimizes some (hyper)parameters of the reservoir, not for a single task but for the range of all possible tasks in which one is potentially interested, before the weights of linear readouts are optimized for a particular computing task. After all, networks of neurons in the brain are also known to be not randomly connected. Rather, their structure and parameters emerge from complex evolutionary and developmental processes, arguably in a way that enhances the speed and accuracy of subsequent learning of any concrete task that is likely to be essential for the survival of the organism. We apply the Learning-to-Learn (L2L) paradigm to mimic this two-tier process, where a set of (hyper)parameters of the reservoir are optimized for a whole family of learning tasks. We found that this substantially enhances the performance of reservoir computing for the families of tasks that we considered. Furthermore, L2L enables a new form of reservoir learning that tends to enable even faster learning, where not even the weights of readouts need to be adjusted for learning a concrete task. We present demos and performance results of these new forms of reservoir computing for reservoirs that consist of networks of spiking neurons and are hence of particular interest from the perspective of neuroscience and implementations in spike-based neuromorphic hardware. We leave it as an open question of what performance advantage the new methods that we propose provide for other types of reservoirs.},
  isbn = {978-981-13-1687-6},
  langid = {english}
}

@article{sudhof2008,
  title = {Understanding {{Synapses}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Understanding {{Synapses}}},
  author = {S{\"u}dhof, Thomas C. and Malenka, Robert C.},
  year = {2008},
  month = nov,
  journal = {Neuron},
  volume = {60},
  number = {3},
  pages = {469--476},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.10.011},
  urldate = {2025-01-19},
  abstract = {Classical physiological work by Katz, Eccles, and others revealed the central importance of synapses in brain function, and characterized the mechanisms involved in synaptic transmission. Building on this work, major advances in the past two decades have elucidated how synapses work molecularly. In the present perspective, we provide a short description of our personal view of these advances, suggest a series of important future questions about synapses, and discuss ideas about how best to achieve further progress in the field.},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\IG65ZT3G\\S√ºdhof and Malenka - 2008 - Understanding Synapses Past, Present, and Future.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\9RSHPHNI\\S0896627308008842.html}
}

@article{szu1987,
  title = {Fast Simulated Annealing},
  author = {Szu, Harold and Hartley, Ralph},
  year = {1987},
  month = jun,
  journal = {Physics Letters A},
  volume = {122},
  number = {3},
  pages = {157--162},
  issn = {0375-9601},
  doi = {10.1016/0375-9601(87)90796-1},
  urldate = {2025-01-19},
  abstract = {Simulated annealing is a stochastic strategy for searching the ground state. A fast simulated annealing (FSA) is a semi-local search and consists of occasional long jumps. The cooling schedule of the FSA algorithm is inversely linear in time which is fast compared with the classical simulated annealing (CSA) which is strictly a local search and requires the cooling schedule to be inversely proportional to the logarithmic function of time. A general D-dimensional Cauchy probability for generating the state is given. Proofs for both FSA and CSA are sketched. A double potential well is used to numerically illustrate both schemes.},
  file = {C:\Users\kmc07\Zotero\storage\NGZ7XIMY\0375960187907961.html}
}

@article{tanaka2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2019},
  month = jul,
  journal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.03.005},
  urldate = {2024-12-10},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  keywords = {Machine learning,Neural networks,Neuromorphic device,Nonlinear dynamical systems,Reservoir computing}
}

@article{tanaka2020,
  title = {Spatially {{Arranged Sparse Recurrent Neural Networks}} for {{Energy Efficient Associative Memory}}},
  author = {Tanaka, Gouhei and Nakane, Ryosho and Takeuchi, Tomoya and Yamane, Toshiyuki and Nakano, Daiju and Katayama, Yasunao and Hirose, Akira},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {1},
  pages = {24--38},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2899344},
  urldate = {2024-12-10},
  abstract = {The development of hardware neural networks, including neuromorphic hardware, has been accelerated over the past few years. However, it is challenging to operate very large-scale neural networks with low-power hardware devices, partly due to signal transmissions through a massive number of interconnections. Our aim is to deal with the issue of communication cost from an algorithmic viewpoint and study learning algorithms for energy-efficient information processing. Here, we consider two approaches to finding spatially arranged sparse recurrent neural networks with the high cost-performance ratio for associative memory. In the first approach following classical methods, we focus on sparse modular network structures inspired by biological brain networks and examine their storage capacity under an iterative learning rule. We show that incorporating long-range intermodule connections into purely modular networks can enhance the cost-performance ratio. In the second approach, we formulate for the first time an optimization problem where the network sparsity is maximized under the constraints imposed by a pattern embedding condition. We show that there is a tradeoff between the interconnection cost and the computational performance in the optimized networks. We demonstrate that the optimized networks can achieve a better cost-performance ratio compared with those considered in the first approach. We show the effectiveness of the optimization approach mainly using binary patterns and apply it also to gray-scale image restoration. Our results suggest that the presented approaches are useful in seeking more sparse and less costly connectivity of neural networks for the enhancement of energy efficiency in hardware neural networks.},
  keywords = {Associative memory,Biological neural networks,energy efficiency,Hardware,Hopfield network,Integrated circuit interconnections,interconnection cost,iterative learning rule,Neurons,Optimization,Recurrent neural networks,sparse neural networks,sparse optimization}
}

@article{tavanaei2019,
  title = {Deep Learning in Spiking Neural Networks},
  author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timoth{\'e}e and Maida, Anthony},
  year = {2019},
  month = mar,
  journal = {Neural Networks},
  volume = {111},
  pages = {47--63},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.12.002},
  urldate = {2025-06-02},
  abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
  keywords = {Biological plausibility,Deep learning,Machine learning,Power-efficient architecture,Spiking neural network},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\8AZKLWPZ\\Tavanaei et al. - 2019 - Deep learning in spiking neural networks.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\SFUY558U\\S0893608018303332.html}
}

@inproceedings{tong2018,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Tong, Zhiqiang and Tanaka, Gouhei},
  year = {2018},
  month = aug,
  pages = {1289--1294},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2018.8545471},
  urldate = {2024-12-10},
  abstract = {Reservoir computing has attracted much attention for its easy training process as well as its ability to deal with temporal data. A reservoir computing system consists of a reservoir part represented as a sparsely connected recurrent neural network and a readout part represented as a simple regression model. In machine learning tasks, the reservoir part is fixed and only the readout part is trained. Although reservoir computing has been mainly applied to time series prediction and recognition, it can be applied to image recognition as well by considering an image data as a sequence of pixel values. However, to achieve a high performance in image recognition with raw image data, a large-scale reservoir including a large number of neurons is required. This is a bottleneck in terms of computer memory and computational cost. To overcome this bottleneck, we propose a new method which combines reservoir computing with untrained convolutional neural networks. We use an untrained convolutional neural network to transform raw image data into a set of smaller feature maps in a preprocessing step of the reservoir computing. We demonstrate that our method achieves a high classification accuracy in an image recognition task with a much smaller number of trainable parameters compared with a previous study.},
  keywords = {Computational modeling,Convolution,Convolutional neural networks,Feature extraction,Image recognition,Reservoirs,Training},
  file = {C:\Users\kmc07\Zotero\storage\J837JLDM\8545471.html}
}

@misc{vettoruzzo2023,
  title = {Advances and {{Challenges}} in {{Meta-Learning}}: {{A Technical Review}}},
  shorttitle = {Advances and {{Challenges}} in {{Meta-Learning}}},
  author = {Vettoruzzo, Anna and Bouguelia, Mohamed-Rafik and Vanschoren, Joaquin and R{\"o}gnvaldsson, Thorsteinn and Santosh, K. C.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.04722},
  eprint = {2307.04722},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.04722},
  urldate = {2025-06-02},
  abstract = {Meta-learning empowers learning systems with the ability to acquire knowledge from multiple tasks, enabling faster adaptation and generalization to new tasks. This review provides a comprehensive technical overview of meta-learning, emphasizing its importance in real-world applications where data may be scarce or expensive to obtain. The paper covers the state-of-the-art meta-learning approaches and explores the relationship between meta-learning and multi-task learning, transfer learning, domain adaptation and generalization, self-supervised learning, personalized federated learning, and continual learning. By highlighting the synergies between these topics and the field of meta-learning, the paper demonstrates how advancements in one area can benefit the field as a whole, while avoiding unnecessary duplication of efforts. Additionally, the paper delves into advanced meta-learning topics such as learning from complex multi-modal task distributions, unsupervised meta-learning, learning to efficiently adapt to data distribution shifts, and continual meta-learning. Lastly, the paper highlights open problems and challenges for future research in the field. By synthesizing the latest research developments, this paper provides a thorough understanding of meta-learning and its potential impact on various machine learning applications. We believe that this technical overview will contribute to the advancement of meta-learning and its practical implications in addressing real-world problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\DS8MTJ6R\\Vettoruzzo et al. - 2023 - Advances and Challenges in Meta-Learning A Technical Review.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\3ZPS85UG\\2307.html}
}

@article{wang2005,
  title = {Phosphorylation of {{AMPA}} Receptors},
  author = {Wang, John Q. and Arora, Anish and Yang, Lu and Parelkar, Nikhil K. and Zhang, Guochi and Liu, Xianyu and Choe, Eun Sang and Mao, Limin},
  year = {2005},
  month = dec,
  journal = {Molecular Neurobiology},
  volume = {32},
  number = {3},
  pages = {237--249},
  issn = {1559-1182},
  doi = {10.1385/MN:32:3:237},
  urldate = {2025-01-19},
  abstract = {The ionotropic {$\alpha$}-amino-3-hydroxy-5-methylisoxazole-4-propionic acid (AMPA) receptor is densely distributed in the mammalian brain and is primarily involved in mediating fast excitatory synaptic transmission. Recent studies in both heterologous expression systems and cultured neurons have shown that the AMPA receptor can be phosphorylated on their subunits (GluR1, GluR2, and GluR4). All phosphorylation sites reside at serine, threonine, or tyrosine on the intracellular C-terminal domain. Several key protein kinases, such as protein kinase A, protein kinase C, Ca2+/calmodulin-dependent protein kinase II, and tyrosine kinases (Trks; receptor or nonreceptor family Trks) are involved in the site-specific regulation of the AMPA receptor phosphorylation. Other glutamate receptors (N-methyl-d-aspartate receptors and metabotropic glutamate receptors) also regulate AMPA receptors through a protein phosphorylation mechanism. Emerging evidence shows that as a rapid and short-term mechanism, the dynamic protein phosphorylation directly modulates the electrophysiological, morphological (externalization and internalization trafficking and clustering), and biochemical (synthesis and subunit composition) properties of the AMPA receptor, as well as protein-protein interactions between the AMPA receptor subunits and various intracellular interacting proteins. These modulations underlie the major molecular mechanisms that ultimately affect many forms of synaptic plasticity.},
  langid = {english},
  keywords = {dopamine,GluR,Glutamate,kainate,mGluR,N-methyl-d-aspartate (NMDA),serine,striatum,tyrosine},
  file = {C:\Users\kmc07\Zotero\storage\4IIZCCMQ\Wang et al. - 2005 - Phosphorylation of AMPA receptors.pdf}
}

@book{wasserman1989,
  title = {Neural Computing: Theory and Practice},
  shorttitle = {Neural Computing},
  author = {Wasserman, P. D.},
  year = {1989},
  month = mar,
  publisher = {Van Nostrand Reinhold Co.},
  address = {USA},
  isbn = {978-0-442-20743-4}
}

@article{whittington2019,
  title = {Theories of {{Error Back-Propagation}} in the {{Brain}}},
  author = {Whittington, James C. R. and Bogacz, Rafal},
  year = {2019},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {3},
  pages = {235--250},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2018.12.005},
  urldate = {2025-06-02},
  abstract = {This review article summarises recently proposed theories on how neural circuits in the brain could approximate the error back-propagation algorithm used by artificial neural networks. Computational models implementing these theories achieve learning as efficient as artificial neural networks, but they use simple synaptic plasticity rules based on activity of presynaptic and postsynaptic neurons. The models have similarities, such as including both feedforward and feedback connections, allowing information about error to propagate throughout the network. Furthermore, they incorporate experimental evidence on neural connectivity, responses, and plasticity. These models provide insights on how brain networks might be organised such that modification of synaptic weights on multiple levels of cortical hierarchy leads to improved performance on tasks.},
  keywords = {deep learning,neural networks,predictive coding,synaptic plasticity},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\GGY3DUJS\\Whittington and Bogacz - 2019 - Theories of Error Back-Propagation in the Brain.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\4LY73PPA\\S1364661319300129.html}
}

@misc{xiao2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017},
  month = sep,
  number = {arXiv:1708.07747},
  eprint = {1708.07747},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1708.07747},
  urldate = {2025-06-02},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\63SX58EY\\Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\PJK3M3ZN\\1708.html}
}

@article{yamamoto2016,
  title = {Unidirectional Signal Propagation in Primary Neurons Micropatterned at a Single-Cell Resolution},
  author = {Yamamoto, H. and Matsumura, R. and Takaoki, H. and Katsurabayashi, S. and {Hirano-Iwata}, A. and Niwano, M.},
  year = {2016},
  month = jul,
  journal = {Applied Physics Letters},
  volume = {109},
  number = {4},
  pages = {043703},
  issn = {0003-6951},
  doi = {10.1063/1.4959836},
  urldate = {2025-06-02},
  abstract = {The structure and connectivity of cultured neuronal networks can be controlled by using micropatterned surfaces. Here, we demonstrate that the direction of signal propagation can be precisely controlled at a single-cell resolution by growing primary neurons on micropatterns. To achieve this, we first examined the process by which axons develop and how synapses form in micropatterned primary neurons using immunocytochemistry. By aligning asymmetric micropatterns with a marginal gap, it was possible to pattern primary neurons with a directed polarization axis at the single-cell level. We then examined how synapses develop on micropatterned hippocampal neurons. Three types of micropatterns with different numbers of short paths for dendrite growth were compared. A normal development in synapse density was observed when micropatterns with three or more short paths were used. Finally, we performed double patch clamp recordings on micropatterned neurons to confirm that these synapses are indeed functional, and that the neuronal signal is transmitted unidirectionally in the intended orientation. This work provides a practical guideline for patterning single neurons to design functional neuronal networks in vitro with the direction of signal propagation being controlled.},
  pmcid = {PMC5030838},
  pmid = {27746482},
  file = {C:\Users\kmc07\Zotero\storage\YNU8EEAQ\Yamamoto et al. - 2016 - Unidirectional signal propagation in primary neurons micropatterned at a single-cell resolution.pdf}
}

@article{yildiz2012,
  title = {Re-Visiting the Echo State Property},
  author = {Yildiz, Izzet B. and Jaeger, Herbert and Kiebel, Stefan J.},
  year = {2012},
  month = nov,
  journal = {Neural Networks},
  volume = {35},
  pages = {1--9},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.07.005},
  urldate = {2024-12-15},
  abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  keywords = {Bifurcation,Diagonally Schur stable,Echo state network,Lyapunov,Spectral radius},
  file = {C\:\\Users\\kmc07\\Zotero\\storage\\DVGN65IF\\Yildiz et al. - 2012 - Re-visiting the echo state property.pdf;C\:\\Users\\kmc07\\Zotero\\storage\\SXTS6G84\\S0893608012001852.html}
}

@article{zhang,
  title = {Feedback {{Alignment Algorithms}}},
  author = {Zhang, Lisa and Wang, Tingwu and Ren, Mengye},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\ZTIW4Q7L\Zhang et al. - Feedback Alignment Algorithms.pdf}
}

@misc{zotero-512,
  title = {Cascade {{Models}} of {{Synaptically Stored Memories}} - {{ScienceDirect}}},
  urldate = {2024-11-28},
  howpublished = {https://www.sciencedirect.com/science/article/pii/S0896627305001170},
  file = {C:\Users\kmc07\Zotero\storage\CV6YNLRM\S0896627305001170.html}
}

@misc{zotero-569,
  title = {Reservoir {{Computing}} with {{Untrained Convolutional Neural Networks}} for {{Image Recognition}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2024-12-10},
  howpublished = {https://ieeexplore.ieee.org/document/8545471},
  file = {C:\Users\kmc07\Zotero\storage\I3NV782M\8545471.html}
}

@misc{zotero-570,
  title = {Scopus - {{Document}} Details - {{Information}} Processing Using a Single Dynamical Node as Complex System},
  doi = {10.1038/ncomms1476},
  urldate = {2024-12-10},
  abstract = {Elsevier's Scopus, the largest abstract and citation database of peer-reviewed literature. Search and access research from the science, technology, medicine, social sciences and arts and humanities fields.},
  howpublished = {https://www.scopus.com/record/display.uri?eid=2-s2.0-80053397808\&origin=inward\&txGid=76e2085e73e050207a9891823415f4ab},
  langid = {american}
}

@misc{zotero-612,
  title = {Principles of {{Neural Science}}, {{Fifth Edition}}},
  journal = {McGraw Hill Medical},
  urldate = {2025-01-19},
  abstract = {Read this chapter of Principles of Neural Science, Fifth Edition online now, exclusively on AccessNeurology. AccessNeurology is a subscription-based resource from McGraw Hill that features trusted medical content from the best minds in medicine.},
  howpublished = {https://neurology.mhmedical.com/content.aspx?sectionid=59138139\&bookid=1049},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\NW5UIWEU\content.html}
}

@misc{zotero-628,
  title = {Fast Simulated Annealing},
  journal = {ResearchGate},
  urldate = {2025-01-19},
  abstract = {Access 135+ million publications and connect with 20+ million researchers. Join for free and gain visibility by uploading your research.},
  howpublished = {https://www.researchgate.net/publication/223828956\_Fast\_simulated\_annealing},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\676HC52A\223828956_Fast_simulated_annealing.html}
}

@misc{zotero-642,
  title = {Evolutionary {{Principles}} in {{Self-Referential Learning}}. {{On Learning}} Now to {{Learn}}: {{The Meta-Meta-Meta}}...-{{Hook}} {\textbar} {{BibSonomy}}},
  urldate = {2025-01-23},
  howpublished = {https://www.bibsonomy.org/bibtex/2a96f7c3d42103ab94b13badef5d869f0/brazovayeye},
  file = {C:\Users\kmc07\Zotero\storage\EDZX8YHX\brazovayeye.html}
}

@misc{zotero-643,
  title = {1987 {{THESIS ON LEARNING HOW TO LEARN}}, {{METALEARNING}}, {{META GENETIC PROGRAMMING}}, {{CREDIT-CONSERVING MACHINE LEARNING ECONOMY}}},
  urldate = {2025-01-23},
  howpublished = {https://people.idsia.ch/{\textasciitilde}juergen/diploma.html},
  file = {C:\Users\kmc07\Zotero\storage\YHDM6G78\diploma.html}
}

@misc{zotero-653,
  title = {Papers with {{Code}} - {{Discriminative Fine-Tuning Explained}}},
  urldate = {2025-02-21},
  abstract = {Discriminative Fine-Tuning is a fine-tuning strategy that is used for ULMFiT type models. Instead of using the same learning rate for all layers of the model, discriminative fine-tuning allows us to tune each layer with different learning rates. For context, the regular stochastic gradient descent (SGD) update of a model's parameters \${\textbackslash}theta\$ at time step \$t\$ looks like the following (Ruder, 2016): \$\$ {\textbackslash}theta\_\{t\} = {\textbackslash}theta\_\{t-1\} - {\textbackslash}eta{\textbackslash}cdot{\textbackslash}nabla\_\{{\textbackslash}theta\}J{\textbackslash}left({\textbackslash}theta{\textbackslash}right)\$\$ where \${\textbackslash}eta\$ is the learning rate and \${\textbackslash}nabla\_\{{\textbackslash}theta\}J{\textbackslash}left({\textbackslash}theta{\textbackslash}right)\$ is the gradient with regard to the model's objective function. For discriminative fine-tuning, we split the parameters \${\textbackslash}theta\$ into \{\${\textbackslash}theta\_\{1\}, {\textbackslash}ldots, {\textbackslash}theta\_\{L\}\$\} where \${\textbackslash}theta\_\{l\}\$ contains the parameters of the model at the \$l\$-th layer and \$L\$ is the number of layers of the model. Similarly, we obtain \{\${\textbackslash}eta\_\{1\}, {\textbackslash}ldots, {\textbackslash}eta\_\{L\}\$\} where \${\textbackslash}theta\_\{l\}\$ where \${\textbackslash}eta\_\{l\}\$ is the learning rate of the \$l\$-th layer. The SGD update with discriminative finetuning is then: \$\$ {\textbackslash}theta\_\{t\}{\textasciicircum}\{l\} = {\textbackslash}theta\_\{t-1\}{\textasciicircum}\{l\} - {\textbackslash}eta{\textasciicircum}\{l\}{\textbackslash}cdot{\textbackslash}nabla\_\{{\textbackslash}theta{\textasciicircum}\{l\}\}J{\textbackslash}left({\textbackslash}theta{\textbackslash}right) \$\$ The authors find that empirically it worked well to first choose the learning rate \${\textbackslash}eta{\textasciicircum}\{L\}\$ of the last layer by fine-tuning only the last layer and using \${\textbackslash}eta{\textasciicircum}\{l-1\}={\textbackslash}eta{\textasciicircum}\{l\}/2.6\$ as the learning rate for lower layers.},
  howpublished = {https://paperswithcode.com/method/discriminative-fine-tuning},
  langid = {english},
  file = {C:\Users\kmc07\Zotero\storage\8BEQE79E\discriminative-fine-tuning.html}
}

@misc{zotero-item-699,
  howpublished = {https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608005X80832/1-s2.0-S0893608005800899/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEKj\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2F\%2FwEaCXVzLWVhc3QtMSJHMEUCIQCh7ilkMNoslzkZDUu1qnb7ggNrHcis2ZXYaOSCotu\%2BAQIgDslnXsCqq5km4WfITgYUKVJTj492igAB0G55QvyXszUqswUIYRAFGgwwNTkwMDM1NDY4NjUiDGBOD6c\%2Fd\%2BHC88I2hiqQBdxU8L39a\%2FpXh7aatuaypbpMLgLskI6SzLZcY5HlBXvWTrYXNXYXoWG9s8YegXeEVv2hkfhBuLhITJFCZ\%2Bg041w\%2FUVuM\%2FLuLNf4cqeITVMdFA6cRYtFrOQZK6x\%2FD8E\%2Fd1Pp08N6TvTSZ1k\%2BPmYHaEE4Sm\%2FNFawsuUwFy4xLhw\%2FDffPLVab\%2BPMulLqKPX\%2FeGTm3k4DgFUe4yARfGgCzyAjzcH5aUBQsXA128Ta\%2BNE4\%2Bn\%2F0tnQVzgiWbLir5ZLzwuqEBN5sEDLM257cXyJP1ZefNXlcA1Se0R7YNafoARej9pYFljy9GOgcmMEEr8tpuaes6hkHhFD19S7b62hV6Z0Uq\%2BHpMQ8VWcJMpY8ICstjwPxoFcnCk9bJ4xgcPXWBONXAdlQAIyUe6PA3M5E5lnjfajhfFMz7V\%2FiAJAj54hyMaGtYZfUeHlNUNQPsCsoAsnMN92FPt8WXXe6w2RSSYv4zx5UyuV2PdMo2IX2nJ4\%2F7tDDdjjUZsuq1zuiFHRcwUprB4VQ3uYpxpztWewpGVkzFf04mZ0cvvRLcfcJl8MgpJjTJzycvadQqbvFkAvlg5ez52qPfXEaM6DUTofBY26fFqxDYVr\%2BGEulgwj4kq5wbnTbacB6wD2cd4KMdEKXiqxvyfmjFP0caVdNetvBbXvnBgBGxp3pgEseX1yRaaagGWuMZ0LE7OjSbgu4c2HzATsGLwSSbP9Ii7j6CLUw9zyT\%2BZJ3N3R08qef8YWn2LdAy7RJNmhsiunUjXegTifxgTM0wzcGVQjzGS2qlZRojBAOq7OXYPCk1cpjluDdn6ssfNg\%2BLyYACleX4NxF6CTj7anGoIrLL\%2BO9ocxpGfYFKmmbaySgWOSq1R7cRsm52ye0ZmDvMKDWosEGOrEBIBJcAhjmL5TisVU16uvcyVv\%2FsJZPjRqKPsHLbe0yoVig07YnduRkzw3OwDfw8kia\%2BvsF9f3ocPoDTg1jeLn8yYW7xJhsCgypln\%2BwyYQoYj28SOs6TngBXMCnfpgGOUXUhjE299FN7gX2gM9DZs\%2Bk6dVAMHSPrUoBvbI7ZQraCW7uaB5bOuVapMjh5TdFqFVhJzkwm\%2Bq0Uh5BZKxWDJmeSD39\%2FmVRGtUnuoh8UmlkCRg6\&X-Amz-Algorithm=AWS4-HMAC-SHA256\&X-Amz-Date=20250517T160121Z\&X-Amz-SignedHeaders=host\&X-Amz-Expires=300\&X-Amz-Credential=ASIAQ3PHCVTYW4JNF4NH\%2F20250517\%2Fus-east-1\%2Fs3\%2Faws4\_request\&X-Amz-Signature=946ce1237ffbb04313665eb0b92f946c785b78474632238cdd0a4e93eb94de05\&hash=331c02c036dcb58c1fb92bfbcad5e3dd564427d9d8d27f5560695a773db7a650\&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61\&pii=S0893608005800899\&tid=spdf-36e1a406-dca2-4b96-8bb6-09d9949bd581\&sid=802034fd8e12a448180ad40121069e501e52gxrqb\&type=client\&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t\&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t\&ua=1d045b5153535756500d57\&rr=9414553b78b0074f\&cc=gb}
}

@misc{zotero-item-705,
  title = {Local Online Learning in Recurrent Networks with Random Feedback {\textbar} {{eLife}}},
  urldate = {2025-05-19},
  howpublished = {https://elifesciences.org/articles/43299},
  file = {C:\Users\kmc07\Zotero\storage\Q8J5TCUA\43299.html}
}
